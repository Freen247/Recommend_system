{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "sys.version_info(major=3, minor=6, micro=7, releaselevel='final', serial=0)\ntensorflow 2.1.0\nmatplotlib 3.2.0\nnumpy 1.18.1\npandas 1.0.1\nsklearn 0.22.2.post1\ntensorflow 2.1.0\ntensorflow_core.python.keras.api._v2.keras 2.2.4-tf\n"
    }
   ],
   "source": [
    "import os, sys, re, pickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow import keras\n",
    "print(sys.version_info)\n",
    "for module in tf, mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ORIGIN_DATA_DIR = os.getcwd() + '\\\\data\\\\BX-CSV-Dump\\\\'\n",
    "FILTERED_DATA_DIR = os.getcwd() + '\\\\tmp\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataLoad:\n",
    "    def __init__(self):\n",
    "        self.BX_Users = self.load_origin('BX-Users')\n",
    "        self.BX_Book_Ratings = self.load_origin('BX-Book-Ratings')\n",
    "        self.books_with_blurbs = self.load_origin('books_with_blurbs', ',')\n",
    "        #合并三个表\n",
    "        self.features = self.get_features()\n",
    "        self.labels = self.features.pop('Book-Rating')\n",
    "    \n",
    "    def load_origin(self,filename,sep=\"\\\";\\\"\"):\n",
    "        \"\"\"\n",
    "        filename;根据文件名获取源文件，获取正确的columns、values等值\n",
    "        sep:根据源文件的分割方式不同，通过传参改变分割方式sep=\"\\\":\\\"\"\n",
    "        \"\"\"\n",
    "        try:\n",
    "            #从缓存的文件夹Filter_data_dir获取基本被过滤的文件\n",
    "            pickled_data = pickle.load(open(FILTERED_DATA_DIR+filename+'.p',mode='rb'))\n",
    "            return pickled_data\n",
    "        except FileNotFoundError:\n",
    "            #如果缓存的文件不存在或者没有，则在ORIGIN_DATA_DIR获取\n",
    "            all_fearures = pd.read_csv(ORIGIN_DATA_DIR+filename+'.csv',engine='python',sep = sep,encoding='utf-8')\n",
    "            # \\\";\\\"  初始过滤的文件\n",
    "            # ,      初始不需要过滤的文件\n",
    "            data_dict = {\"\\\";\\\"\":self.filtrator(all_fearures), ',':all_fearures}\n",
    "            # 因为没获得处理后的文件，所以我们在获取源文件后可以保存一下处理后的文件\n",
    "            pickle.dump((data_dict[sep]), open(FILTERED_DATA_DIR+filename+'.p', 'wb'))\n",
    "            return data_dict[sep]\n",
    "        except UnicodeDecodeError as e:\n",
    "            ''' 测试时经常会出现编码错误，如果尝试更换编码方式无效，可以将编码错误的部分位置重新复制粘贴就可以了，这里我们都默认UTF-8'''\n",
    "            print('UnicodeDecodeError:',e)\n",
    "        except pd.errors.ParserError as e:\n",
    "            print(\"connect error|pandas Error: %s\" % e)\n",
    "        \n",
    "    def filtrator(self,f_data):\n",
    "        \"\"\"\n",
    "        f_data:需要初步进行filter的数据\n",
    "        第一列的列名和最后一列的列名都带\"\n",
    "        \"\"\"\n",
    "        Nonetype_age = 0\n",
    "        f_data = f_data.rename(columns={f_data.columns[0]:f_data.columns[0][1:], f_data.columns[-1]:f_data.columns[-1][:-1]})\n",
    "        f_data[f_data.columns[0]] = f_data[f_data.columns[0]].map(lambda v:v[1:] if v!=None else Nonetype_age)\n",
    "        f_data[f_data.columns[-1]] = f_data[f_data.columns[-1]].map(lambda v:v[:-1] if v!=None else Nonetype_age)\n",
    "        try:\n",
    "            f_data = f_data[f_data['Location'].notnull()][f_data[f_data['Location'].notnull()]['Location'].str.contains('\\\";NULL')]\n",
    "            f_data['Location'] = f_data['Location'].map(lambda location:location[:-6])\n",
    "        except:\n",
    "            pass\n",
    "        return f_data\n",
    "    def get_features(self):\n",
    "        \"\"\"\n",
    "        获取整个数据集的所有features，并对每个文本字段做处理\n",
    "       User-ID:id2word\n",
    "        Location：id2list\n",
    "        ISBN：id2word\n",
    "        Title：id2text\n",
    "        Author：id2word\n",
    "        Year：id2word\n",
    "        Publisher：id2word\n",
    "        Blurb:id2text\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 从缓存的文件夹FILTERED_DATA_DIR获取features的文件\n",
    "            pickled_data = pickle.load(open(FILTERED_DATA_DIR+'features.p', mode='rb'))\n",
    "            return pickled_data\n",
    "        except FileNotFoundError:\n",
    "            #将所有的数据组成features大表\n",
    "            all_fearures = pd.merge(pd.merge(self.BX_Users,self.BX_Book_Ratings),self.books_with_blurbs)\n",
    "            #删除age列\n",
    "            all_fearures.pop('Age')\n",
    "            all_fearures['Title'] = self.feature2int(all_fearures['Title'], 'text', 15)\n",
    "            all_fearures['Blurb'] = self.feature2int(all_fearures['Blurb'], 'text', 200)\n",
    "            all_fearures['ISBN'] = self.feature2int(all_fearures['ISBN'], 'word')\n",
    "            all_fearures['Author'] = self.feature2int(all_fearures['Author'], 'word')\n",
    "            all_fearures['Publisher'] = self.feature2int(all_fearures['Publisher'], 'word')\n",
    "            all_fearures['User-ID'] = self.feature2int(all_fearures['User-ID'], 'word')\n",
    "            all_fearures['Year'] = self.feature2int(all_fearures['Year'], 'word')\n",
    "            all_fearures['Location'] = self.feature2int(all_fearures['Location'], 'list')\n",
    "            all_fearures['Book-Rating'] = all_fearures['Book-Rating'].astype('float32')\n",
    "            pickle.dump(all_fearures, open(FILTERED_DATA_DIR+'features.p', 'wb'))\n",
    "            return all_fearures\n",
    "    def feature2int(self, \n",
    "        feature:'特征值',\n",
    "        feature_type:'text/word/list',\n",
    "        length:'文本设置的最大长度' = 0,\n",
    "        ):\n",
    "        '''\n",
    "        将文本字段比如title、blurb只取英文单词，并用空格为分隔符，做成一个带index值的集合，并用index值表示各个单词，作为文本得表示\n",
    "        '''\n",
    "        pattern = re.compile(r'[^a-zA-Z]')\n",
    "        #filtered_map:匹配的一个规则\n",
    "        filtered_map = {val:re.sub(pattern, ' ', str(val)) for ii,val in enumerate(set(feature)) }\n",
    "        #1.letter_filter:非英文单词的用空格替换掉的一个匿名过滤的函数\n",
    "        letter_filter = lambda feature:feature.map({val:re.sub(pattern, ' ', str(val)) for ii,val in enumerate(set(feature)) })\n",
    "        text_words = set()\n",
    "        filtered_feature = letter_filter(feature)\n",
    "        #2.text_words:更新单词表set  \n",
    "        for val in filtered_feature.str.split():\n",
    "            text_words.update(val)\n",
    "        text_words.add('<PAD>')\n",
    "        text2int = {val:ii for ii, val in enumerate(text_words)}\n",
    "        text_map = {val:[text2int[row] for row in filtered_map[val].split()][:length] for ii,val in enumerate(set(feature))}\n",
    "        #按照索引值进行插入，超出索引值直接插0即可\n",
    "        for key in text_map:\n",
    "            for cnt in range(length - len(text_map[key])):\n",
    "                text_map[key].insert(len(text_map[key]) + cnt,text2int['<PAD>'])   \n",
    "\n",
    "        word_map = {val:ii for ii,val in enumerate(set(feature))}\n",
    "\n",
    "        try:\n",
    "            cities = set()\n",
    "            for val in feature.str.split(','):\n",
    "                cities.update(val)\n",
    "            city_index = {val:ii for ii, val in enumerate(cities)}\n",
    "            list_map = {val:[city_index[row] for row in val.split(',')][:3] for ii,val in enumerate(set(feature))}\n",
    "        except AttributeError :\n",
    "            list_map = {}\n",
    "        feature_dict = {\n",
    "            'text':feature.map(text_map),\n",
    "            'word':feature.map(word_map),\n",
    "            'list':feature.map(list_map),\n",
    "            }\n",
    "        return feature_dict[feature_type]\n",
    "\n",
    "    def __del__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'pop'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c2ea2103ab6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-49c94f25d018>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;31m#合并三个表\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Book-Rating'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload_origin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\\";\\\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'pop'"
     ]
    }
   ],
   "source": [
    "data = DataLoad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-4ab12a3912ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlength\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Blurb'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "length  = [len(i) for i in data.features['Blurb']]\n",
    "np.mean(np.array(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-3e79c1090e33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-1c60a099ee6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#User-ID number,max_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mall_user_id_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'User-ID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'all_user_id_number='\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mall_user_id_number\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#Location number index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "#User-ID number,max_index\n",
    "all_user_id_number = len(set(data.features['User-ID']))\n",
    "print('all_user_id_number=',all_user_id_number)\n",
    "\n",
    "#Location number index\n",
    "location_length = len(data.features['Location'][0])\n",
    "all_location_words_number = max([j for i in data.features['Location'] for j in i])\n",
    "print('location_length=%d,all_location_words_number=%d '% (location_length,all_location_words_number) )\n",
    "\n",
    "#ISBN numner\n",
    "all_isbn_words_number = len(set(data.features['ISBN']))\n",
    "print('all_isbn_words_number ',all_isbn_words_number)\n",
    "\n",
    "#Title\n",
    "title_length = len(data.features['Title'][0])\n",
    "all_title_words_number = max([j for i in data.features['Title'] for j in i])\n",
    "print('title_length=%d,all_title_words_number=%d'% (title_length,all_title_words_number))\n",
    "\n",
    "#Author\n",
    "all_author_words_number = len(set(data.features['Author']))\n",
    "print('all_author_words_number ',all_author_words_number)\n",
    "\n",
    "#Year\n",
    "all_year_words_number = len(set(data.features['Year']))\n",
    "print('all_year_words_number ',all_year_words_number)\n",
    "\n",
    "# Publisher\n",
    "all_publisher_words_number = len(set(data.features['Publisher']))\n",
    "print('all_publisher_words_number ',all_publisher_words_number)\n",
    "\n",
    "# Blurb\n",
    "blurb_length = len(data.features['Blurb'][0])\n",
    "all_blurb_words_number = max([j for i in data.features['Blurb'] for j in i])\n",
    "print('blurb_length=%d,all_blurb_words_number=%d'% (blurb_length,all_blurb_words_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "超参数\n",
    "\"\"\"\n",
    "# Number of Epochs\n",
    "num_epochs = 5\n",
    "# Batch_size\n",
    "batch_size = 256\n",
    "# dropout_keep\n",
    "dropout_keep = 0.5\n",
    "# Learning_rate\n",
    "learning_rate = 0.001\n",
    "# embedding_dim\n",
    "embedding_dim = 16\n",
    "# 文本卷积滑动的单词个数\n",
    "window_sizes = {2,3,4,5}\n",
    "# filter num\n",
    "filter_num = 8\n",
    "# dense dim\n",
    "dense_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "#     用户特征输入占位符\n",
    "    # 用户特征输入\n",
    "    user_id = keras.layers.Input(shape=(1,), dtype='int32', name='user_id_input')\n",
    "    user_location = keras.layers.Input(shape=(3,), dtype='int32', name='user_location_input')\n",
    "    \n",
    "    # 书籍特征输入\n",
    "    book_isbn = keras.layers.Input(shape=(1,),  dtype='int32', name='book_isbn_input')\n",
    "    book_author = keras.layers.Input(shape=(1,),  dtype='int32', name='book_author_input')\n",
    "    book_year = keras.layers.Input(shape=(1,),  dtype='int32', name='book_year_input')\n",
    "    book_publisher = keras.layers.Input(shape=(1,),  dtype='int32', name='book_publisher_input')  \n",
    "    book_title = keras.layers.Input(shape=(15, ), dtype='int32', name='book_title_input')\n",
    "    book_blurb = keras.layers.Input(shape=(200, ), dtype='int32', name='book_blurb_input')\n",
    "    return user_id, user_location, book_isbn, book_author, book_year, book_publisher, book_title, book_blurb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "构建User神经网络\n",
    "\"\"\"\n",
    "def get_user_embedding(user_id,user_location):\n",
    "    uid_embed_layer = tf.keras.layers.Embedding(all_user_id_number,embedding_dim,input_length=1,name='uid_embed_layer')(user_id)\n",
    "    location_embed_layer = tf.keras.layers.Embedding(all_location_words_number,embedding_dim,input_length=location_length,name='location_embed_layer')(user_location)\n",
    "    return uid_embed_layer, location_embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_user_feature_layer(uid_embed_layer,location_embed_layer):\n",
    "#     第一层全连接\n",
    "    uid_fc_layer = tf.keras.layers.Dense(dense_dim,name='uid_fc_layer',activation='relu')(uid_embed_layer)\n",
    "    location_fc_layer = tf.keras.layers.Dense(dense_dim,name='location_fc_layer',activation='relu')(location_embed_layer)\n",
    "#  对location进行Encoder提取特征\n",
    "    location_gru_layer = tf.keras.layers.GRU(units=dense_dim,dropout=0.5,name='location_gru_layer')(location_fc_layer)\n",
    "#     [None,32]\n",
    "    print(location_gru_layer.shape)\n",
    "    location_gru_expand_layer = tf.expand_dims(location_gru_layer,axis=1)\n",
    "    \n",
    "#     第二层全连接\n",
    "    user_combine_layer = tf.keras.layers.concatenate([uid_fc_layer,location_gru_expand_layer],2)\n",
    "    user_dense_layer = tf.keras.layers.Dense(200,activation='tanh',name='user_dense_layer')(user_combine_layer)\n",
    "    user_dense_layer_flat = tf.keras.layers.Reshape([200], name=\"user_combine_layer_flat\")(user_dense_layer)\n",
    "    return user_dense_layer,user_dense_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "构建book神经网络\n",
    "\"\"\"\n",
    "def get_book_embedding(book_isbn, book_author, book_year, book_publisher, book_title, book_blurb):\n",
    "    book_isbn_embed_layer = tf.keras.layers.Embedding(all_isbn_words_number,embedding_dim,input_length = 1,name='book_isbn_embed_layer')(book_isbn)\n",
    "    book_author_embed_layer = tf.keras.layers.Embedding(all_author_words_number,embedding_dim,input_length=1,name='book_author_embed_layer')(book_author)\n",
    "    book_year_embed_layer = tf.keras.layers.Embedding(all_year_words_number,embedding_dim,input_length=1,name='book_year_embed_layer')(book_year)\n",
    "    book_publisher_embed_layer = tf.keras.layers.Embedding(all_publisher_words_number,embedding_dim,input_length = 1,name='book_publisher_embed_layer')(book_publisher)\n",
    "    book_title_embed_layer = tf.keras.layers.Embedding(all_title_words_number,embedding_dim,input_length=title_length,name='book_title_embed_layer')(book_title)\n",
    "    book_blurb_embed_layer = tf.keras.layers.Embedding(all_blurb_words_number,embedding_dim,input_length = blurb_length,name='book_blurb_embed_layer')(book_blurb)\n",
    "    return book_isbn_embed_layer,book_author_embed_layer,book_year_embed_layer,book_publisher_embed_layer,book_title_embed_layer,book_blurb_embed_layer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_book_feature_layer(book_isbn_embed_layer,book_author_embed_layer,book_year_embed_layer,book_publisher_embed_layer,book_title_embed_layer,book_blurb_embed_layer):\n",
    "#     对isbn,author,year,publisher第一层全连接\n",
    "    book_isbn_dense_layer = tf.keras.layers.Dense(dense_dim,activation='relu',name='book_isbn_dense_layer')(book_isbn_embed_layer)\n",
    "    book_author_dense_layer = tf.keras.layers.Dense(dense_dim,activation='relu',name='book_author_dense_layer')(book_author_embed_layer)\n",
    "    book_year_dense_layer = tf.keras.layers.Dense(dense_dim,activation='relu',name='book_year_dense_layer')(book_year_embed_layer)\n",
    "    book_publisher_dense_layer = tf.keras.layers.Dense(dense_dim,activation='relu',name='book_publisher_dense_layer')(book_publisher_embed_layer)\n",
    "    book_title_embed_layer_expand = tf.expand_dims(book_title_embed_layer,axis=-1)\n",
    "#     对title进行文本卷积\n",
    "#     book_title_embed_layer_expand:[None,15,16,1]\n",
    "#     对文本嵌入层使用不同的卷积核做卷积核最大池化\n",
    "    pool_layer_list = []\n",
    "    for window_size in window_sizes:\n",
    "        title_conv_layer = tf.keras.layers.Conv2D(filters = filter_num,kernel_size = (window_size,embedding_dim),strides=1,activation='relu')(book_title_embed_layer_expand)\n",
    "        title_maxpool_layer = tf.keras.layers.MaxPooling2D(pool_size=(title_length-window_size+1,1),strides=1)(title_conv_layer)\n",
    "        pool_layer_list.append(title_maxpool_layer)\n",
    "    pool_layer_layer = tf.keras.layers.concatenate(pool_layer_list,axis=-1,name='title_pool_layer')\n",
    "    max_num = len(window_sizes)*filter_num\n",
    "    pool_layer_flat = tf.keras.layers.Reshape([1,max_num],name='pool_layer_flat')(pool_layer_layer)\n",
    "    dropout_layer = tf.keras.layers.Dropout(dropout_keep, name = \"dropout_layer\")(pool_layer_flat)\n",
    "\n",
    "    # 对简介进行Encoder特征提取\n",
    "    book_blurb_dense_layer = tf.keras.layers.Dense(dense_dim,activation='relu',name='book_blurb_dense_layer')(book_blurb_embed_layer)\n",
    "    book_blurb_gru_layer = tf.keras.layers.GRU(units=dense_dim,dropout=0.5,name='book_blurb_gru_layer')(book_blurb_dense_layer)\n",
    "    print('book_blurb_gru_layer=',book_blurb_gru_layer.shape)\n",
    "    book_blurb_gru_expand_layer = tf.expand_dims(book_blurb_gru_layer,axis=1)\n",
    "    book_combine_layer = tf.keras.layers.concatenate([book_isbn_dense_layer,book_author_dense_layer,book_year_dense_layer,book_publisher_dense_layer,dropout_layer,book_blurb_gru_expand_layer],axis=-1)\n",
    "    book_dense_layer = tf.keras.layers.Dense(200, activation='tanh')(book_combine_layer)\n",
    "    book_dense_layer_flat = tf.keras.layers.Reshape([200], name=\"book_dense_layer_flat\")(book_dense_layer)\n",
    "    return book_dense_layer,book_dense_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rating(user_feature, book_feature):\n",
    "    multiply_layer = keras.layers.Lambda(lambda layer: tf.reduce_sum(layer[0]*layer[1],axis=1,keepdims=True), name = 'user_book_feature')(user_feature, book_feature)\n",
    "    print(multiply_layer.shape)\n",
    "    return multiply_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "TensorShape([2, 1])"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "a = tf.constant([[2,1,1],[1,3,4]])\n",
    "b = tf.constant([[0,1,1],[1,0,4]])\n",
    "c = inference = tf.keras.layers.Lambda(lambda layer:tf.reduce_sum(layer[0] * layer[1], axis=1,keepdims=True))((a,b))\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-e21a0a434dc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "features = data.features.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-489db814cf36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "features[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-6e03fa5a064b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtargets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "targets=data.labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get_Batch\n",
    "def get_batch(Xs,ys,batchsize):\n",
    "    for start in range(0,len(Xx,batchsize)):\n",
    "        end = min(start+batchsize,len(Xs))\n",
    "        yield xS[start:end],ys[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建计算图\n",
    "将用户特征和书籍特征作为输入，经过一个全连接，输出预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[1, 2, 5]"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# test\n",
    "a = np.array([[1, 2, 4, ([1, 2, 5])], \n",
    "             [3, 2, 6, ([6, 5, 1])],\n",
    "             [6, 9, 4, ([3, 7, 5])]])\n",
    "a.take(3,1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mv_network(object):\n",
    "    def __init__(self, batch_size=256):\n",
    "        self.batch_size = batch_size\n",
    "        self.best_loss = 9999\n",
    "        self.losses = {'train': [], 'test': []}\n",
    "\n",
    "        # 获取输入占位符\n",
    "        user_id, user_location, book_isbn, book_author, book_year, book_publisher, book_title, book_blurb = get_inputs()\n",
    "        # 获取User的2个嵌入向量\n",
    "        uid_embed_layer, location_embed_layer = get_user_embedding(user_id,user_location)\n",
    "        # 得到用户特征\n",
    "        user_dense_layer,user_dense_layer_flat =get_user_feature_layer(uid_embed_layer,location_embed_layer)\n",
    "        # 获取书籍的嵌入向量\n",
    "        book_isbn_embed_layer,book_author_embed_layer,book_year_embed_layer,book_publisher_embed_layer,book_title_embed_layer,book_blurb_embed_layer=get_book_embedding(book_isbn, book_author, book_year, book_publisher, book_title, book_blurb)\n",
    "        # 获取书籍特征\n",
    "        book_dense_layer,book_dense_layer_flat=get_book_feature_layer(book_isbn_embed_layer,book_author_embed_layer,book_year_embed_layer,book_publisher_embed_layer,book_title_embed_layer,book_blurb_embed_layer)\n",
    "        \n",
    "        # 计算出评分\n",
    "        # 将用户特征和电影特征做矩阵乘法得到一个预测评分的方案\n",
    "        print(\"user_dense_layer_flat=\",user_dense_layer_flat.shape)\n",
    "        print(\"book_dense_layer_flat=\",book_dense_layer_flat.shape)\n",
    "        inference = get_rating(user_dense_layer_flat,book_dense_layer_flat)\n",
    "\n",
    "        # 将用户特征和电影特征作为输入，经过全连接，输出一个值的方案\n",
    "        #         inference_layer = tf.keras.layers.concatenate([user_combine_layer_flat, movie_combine_layer_flat],\n",
    "        #                                                       1)  # (?, 400)\n",
    "        # 你可以使用下面这个全连接层，试试效果\n",
    "        # inference_dense = tf.keras.layers.Dense(64, kernel_regularizer=tf.nn.l2_loss, activation='relu')(\n",
    "        #    inference_layer)\n",
    "        #         inference = tf.keras.layers.Dense(1, name=\"inference\")(inference_layer)  # inference_dense\n",
    "\n",
    "        self.model = tf.keras.Model(\n",
    "            inputs=[user_id, user_location, book_isbn, book_title, book_author, book_year, book_publisher, book_blurb],\n",
    "            outputs=[inference])\n",
    "\n",
    "        self.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'all_user_id_number' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-5cf91a88a6ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmv_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-430f1b32fc34>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0muser_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_isbn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_author\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_year\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_publisher\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_title\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_blurb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# 获取User的2个嵌入向量\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0muid_embed_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation_embed_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_user_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muser_location\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;31m# 得到用户特征\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0muser_dense_layer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muser_dense_layer_flat\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mget_user_feature_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muid_embed_layer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlocation_embed_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-1e0209207a1e>\u001b[0m in \u001b[0;36mget_user_embedding\u001b[1;34m(user_id, user_location)\u001b[0m\n\u001b[0;32m      3\u001b[0m \"\"\"\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_user_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muser_location\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0muid_embed_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_user_id_number\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'uid_embed_layer'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mlocation_embed_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_location_words_number\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocation_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'location_embed_layer'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_location\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0muid_embed_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation_embed_layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_user_id_number' is not defined"
     ]
    }
   ],
   "source": [
    "model = mv_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}