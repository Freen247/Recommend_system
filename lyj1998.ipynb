{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\360Downloads\\Software\\Anaconda3-5.0.1\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=6, micro=3, releaselevel='final', serial=0)\n",
      "tensorflow 2.0.0\n",
      "matplotlib 2.0.2\n",
      "numpy 1.18.2\n",
      "pandas 0.20.3\n",
      "sklearn 0.19.0\n",
      "tensorflow 2.0.0\n",
      "tensorflow_core.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import os, sys, re, pickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow import keras\n",
    "print(sys.version_info)\n",
    "for module in tf, mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ORIGIN_DATA_DIR = os.getcwd() + '\\\\data\\\\BX-CSV-Dump\\\\'\n",
    "FILTERED_DATA_DIR = os.getcwd() + '\\\\tmp\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataLoad:\n",
    "    def __init__(self):\n",
    "        self.BX_Users = self.load_origin('BX-Users')\n",
    "        self.BX_Book_Ratings = self.load_origin('BX-Book-Ratings')\n",
    "        self.books_with_blurbs = self.load_origin('books_with_blurbs', ',')\n",
    "        #合并三个表\n",
    "        self.features = self.get_features()\n",
    "        self.labels = self.features.pop('Book-Rating')\n",
    "    \n",
    "    def load_origin(self,filename,sep=\"\\\";\\\"\"):\n",
    "        \"\"\"\n",
    "        filename;根据文件名获取源文件，获取正确的columns、values等值\n",
    "        sep:根据源文件的分割方式不同，通过传参改变分割方式sep=\"\\\":\\\"\"\n",
    "        \"\"\"\n",
    "        try:\n",
    "            #从缓存的文件夹Filter_data_dir获取基本被过滤的文件\n",
    "            pickled_data = pickle.load(open(FILTERED_DATA_DIR+filename+'.p',mode='rb'))\n",
    "            return pickled_data\n",
    "        except FileNotFoundError:\n",
    "            #如果缓存的文件不存在或者没有，则在ORIGIN_DATA_DIR获取\n",
    "            all_fearures = pd.read_csv(ORIGIN_DATA_DIR+filename+'.csv',engine='python',sep = sep,encoding='utf-8')\n",
    "            # \\\";\\\"  初始过滤的文件\n",
    "            # ,      初始不需要过滤的文件\n",
    "            data_dict = {\"\\\";\\\"\":self.filtrator(all_fearures), ',':all_fearures}\n",
    "            # 因为没获得处理后的文件，所以我们在获取源文件后可以保存一下处理后的文件\n",
    "            pickle.dump((data_dict[sep]), open(FILTERED_DATA_DIR+filename+'.p', 'wb'))\n",
    "            return data_dict[sep]\n",
    "        except UnicodeDecodeError as e:\n",
    "            ''' 测试时经常会出现编码错误，如果尝试更换编码方式无效，可以将编码错误的部分位置重新复制粘贴就可以了，这里我们都默认UTF-8'''\n",
    "            print('UnicodeDecodeError:',e)\n",
    "        except pd.errors.ParserError as e:\n",
    "            print(\"connect error|pandas Error: %s\" % e)\n",
    "        \n",
    "    def filtrator(self,f_data):\n",
    "        \"\"\"\n",
    "        f_data:需要初步进行filter的数据\n",
    "        第一列的列名和最后一列的列名都带\"\n",
    "        \"\"\"\n",
    "        Nonetype_age = 0\n",
    "        f_data = f_data.rename(columns={f_data.columns[0]:f_data.columns[0][1:], f_data.columns[-1]:f_data.columns[-1][:-1]})\n",
    "        f_data[f_data.columns[0]] = f_data[f_data.columns[0]].map(lambda v:v[1:] if v!=None else Nonetype_age)\n",
    "        f_data[f_data.columns[-1]] = f_data[f_data.columns[-1]].map(lambda v:v[:-1] if v!=None else Nonetype_age)\n",
    "        try:\n",
    "            f_data = f_data[f_data['Location'].notnull()][f_data[f_data['Location'].notnull()]['Location'].str.contains('\\\";NULL')]\n",
    "            f_data['Location'] = f_data['Location'].map(lambda location:location[:-6])\n",
    "        except:\n",
    "            pass\n",
    "        return f_data\n",
    "    def get_features(self):\n",
    "        \"\"\"\n",
    "        获取整个数据集的所有features，并对每个文本字段做处理\n",
    "       User-ID:id2word\n",
    "        Location：id2list\n",
    "        ISBN：id2word\n",
    "        Title：id2text\n",
    "        Author：id2word\n",
    "        Year：id2word\n",
    "        Publisher：id2word\n",
    "        Blurb:id2text\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 从缓存的文件夹FILTERED_DATA_DIR获取features的文件\n",
    "            pickled_data = pickle.load(open(FILTERED_DATA_DIR+'features.p', mode='rb'))\n",
    "            return pickled_data\n",
    "        except FileNotFoundError:\n",
    "            #将所有的数据组成features大表\n",
    "            all_fearures = pd.merge(pd.merge(self.BX_Users,self.BX_Book_Ratings),self.books_with_blurbs)\n",
    "            #删除age列\n",
    "            all_fearures.pop('Age')\n",
    "            all_fearures['Title'] = self.feature2int(all_fearures['Title'], 'text', 15)\n",
    "            all_fearures['Blurb'] = self.feature2int(all_fearures['Blurb'], 'text', 200)\n",
    "            all_fearures['ISBN'] = self.feature2int(all_fearures['ISBN'], 'word')\n",
    "            all_fearures['Author'] = self.feature2int(all_fearures['Author'], 'word')\n",
    "            all_fearures['Publisher'] = self.feature2int(all_fearures['Publisher'], 'word')\n",
    "            all_fearures['User-ID'] = self.feature2int(all_fearures['User-ID'], 'word')\n",
    "            all_fearures['Year'] = self.feature2int(all_fearures['Year'], 'word')\n",
    "            all_fearures['Location'] = self.feature2int(all_fearures['Location'], 'list')\n",
    "            all_fearures['Book-Rating'] = all_fearures['Book-Rating'].astype('float32')\n",
    "            pickle.dump(all_fearures, open(FILTERED_DATA_DIR+'features.p', 'wb'))\n",
    "            return all_fearures\n",
    "    def feature2int(self, \n",
    "        feature:'特征值',\n",
    "        feature_type:'text/word/list',\n",
    "        length:'文本设置的最大长度' = 0,\n",
    "        ):\n",
    "        '''\n",
    "        将文本字段比如title、blurb只取英文单词，并用空格为分隔符，做成一个带index值的集合，并用index值表示各个单词，作为文本得表示\n",
    "        '''\n",
    "        pattern = re.compile(r'[^a-zA-Z]')\n",
    "#         filtered_map:匹配的一个规则\n",
    "        filtered_map = {val:re.sub(pattern, ' ', str(val)) for ii,val in enumerate(set(feature)) }\n",
    "#         1.letter_filter:非英文单词的用空格替换掉的一个匿名过滤的函数\n",
    "        letter_filter = lambda feature:feature.map({val:re.sub(pattern, ' ', str(val)) for ii,val in enumerate(set(feature)) })\n",
    "        text_words = set()\n",
    "        filtered_feature = letter_filter(feature)\n",
    "#         2.text_words:更新单词表set  \n",
    "        for val in filtered_feature.str.split():\n",
    "            text_words.update(val)\n",
    "        text_words.add('<PAD>')\n",
    "        text2int = {val:ii for ii, val in enumerate(text_words)}\n",
    "        \n",
    "        \n",
    "        \n",
    "        text_map = {val:[text2int[row] for row in filtered_map[val].split()][:length] for ii,val in enumerate(set(feature))}\n",
    "    #按照索引值进行插入，超出索引值直接插0即可\n",
    "        for key in text_map:\n",
    "            for cnt in range(length - len(text_map[key])):\n",
    "                text_map[key].insert(len(text_map[key]) + cnt,text2int['<PAD>'])     \n",
    "        word_map = {val:ii for ii,val in enumerate(set(feature))}\n",
    "\n",
    "        try:\n",
    "            cities = set()\n",
    "            for val in feature.str.split(','):\n",
    "                cities.update(val)\n",
    "            city_index = {val:ii for ii, val in enumerate(cities)}\n",
    "            list_map = {val:[city_index[row] for row in val.split(',')][:3] for ii,val in enumerate(set(feature))}\n",
    "        except AttributeError :\n",
    "            list_map = {}\n",
    "        feature_dict = {\n",
    "            'text':feature.map(text_map),\n",
    "            'word':feature.map(word_map),\n",
    "            'list':feature.map(list_map),\n",
    "            }\n",
    "        return feature_dict[feature_type]\n",
    "\n",
    "    def __del__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = DataLoad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length  = [len(i) for i in data.features['Blurb']]\n",
    "np.mean(np.array(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Location</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Year</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Blurb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15870</td>\n",
       "      <td>[6686, 1954, 7478]</td>\n",
       "      <td>18165</td>\n",
       "      <td>[695, 6732, 9860, 20158, 20158, 20158, 20158, ...</td>\n",
       "      <td>2760</td>\n",
       "      <td>63</td>\n",
       "      <td>1833</td>\n",
       "      <td>[28320, 119115, 852, 81179, 88419, 81651, 3727...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15870</td>\n",
       "      <td>[6686, 1954, 7478]</td>\n",
       "      <td>20412</td>\n",
       "      <td>[5675, 10475, 13197, 19978, 325, 7154, 16184, ...</td>\n",
       "      <td>12723</td>\n",
       "      <td>71</td>\n",
       "      <td>2585</td>\n",
       "      <td>[91516, 27939, 112885, 111528, 116700, 852, 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9513</td>\n",
       "      <td>[4926, 947, 7478]</td>\n",
       "      <td>20412</td>\n",
       "      <td>[5675, 10475, 13197, 19978, 325, 7154, 16184, ...</td>\n",
       "      <td>12723</td>\n",
       "      <td>71</td>\n",
       "      <td>2585</td>\n",
       "      <td>[91516, 27939, 112885, 111528, 116700, 852, 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26193</td>\n",
       "      <td>[5583, 6597, 5877]</td>\n",
       "      <td>20412</td>\n",
       "      <td>[5675, 10475, 13197, 19978, 325, 7154, 16184, ...</td>\n",
       "      <td>12723</td>\n",
       "      <td>71</td>\n",
       "      <td>2585</td>\n",
       "      <td>[91516, 27939, 112885, 111528, 116700, 852, 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18261</td>\n",
       "      <td>[2935, 7264, 5877]</td>\n",
       "      <td>20412</td>\n",
       "      <td>[5675, 10475, 13197, 19978, 325, 7154, 16184, ...</td>\n",
       "      <td>12723</td>\n",
       "      <td>71</td>\n",
       "      <td>2585</td>\n",
       "      <td>[91516, 27939, 112885, 111528, 116700, 852, 11...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID            Location   ISBN  \\\n",
       "0    15870  [6686, 1954, 7478]  18165   \n",
       "1    15870  [6686, 1954, 7478]  20412   \n",
       "2     9513   [4926, 947, 7478]  20412   \n",
       "3    26193  [5583, 6597, 5877]  20412   \n",
       "4    18261  [2935, 7264, 5877]  20412   \n",
       "\n",
       "                                               Title  Author  Year  Publisher  \\\n",
       "0  [695, 6732, 9860, 20158, 20158, 20158, 20158, ...    2760    63       1833   \n",
       "1  [5675, 10475, 13197, 19978, 325, 7154, 16184, ...   12723    71       2585   \n",
       "2  [5675, 10475, 13197, 19978, 325, 7154, 16184, ...   12723    71       2585   \n",
       "3  [5675, 10475, 13197, 19978, 325, 7154, 16184, ...   12723    71       2585   \n",
       "4  [5675, 10475, 13197, 19978, 325, 7154, 16184, ...   12723    71       2585   \n",
       "\n",
       "                                               Blurb  \n",
       "0  [28320, 119115, 852, 81179, 88419, 81651, 3727...  \n",
       "1  [91516, 27939, 112885, 111528, 116700, 852, 11...  \n",
       "2  [91516, 27939, 112885, 111528, 116700, 852, 11...  \n",
       "3  [91516, 27939, 112885, 111528, 116700, 852, 11...  \n",
       "4  [91516, 27939, 112885, 111528, 116700, 852, 11...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_user_id_number= 28836\n",
      "location_length=3,all_location_words_number=7573 \n",
      "all_isbn_words_number  38036\n",
      "title_length=15,all_title_words_number=23816\n",
      "all_author_words_number  15196\n",
      "all_year_words_number  81\n",
      "all_publisher_words_number  2909\n",
      "blurb_length=200,all_blurb_words_number=127186\n"
     ]
    }
   ],
   "source": [
    "#User-ID number,max_index\n",
    "all_user_id_number = len(set(data.features['User-ID']))\n",
    "print('all_user_id_number=',all_user_id_number)\n",
    "\n",
    "#Location number index\n",
    "location_length = len(data.features['Location'][0])\n",
    "all_location_words_number = max([j for i in data.features['Location'] for j in i])+1\n",
    "print('location_length=%d,all_location_words_number=%d '% (location_length,all_location_words_number) )\n",
    "\n",
    "#ISBN numner\n",
    "all_isbn_words_number = len(set(data.features['ISBN']))\n",
    "print('all_isbn_words_number ',all_isbn_words_number)\n",
    "\n",
    "#Title\n",
    "title_length = len(data.features['Title'][0])\n",
    "all_title_words_number = max([j for i in data.features['Title'] for j in i])+1\n",
    "print('title_length=%d,all_title_words_number=%d'% (title_length,all_title_words_number))\n",
    "\n",
    "#Author\n",
    "all_author_words_number = len(set(data.features['Author']))\n",
    "print('all_author_words_number ',all_author_words_number)\n",
    "\n",
    "#Year\n",
    "all_year_words_number = len(set(data.features['Year']))\n",
    "print('all_year_words_number ',all_year_words_number)\n",
    "\n",
    "# Publisher\n",
    "all_publisher_words_number = len(set(data.features['Publisher']))\n",
    "print('all_publisher_words_number ',all_publisher_words_number)\n",
    "\n",
    "# Blurb\n",
    "blurb_length = len(data.features['Blurb'][0])\n",
    "all_blurb_words_number = max([j for i in data.features['Blurb'] for j in i])+1\n",
    "print('blurb_length=%d,all_blurb_words_number=%d'% (blurb_length,all_blurb_words_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "超参数\n",
    "\"\"\"\n",
    "# Number of Epochs\n",
    "num_epochs = 5\n",
    "# Batch_size\n",
    "batch_size = 256\n",
    "# dropout_keep\n",
    "dropout_keep = 0.5\n",
    "# Learning_rate\n",
    "learning_rate = 0.001\n",
    "# embedding_dim\n",
    "embedding_dim = 16\n",
    "# 文本卷积滑动的单词个数\n",
    "window_sizes = {2,3,4,5}\n",
    "# filter num\n",
    "filter_num = 8\n",
    "# dense dim\n",
    "dense_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "#     用户特征输入占位符\n",
    "    # 用户特征输入\n",
    "    user_id = keras.layers.Input(shape=(1,), dtype='int32', name='user_id_input')\n",
    "    user_location = keras.layers.Input(shape=(3,), dtype='int32', name='user_location_input')\n",
    "    \n",
    "    # 书籍特征输入\n",
    "    book_isbn = keras.layers.Input(shape=(1,),  dtype='int32', name='book_isbn_input')\n",
    "    book_author = keras.layers.Input(shape=(1,),  dtype='int32', name='book_author_input')\n",
    "    book_year = keras.layers.Input(shape=(1,),  dtype='int32', name='book_year_input')\n",
    "    book_publisher = keras.layers.Input(shape=(1,),  dtype='int32', name='book_publisher_input')  \n",
    "    book_title = keras.layers.Input(shape=(15, ), dtype='int32', name='book_title_input')\n",
    "    book_blurb = keras.layers.Input(shape=(200, ), dtype='int32', name='book_blurb_input')\n",
    "    return user_id, user_location, book_isbn, book_author, book_year, book_publisher, book_title, book_blurb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "构建User神经网络\n",
    "\"\"\"\n",
    "def get_user_embedding(user_id,user_location):\n",
    "    uid_embed_layer = tf.keras.layers.Embedding(all_user_id_number,embedding_dim,input_length=1,name='uid_embed_layer')(user_id)\n",
    "    location_embed_layer = tf.keras.layers.Embedding(all_location_words_number,embedding_dim,input_length=location_length,name='location_embed_layer')(user_location)\n",
    "    return uid_embed_layer, location_embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_user_feature_layer(uid_embed_layer,location_embed_layer):\n",
    "#     第一层全连接\n",
    "    uid_fc_layer = tf.keras.layers.Dense(dense_dim,name='uid_fc_layer',activation='relu')(uid_embed_layer)\n",
    "    location_fc_layer = tf.keras.layers.Dense(dense_dim,name='location_fc_layer',activation='relu')(location_embed_layer)\n",
    "#  对location进行Encoder提取特征\n",
    "    location_gru_layer = tf.keras.layers.GRU(units=dense_dim,dropout=0.5,name='location_gru_layer')(location_fc_layer)\n",
    "#     [None,32]\n",
    "    print(location_gru_layer.shape)\n",
    "    location_gru_expand_layer = tf.expand_dims(location_gru_layer,axis=1)\n",
    "    \n",
    "#     第二层全连接\n",
    "    user_combine_layer = tf.keras.layers.concatenate([uid_fc_layer,location_gru_expand_layer],2)\n",
    "    user_dense_layer = tf.keras.layers.Dense(200,activation='tanh',name='user_dense_layer')(user_combine_layer)\n",
    "    user_dense_layer_flat = tf.keras.layers.Reshape([200], name=\"user_combine_layer_flat\")(user_dense_layer)\n",
    "    return user_dense_layer,user_dense_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "构建book神经网络\n",
    "\"\"\"\n",
    "def get_book_embedding(book_isbn, book_author, book_year, book_publisher, book_title, book_blurb):\n",
    "    book_isbn_embed_layer = tf.keras.layers.Embedding(all_isbn_words_number,embedding_dim,input_length = 1,name='book_isbn_embed_layer')(book_isbn)\n",
    "    book_author_embed_layer = tf.keras.layers.Embedding(all_author_words_number,embedding_dim,input_length=1,name='book_author_embed_layer')(book_author)\n",
    "    book_year_embed_layer = tf.keras.layers.Embedding(all_year_words_number,embedding_dim,input_length=1,name='book_year_embed_layer')(book_year)\n",
    "    book_publisher_embed_layer = tf.keras.layers.Embedding(all_publisher_words_number,embedding_dim,input_length = 1,name='book_publisher_embed_layer')(book_publisher)\n",
    "    book_title_embed_layer = tf.keras.layers.Embedding(all_title_words_number,embedding_dim,input_length=title_length,name='book_title_embed_layer')(book_title)\n",
    "    book_blurb_embed_layer = tf.keras.layers.Embedding(all_blurb_words_number,embedding_dim,input_length = blurb_length,name='book_blurb_embed_layer')(book_blurb)\n",
    "    return book_isbn_embed_layer,book_author_embed_layer,book_year_embed_layer,book_publisher_embed_layer,book_title_embed_layer,book_blurb_embed_layer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_book_feature_layer(book_isbn_embed_layer,book_author_embed_layer,book_year_embed_layer,book_publisher_embed_layer,book_title_embed_layer,book_blurb_embed_layer):\n",
    "#     对isbn,author,year,publisher第一层全连接\n",
    "    book_isbn_dense_layer = tf.keras.layers.Dense(dense_dim,activation='relu',name='book_isbn_dense_layer')(book_isbn_embed_layer)\n",
    "    book_author_dense_layer = tf.keras.layers.Dense(dense_dim,activation='relu',name='book_author_dense_layer')(book_author_embed_layer)\n",
    "    book_year_dense_layer = tf.keras.layers.Dense(dense_dim,activation='relu',name='book_year_dense_layer')(book_year_embed_layer)\n",
    "    book_publisher_dense_layer = tf.keras.layers.Dense(dense_dim,activation='relu',name='book_publisher_dense_layer')(book_publisher_embed_layer)\n",
    "    book_title_embed_layer_expand = tf.expand_dims(book_title_embed_layer,axis=-1)\n",
    "#     对title进行文本卷积\n",
    "#     book_title_embed_layer_expand:[None,15,16,1]\n",
    "#     对文本嵌入层使用不同的卷积核做卷积核最大池化\n",
    "    pool_layer_list = []\n",
    "    for window_size in window_sizes:\n",
    "        title_conv_layer = tf.keras.layers.Conv2D(filters = filter_num,kernel_size = (window_size,embedding_dim),strides=1,activation='relu')(book_title_embed_layer_expand)\n",
    "        title_maxpool_layer = tf.keras.layers.MaxPooling2D(pool_size=(title_length-window_size+1,1),strides=1)(title_conv_layer)\n",
    "        pool_layer_list.append(title_maxpool_layer)\n",
    "    pool_layer_layer = tf.keras.layers.concatenate(pool_layer_list,axis=-1,name='title_pool_layer')\n",
    "    max_num = len(window_sizes)*filter_num\n",
    "    pool_layer_flat = tf.keras.layers.Reshape([1,max_num],name='pool_layer_flat')(pool_layer_layer)\n",
    "    dropout_layer = tf.keras.layers.Dropout(dropout_keep, name = \"dropout_layer\")(pool_layer_flat)\n",
    "\n",
    "    # 对简介进行Encoder特征提取\n",
    "    book_blurb_dense_layer = tf.keras.layers.Dense(dense_dim,activation='relu',name='book_blurb_dense_layer')(book_blurb_embed_layer)\n",
    "    book_blurb_gru_layer = tf.keras.layers.GRU(units=dense_dim,dropout=0.5,name='book_blurb_gru_layer')(book_blurb_dense_layer)\n",
    "    print('book_blurb_gru_layer=',book_blurb_gru_layer.shape)\n",
    "    book_blurb_gru_expand_layer = tf.expand_dims(book_blurb_gru_layer,axis=1)\n",
    "    book_combine_layer = tf.keras.layers.concatenate([book_isbn_dense_layer,book_author_dense_layer,book_year_dense_layer,book_publisher_dense_layer,dropout_layer,book_blurb_gru_expand_layer],axis=-1)\n",
    "    book_dense_layer = tf.keras.layers.Dense(200, activation='tanh')(book_combine_layer)\n",
    "    book_dense_layer_flat = tf.keras.layers.Reshape([200], name=\"book_dense_layer_flat\")(book_dense_layer)\n",
    "    return book_dense_layer,book_dense_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rating(user_feature, book_feature):\n",
    "    multiply_layer = keras.layers.Lambda(lambda layer: tf.reduce_sum(layer[0]*layer[1],axis=1,keepdims=True), name = 'user_book_feature')((user_feature, book_feature))\n",
    "    print(multiply_layer.shape)\n",
    "    return multiply_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([[2,1,1],[1,3,4]])\n",
    "b = tf.constant([[0,1,1],[1,0,4]])\n",
    "c = inference = tf.keras.layers.Lambda(lambda layer:tf.reduce_sum(a * b, axis=1,keepdims=True))(a,b)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = data.features.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15870, list([6686, 1954, 7478]), 18165,\n",
       "       list([695, 6732, 9860, 20158, 20158, 20158, 20158, 20158, 20158, 20158, 20158, 20158, 20158, 20158, 20158]),\n",
       "       2760, 63, 1833,\n",
       "       list([28320, 119115, 852, 81179, 88419, 81651, 37277, 83273, 17012, 31610, 30921, 96926, 1787, 18503, 118504, 79542, 70919, 3237, 57064, 852, 33928, 7410, 1787, 47611, 115409, 852, 1542, 116700, 852, 31279, 82172, 101336, 116700, 90011, 96045, 120308, 81651, 87798, 71052, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162, 117162])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets=data.labels.values\n",
    "type(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get_Batch\n",
    "def get_batch(Xs,ys,batchsize):\n",
    "    for start in range(0,len(Xx,batchsize)):\n",
    "        end = min(start+batchsize,len(Xs))\n",
    "        yield xS[start:end],ys[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建计算图\n",
    "将用户特征和书籍特征作为输入，经过一个全连接，输出预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 5.],\n",
       "       [6., 5., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "a = np.array([[1, 2, 4, ([1, 2, 5])], \n",
    "             [3, 2, 6, ([6, 5, 1])],\n",
    "             [6, 9, 4, ([3, 7, 5])]])\n",
    "b = np.zeros([3,3])\n",
    "for i in range(3):\n",
    "    b[i] = a.take(3,1)[i]\n",
    "b[:2]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "dataset_length = len(features)\n",
    "\n",
    "def get_train_val_test():\n",
    "    location = np.zeros([dataset_length, 3])\n",
    "    title = np.zeros([dataset_length, 15])\n",
    "    blurb = np.zeros([dataset_length, 200])\n",
    "    for i in range(dataset_length):\n",
    "        location[i] = np.array(features[i, 1])\n",
    "        title[i] = np.array(features[i, 3])\n",
    "        blurb[i] = np.array(features[i, 7])\n",
    "    input_features = [features.take(0, 1).astype(np.float64),\n",
    "                      location,\n",
    "                      features.take(2, 1).astype(np.float64),\n",
    "                      title,\n",
    "                      features.take(4, 1).astype(np.float64),\n",
    "                      features.take(5, 1).astype(np.float64),\n",
    "                      features.take(6, 1).astype(np.float64),\n",
    "                      blurb]\n",
    "    for i in range(len(input_features)):\n",
    "        print(input_features[i].dtype)\n",
    "        print(type(input_features[i]))\n",
    "    labels = targets\n",
    "    #     分割数据集以及shuffle\n",
    "    np.random.seed(100)\n",
    "    number_features = len(input_features)\n",
    "    shuffle_index = np.random.permutation(dataset_length)\n",
    "    shuffle_train_index = shuffle_index[:math.ceil(dataset_length * 0.96)]\n",
    "    shuffle_val_index = shuffle_index[math.ceil(dataset_length * 0.96):math.ceil(dataset_length * 0.98)]\n",
    "    shuffle_test_index = shuffle_index[math.ceil(dataset_length * 0.98):]\n",
    "    train_features = [input_features[i][shuffle_train_index] for i in range(number_features)]\n",
    "    train_labels = labels[shuffle_train_index]\n",
    "    val_features = [input_features[i][shuffle_val_index] for i in range(number_features)]\n",
    "    val_lables = labels[shuffle_val_index]\n",
    "    test_features = [input_features[i][shuffle_test_index] for i in range(number_features)]\n",
    "    test_lables = labels[shuffle_test_index]\n",
    "    return train_features, train_labels, val_features, val_lables, test_features, test_lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "<class 'numpy.ndarray'>\n",
      "float64\n",
      "<class 'numpy.ndarray'>\n",
      "float64\n",
      "<class 'numpy.ndarray'>\n",
      "float64\n",
      "<class 'numpy.ndarray'>\n",
      "float64\n",
      "<class 'numpy.ndarray'>\n",
      "float64\n",
      "<class 'numpy.ndarray'>\n",
      "float64\n",
      "<class 'numpy.ndarray'>\n",
      "float64\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "(3442,)\n",
      "(3442,)\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels, val_features, val_lables, test_features, test_lables = get_train_val_test()\n",
    "print(type(train_features))\n",
    "print(type(train_labels))\n",
    "print(val_features[0].shape)\n",
    "print(test_features[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class mv_network(object):\n",
    "    def __init__(self, batch_size,epoch):\n",
    "        self.batchsize = 256\n",
    "        self.epoch = 5\n",
    "         # 获取输入占位符\n",
    "        user_id, user_location, book_isbn, book_author, book_year, book_publisher, book_title, book_blurb = get_inputs()\n",
    "        # 获取User的2个嵌入向量\n",
    "        uid_embed_layer, location_embed_layer = get_user_embedding(user_id,user_location)\n",
    "        # 得到用户特征\n",
    "        user_dense_layer,user_dense_layer_flat =get_user_feature_layer(uid_embed_layer,location_embed_layer)\n",
    "        # 获取书籍的嵌入向量\n",
    "        book_isbn_embed_layer,book_author_embed_layer,book_year_embed_layer,book_publisher_embed_layer,book_title_embed_layer,book_blurb_embed_layer=get_book_embedding(book_isbn, book_author, book_year, book_publisher, book_title, book_blurb)\n",
    "        # 获取书籍特征\n",
    "        book_dense_layer,book_dense_layer_flat=get_book_feature_layer(book_isbn_embed_layer,book_author_embed_layer,book_year_embed_layer,book_publisher_embed_layer,book_title_embed_layer,book_blurb_embed_layer)\n",
    "        \n",
    "        # 计算出评分\n",
    "        # 将用户特征和电影特征做矩阵乘法得到一个预测评分的方案\n",
    "        print(\"user_dense_layer_flat=\",user_dense_layer_flat.shape)\n",
    "        print(\"book_dense_layer_flat=\",book_dense_layer_flat.shape)\n",
    "        inference = get_rating(user_dense_layer_flat,book_dense_layer_flat)\n",
    "\n",
    "        # 将用户特征和电影特征作为输入，经过全连接，输出一个值的方案\n",
    "        #         inference_layer = tf.keras.layers.concatenate([user_combine_layer_flat, movie_combine_layer_flat],\n",
    "        #                                                       1)  # (?, 400)\n",
    "        # 你可以使用下面这个全连接层，试试效果\n",
    "        # inference_dense = tf.keras.layers.Dense(64, kernel_regularizer=tf.nn.l2_loss, activation='relu')(\n",
    "        #    inference_layer)\n",
    "        #         inference = tf.keras.layers.Dense(1, name=\"inference\")(inference_layer)  # inference_dense\n",
    "\n",
    "        self.model = tf.keras.Model(\n",
    "            inputs=[user_id, user_location, book_isbn, book_title, book_author, book_year, book_publisher, book_blurb],\n",
    "            outputs=[inference])\n",
    "        self.model.summary()\n",
    "    def train_model(self):\n",
    "        model_optimizer = tf.keras.optimizers.Adam()\n",
    "        self.model.compile(optimizer=model_optimizer, loss=keras.losses.mse)\n",
    "        history = self.model.fit(train_features, train_labels, validation_data=(val_features, val_lables), epochs=self.epoch, batch_size=self.batchsize, verbose=1)\n",
    "        return  history\n",
    "    def predict_model(self, model):\n",
    "        test_loss = self.model.evaluate(test_features, test_lables, verbose=0)\n",
    "        return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 32)\n",
      "book_blurb_gru_layer= (None, 32)\n",
      "user_dense_layer_flat= (None, 200)\n",
      "book_dense_layer_flat= (None, 200)\n",
      "(None, 1)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "book_title_input (InputLayer)   [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "book_title_embed_layer (Embeddi (None, 15, 16)       381056      book_title_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_1 (Tenso [(None, 15, 16, 1)]  0           book_title_embed_layer[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "user_location_input (InputLayer [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 14, 1, 8)     264         tf_op_layer_ExpandDims_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 13, 1, 8)     392         tf_op_layer_ExpandDims_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 12, 1, 8)     520         tf_op_layer_ExpandDims_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 11, 1, 8)     648         tf_op_layer_ExpandDims_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "book_blurb_input (InputLayer)   [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "location_embed_layer (Embedding (None, 3, 16)        121168      user_location_input[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 1, 1, 8)      0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 8)      0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 8)      0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 8)      0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "book_blurb_embed_layer (Embeddi (None, 200, 16)      2034976     book_blurb_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "user_id_input (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "location_fc_layer (Dense)       (None, 3, 32)        544         location_embed_layer[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "book_isbn_input (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "book_author_input (InputLayer)  [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "book_year_input (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "book_publisher_input (InputLaye [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_pool_layer (Concatenate)  (None, 1, 1, 32)     0           max_pooling2d[0][0]              \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "book_blurb_dense_layer (Dense)  (None, 200, 32)      544         book_blurb_embed_layer[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "uid_embed_layer (Embedding)     (None, 1, 16)        461376      user_id_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "location_gru_layer (GRU)        (None, 32)           6336        location_fc_layer[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "book_isbn_embed_layer (Embeddin (None, 1, 16)        608576      book_isbn_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "book_author_embed_layer (Embedd (None, 1, 16)        243136      book_author_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "book_year_embed_layer (Embeddin (None, 1, 16)        1296        book_year_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "book_publisher_embed_layer (Emb (None, 1, 16)        46544       book_publisher_input[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool_layer_flat (Reshape)       (None, 1, 32)        0           title_pool_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "book_blurb_gru_layer (GRU)      (None, 32)           6336        book_blurb_dense_layer[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "uid_fc_layer (Dense)            (None, 1, 32)        544         uid_embed_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims (TensorF [(None, 1, 32)]      0           location_gru_layer[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "book_isbn_dense_layer (Dense)   (None, 1, 32)        544         book_isbn_embed_layer[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "book_author_dense_layer (Dense) (None, 1, 32)        544         book_author_embed_layer[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "book_year_dense_layer (Dense)   (None, 1, 32)        544         book_year_embed_layer[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "book_publisher_dense_layer (Den (None, 1, 32)        544         book_publisher_embed_layer[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_layer (Dropout)         (None, 1, 32)        0           pool_layer_flat[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_2 (Tenso [(None, 1, 32)]      0           book_blurb_gru_layer[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1, 64)        0           uid_fc_layer[0][0]               \n",
      "                                                                 tf_op_layer_ExpandDims[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 192)       0           book_isbn_dense_layer[0][0]      \n",
      "                                                                 book_author_dense_layer[0][0]    \n",
      "                                                                 book_year_dense_layer[0][0]      \n",
      "                                                                 book_publisher_dense_layer[0][0] \n",
      "                                                                 dropout_layer[0][0]              \n",
      "                                                                 tf_op_layer_ExpandDims_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "user_dense_layer (Dense)        (None, 1, 200)       13000       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1, 200)       38600       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "user_combine_layer_flat (Reshap (None, 200)          0           user_dense_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "book_dense_layer_flat (Reshape) (None, 200)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "user_book_feature (Lambda)      (None, 1)            0           user_combine_layer_flat[0][0]    \n",
      "                                                                 book_dense_layer_flat[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 3,968,032\n",
      "Trainable params: 3,968,032\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 165218 samples, validate on 3442 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165218/165218 [==============================] - 343s 2ms/sample - loss: 13.0151 - val_loss: 11.5620\n",
      "Epoch 2/5\n",
      "165218/165218 [==============================] - 321s 2ms/sample - loss: 10.4039 - val_loss: 11.7905\n",
      "Epoch 3/5\n",
      "165218/165218 [==============================] - 319s 2ms/sample - loss: 8.9847 - val_loss: 12.4061\n",
      "Epoch 4/5\n",
      "165218/165218 [==============================] - 314s 2ms/sample - loss: 7.9888 - val_loss: 12.7829\n",
      "Epoch 5/5\n",
      "165218/165218 [==============================] - 320s 2ms/sample - loss: 7.2222 - val_loss: 13.6335\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl0lNX9x/H3FxITQBTECLIoWFlUoqBxRdw1iCiLCu5A\nPVJRFC1SEXfUuqFdFKFYEX+KKIIU6woiSrFIDZS1LBEVCC4EUIRiZLu/P26wCGSbmcydmXxe53CS\nTCbzfOAkH57c5z73mnMOERFJftVCBxARkdhQoYuIpAgVuohIilChi4ikCBW6iEiKUKGLiKQIFbqI\nSIpQoYuIpAgVuohIikiL58EOPPBA17Rp03geUkQk6c2ePXutcy6rrOfFtdCbNm1KXl5ePA8pIpL0\nzGxFeZ6nIRcRkRShQhcRSREqdBGRFBHXMXQRST1bt26loKCAoqKi0FGSXmZmJo0bNyY9PT2ir1eh\ni0hUCgoKqF27Nk2bNsXMQsdJWs451q1bR0FBAc2aNYvoNTTkIiJRKSoqol69eirzKJkZ9erVi+o3\nHRW6iERNZR4b0f47qtBTyPz5MGQI7NgROomIhKBCTxFffw2dOsFf/gLr1oVOIyIhqNBTwObN0Lmz\nL/K//x2yyrxBWCR1fP/99zzzzDMV/rqOHTvy/fffV/jrevXqxfjx4yv8dfGgQk9yO3bANddAXh68\n/DIce2zoRCLxVVKhb9u2rdSve/vtt6lTp05lxQpC0xaT3J13woQJ8MQT/ixdJKhbboG5c2P7mm3a\nwB//WOKnBw0axPLly2nTpg3p6elkZmZSt25dlixZwrJly+jSpQurVq2iqKiI/v3706dPH+B/a0tt\n2rSJ888/n1NPPZV//vOfNGrUiEmTJlGjRo0yo02dOpXbbruNbdu2cfzxxzN8+HAyMjIYNGgQb7zx\nBmlpaZx33nkMHTqU1157jfvvv5/q1auz//77M3369Jj9E+2kQk9io0bBI4/Ab34Dt94aOo1IGI88\n8ggLFy5k7ty5fPjhh1xwwQUsXLjw57nco0aN4oADDuDHH3/k+OOP5+KLL6ZevXq/eI38/HzGjh3L\ns88+S/fu3ZkwYQJXXXVVqcctKiqiV69eTJ06lRYtWnDNNdcwfPhwrr76aiZOnMiSJUsws5+HdYYM\nGcJ7771Ho0aNIhrqKQ8VepKaNs0X+bnnwlNPgWaNSUIo5Uw6Xk444YRf3Jjz5z//mYkTJwKwatUq\n8vPz9yj0Zs2a0aZNGwCOO+44vvzyyzKPs3TpUpo1a0aLFi0A6NmzJ8OGDaNfv35kZmZy7bXX0qlT\nJzp16gRAu3bt6NWrF927d6dbt26x+KvuQWPoSWjpUujWDVq0gHHjIMK7hEVSUq1atX5+/8MPP+T9\n999n5syZzJs3j7Zt2+71xp2MjIyf369evXqZ4++lSUtL41//+heXXHIJb775Jh06dABgxIgRPPjg\ng6xatYrjjjuOdZUwHU1n6Elm7Vq44AJf4m++CSl2TUekwmrXrs3GjRv3+rkNGzZQt25datasyZIl\nS/jkk09idtyWLVvy5Zdf8tlnn3H44Yfz4osvcvrpp7Np0yY2b95Mx44dadeuHYcddhgAy5cv58QT\nT+TEE0/knXfeYdWqVXv8phAtFXoS+ekn6NoVCgr8kEuEyz2IpJR69erRrl07WrduTY0aNahfv/7P\nn+vQoQMjRozgiCOOoGXLlpx00kkxO25mZibPP/88l1566c8XRa+//nrWr19P586dKSoqwjnHk08+\nCcDAgQPJz8/HOcfZZ5/NMcccE7MsO5lzLuYvWpKcnBynHYsi45yfnvjSS/DKK9CjR+hEIt7ixYs5\n4ogjQsdIGXv79zSz2c65nLK+VmPoSeKhh3yZP/CAylxE9k5DLknglVfg7rvh6qv9vHMRqXw33ngj\nH3/88S8e69+/P7179w6UqGwq9AQ3cyb06gXt28Ozz2p6oki8DBs2LHSECtOQSwL74gt/92eTJjBx\nIuwys0pEZA8q9AT1/fd+euK2bX56YoxnN4lICtKQSwLauhW6d4f8fJg8GVq2DJ1IRJJBmWfoZjbK\nzNaY2cJdHnvAzOab2Vwzm2xmDSs3ZtXhHNx0E0yZAiNHwplnhk4kIsmiPEMuo4EOuz32uHPuaOdc\nG+BN4J5YB6uq/vAHv0nFoEGQwBfTRZLWvvvuW+LnvvzyS1q3bh3HNLFVZqE756YD63d77IddPqwF\nlO/upEpaYSxVvPEG3HYbXHyxn3cuIlIREY+hm9lDwDXABqB8AwPffBPp4VLenDlw+eWQkwP/939Q\nTZerJQkFWA6dQYMG0aRJE2688UYA7rvvPtLS0pg2bRrfffcdW7du5cEHH6RzBTcMKCoqom/fvuTl\n5ZGWlsaTTz7JmWeeyaJFi+jduzdbtmxhx44dTJgwgYYNG9K9e3cKCgrYvn07d999Nz0C3AEYcW04\n5+50zjUBxgD9SnqemfUxszwzy+O//4X160t6apW1ejVceCEceKA/S69ZM3QikeTRo0cPxo0b9/PH\n48aNo2fPnkycOJE5c+Ywbdo0BgwYQEWXORk2bBhmxoIFCxg7diw9e/akqKiIESNG0L9/f+bOnUte\nXh6NGzfm3XffpWHDhsybN4+FCxf+vMJivMVilssY4G3g3r190jk3EhgJkGPmeP99P4VDANi0yZf5\nxo3w8cfQoEHoRCKRC7Ecetu2bVmzZg1fffUVhYWF1K1blwYNGnDrrbcyffp0qlWrxurVq/n2229p\nUIEfsBkzZnDTTTcB0KpVKw499FCWLVvGySefzEMPPURBQQHdunWjefPmZGdnM2DAAG6//XY6depE\n+/btK+uvW6qIztDNrPkuH3YGlpTrC6tXh/fei+SQKWn7drjySpg3D159FbKzQycSSU6XXnop48eP\n59VXX6VHjx6MGTOGwsJCZs+ezdy5c6lfv/5e10GPxBVXXMEbb7xBjRo16NixIx988AEtWrRgzpw5\nZGdnc9dddzFkyJCYHKuiyjxDN7OxwBnAgWZWgD8T72hmLYEdwArg+nIdrXZtX+jO6R524He/80Ms\nTz8N558fOo1I8urRowfXXXcda9eu5aOPPmLcuHEcdNBBpKenM23aNFasWFHh12zfvj1jxozhrLPO\nYtmyZaxcuZKWLVvy+eefc9hhh3HzzTezcuVK5s+fT6tWrTjggAO46qqrqFOnDn/9618r4W9ZtjIL\n3Tl3+V4efi6io+2/P6xYAf/5Dxx1VEQvkSpGjIAnn4Sbb4biazkiEqGjjjqKjRs30qhRIw4++GCu\nvPJKLrzwQrKzs8nJyaFVq1YVfs0bbriBvn37kp2dTVpaGqNHjyYjI4Nx48bx4osvkp6eToMGDRg8\neDCffvopAwcOpFq1aqSnpzN8+PBK+FuWLb7roR99tMtbsMBvUf/b38btuInm3XehUyfo0AEmTfIj\nUSLJSuuhx1byrIe+zz7QqlWVHUfPz4crroCOHf0vKGPHqsxFJHbiv5ZLbq6/FfLHH6FGjbgfPoSV\nK/3GFM8/71dMvP12/6d27dDJRKqmBQsWcPXVV//isYyMDGbNmhUoUWyEKfQ//QmmT/fvp7Bvv4Xf\n/96Pl4MfK7/jDk1NlNTjnMOSaKJDdnY2c2N9B1QMRDsEHv/7EU8/3Z+mpvCwy/r1vrgPOwyGDfN7\ngebn+//HVOaSajIzM1m3bl3UZVTVOedYt24dmZmZEb9G/M/Qa9b02++kYKFv3OhLe+hQ+OEHuOwy\nuP9+aN687K8VSVaNGzemoKCAwsLC0FGSXmZmJo0bN47468Osh56bCwMHwqpVfjueJPfjjzB8ODz8\nMKxd63cZeuAB3SgkVUN6ejrNmjULHUMItWPRzrHzyZODHD5Wtm7113ebN4cBA/wiQp98An/7m8pc\nROIvTKG3bg0NGybtsMv27fDii34G5vXXwyGHwAcf+E0pTjwxdDoRqarCFLoZnHcevP++b8ck4RxM\nmABHH+0vdO63n9/v8+OPtbOQiIQXbtXt3Fz47jv49NNgEcrLOX935/HHwyWXwI4dMG4czJ7tN3JO\notlaIpLCwhX6uef6JkzwcfTp0+G00/ziWevW+ZuDFiyASy/VJhQikljCVVK9en57ngQdR//6a1/i\np58Oy5f7+eRLl0KvXpAWZm6QiEipwp5j5ubCrFkJt9fovHn+4ub06fDYY/DZZ3DDDX4pGhGRRBW+\n0Ldvh6lTg8bY1Vtvwamn+nHyGTP8dHltCSciySBsoZ94op8qkiDDLk89BRdd5OeVz5oFbduGTiQi\nUn5hCz09Hc4++3+7GAWybRv06+c3m7jwQvjHP6BRo2BxREQiEn6eRm6uX1926dIgh//hB1/iw4bB\nbbf5eea1agWJIiISlcQodAgy7LJiBbRr5+/w/Mtf4PHHteGEiCSv8IXetCm0aBH3Qp81yw/hr1rl\nbxrq0yeuhxcRibnwhQ7+LP3DD6GoKC6He+01OOMMP3tl5kw455y4HFZEpFIlTqH/+KOfJ1iJnPNL\n3HbvDsce68/StbetiKSKMgvdzEaZ2RozW7jLY4+b2RIzm29mE82sTlQpzjjD37VTicMuW7ZA794w\neLDfqHnqVMjKqrTDiYjEXXnO0EcDHXZ7bArQ2jl3NLAMuCOqFLVq+bt5KqnQ163zizu+8ALcdx+8\n9BJEscuTiEhCKrPQnXPTgfW7PTbZObet+MNPgMj3TNopN9evevXVV1G/1K7y8+Hkk/1Y+Usvwb33\nanVEEUlNsRhD/zXwTtSvUgm7GH30EZx0kl+l94MP4MorY/bSIiIJJ6pCN7M7gW3AmFKe08fM8sws\nr9RNZI8+Gho0iNmwy+jRfoXegw7yFz/btYvJy4qIJKyIC93MegGdgCudK/m+fefcSOdcjnMuJ6u0\nq5A7dzGaMiWqXYx27PAXPnv39uuYz5wJhx0W8cuJiCSNiArdzDoAvwMucs5tjlma3Fx/BXPOnIi+\nfPNm6NHDT0287jp45x2oE938GxGRpFGeaYtjgZlASzMrMLNrgaeB2sAUM5trZiNikmbnHT4RDLt8\n842f/ThhAgwd6m/lT0+PSSoRkaRgpYyWxFxOTo7Ly8sr/UnHHedv4fzHP8r9ugsWQKdOsHYtjBkD\nXbpEGVREJIGY2WznXE5Zz0uMO0V3lZvrB743bCj1ac7BokXw4IP+gue2bf7/AJW5iFRViVno27f7\neYa72bEDPvkEbr8dWraE1q3h7rv91qSzZvnb+UVEqqrE2+745JNh3339OHrXrmzZ4tftmjgRJk3y\nmzenpcGZZ8Ktt0LnztCwYejQIiLhJV6h77MPm07ryLuvV+Nvmxxvvmls2OCH1c8/H7p2hY4doW7d\n0EFFRBJLwhT62rXw97/7M/EpU8ZQtDWNem9tp1u36nTp4m8SqlEjdEoRkcQVtNBXrvQFPnGiv6C5\nYwcccgj0uWITXV/owqn3XkLaLf1CRhQRSRpxL/RFi/5X4jvvHzrqKH93Z9eu0LYtmNWBGQXw/rug\nQhcRKZe4FvrChX5mCvhrn4895qcZNm++lyfn5voFWX76CTIy4hlTRCQpxXXaYkYGDB/uV8j95z9h\n4MASyhx8oW/eDB9/HM+IIiJJK66F3rw5XH89HHxwOZ585pl+fmKcN48WEUlWiXdj0U61a/tbQFXo\nIiLlkriFDn7YZd48v/KWiIiUKvELHWK6i5GISKpK7EJv0waysjTsIiJSDold6NWq+V2MJk/2dx2J\niEiJErvQwQ+7rF0Lc+eGTiIiktASv9DPO8+/1bCLiEipEr/Q69f3Y+kqdBGRUiV+oYMfdvn4Y9i4\nMXQSEZGElTyFvm0bTJsWOomISMJKjkJv1w5q1dKwi4hIKZKj0PfZx6/tokIXESlRmYVuZqPMbI2Z\nLdzlsUvNbJGZ7TCznMqNWCw3F5Yv939ERGQP5TlDHw102O2xhUA3YHqsA5Vo5zIAOksXEdmrMgvd\nOTcdWL/bY4udc0srLdXeHH44NGumQhcRKUGlj6GbWR8zyzOzvMLCwmheyJ+lf/ABbNkSu4AiIimi\n0gvdOTfSOZfjnMvJysqK7sVyc2HTJpg5MzbhRERSSHLMctnprLO0i5GISAmSq9D328/vLq1CFxHZ\nQ3mmLY4FZgItzazAzK41s65mVgCcDLxlZvFr2NxcmDMH1qyJ2yFFRJJBeWa5XO6cO9g5l+6ca+yc\ne845N7H4/QznXH3nXG48wgL/m744ZUrcDikikgySa8gF4NhjoV49DbuIiOwm+Qq9WjU491ztYiQi\nspvkK3Twwy7ffgvz54dOIiKSMJKz0LWLkYjIHpKz0Bs2hOxsFbqIyC6Ss9DBD7vMmOHvHBURkSQv\n9K1b4cMPQycREUkIyVvop54KNWpo2EVEpFjyFnpmJpxxhgpdRKRY8hY6+GGX/Hz44ovQSUREgkv+\nQgedpYuIkOyF3rIlHHKICl1EhGQv9J27GE2dCv/9b+g0IiJBJXehA/TqBRs3wjPPhE4iIhJU8hf6\nKaf4s/RHH/XFLiJSRSV/oQMMGQLr1sFTT4VOIiISTGoU+gknQKdOMHQobNgQOo2ISBCpUejgz9K/\n+w7+8IfQSUREgkidQm/bFrp184W+fn3oNCIicZc6hQ5w333+wugTT4ROIiISd6lV6NnZ0L07/OlP\nsHZt6DQiInGVWoUOcO+98OOP8NhjoZOIiMRVmYVuZqPMbI2ZLdzlsQPMbIqZ5Re/rVu5MSvgiCPg\niivg6afhm29CpxERiZvynKGPBjrs9tggYKpzrjkwtfjjxHHPPbBli7/ZSESkiiiz0J1z04Hdp410\nBl4ofv8FoEuMc0WneXO45hoYPhxWrw6dRkQkLiIdQ6/vnPu6+P1vgPolPdHM+phZnpnlFRYWRni4\nCNx9N2zfDg8/HL9jiogEFPVFUeecA1wpnx/pnMtxzuVkZWVFe7jya9YMfv1rePZZWLkyfscVEQkk\n0kL/1swOBih+uyZ2kWLozjv92wcfDJtDRCQOIi30N4Cexe/3BCbFJk6MHXIIXHcdPP88fP556DQi\nIpWqPNMWxwIzgZZmVmBm1wKPAOeaWT5wTvHHiWnwYEhLgwceCJ1ERKRSpZX1BOfc5SV86uwYZ6kc\nDRtC377+7tHBg/0MGBGRFJR6d4ruze23Q2Ym3H9/6CQiIpWmahR6/frQrx+8/DIsXhw6jYhIpaga\nhQ4wcCDUquVXZBQRSUFVp9APPBD694dx42D+/NBpRERiruoUOsCAAbDffjpLF5GUVLUKvW5d+O1v\nYeJEmDMndBoRkZiqWoUOcMstvtjvvTd0EhGRmKp6hb7//nDbbfDmmzBrVug0IiIxU/UKHeDmm/1F\n0nvuCZ1ERCRmqmah77uvv9lo8mSYMSN0GhGRmKiahQ5www3+hiOdpYtIiqi6hV6zJtxxB0yb5v+I\niCS5qlvoAL/5jV+86557wJW4R4eISFKo2oWemek3wZgxA6ZMCZ1GRCQqVbvQAa691m+EobN0EUly\nKvSMDLjrLj8n/e23Q6cREYmYCh2gVy+/qbTO0kUkianQAdLTfZnPmQOTEnN7VBGRsqjQd7rqKmjR\nwq/xsmNH6DQiIhWmQt8pLc2X+fz5MGFC6DQiIhWmQt9Vjx5w5JG+2LdvD51GRKRCoip0M+tvZgvN\nbJGZ3RKrUMFUr+43v1i8GF59NXQaEZEKibjQzaw1cB1wAnAM0MnMDo9VsGAuvhiOPtoX+7ZtodOI\niJRbNGfoRwCznHObnXPbgI+AbrGJFVC1anD//ZCfD2PGhE4jIlJu0RT6QqC9mdUzs5pAR6BJbGIF\n1rkzHHssDBkCW7eGTiMiUi4RF7pzbjHwKDAZeBeYC+xxJdHM+phZnpnlFRYWRhw0rsx8mX/+OTzx\nROg0IiLlEtVFUefcc86545xzpwHfAcv28pyRzrkc51xOVlZWNIeLr44d4dJL4e674V//Cp1GRKRM\n0c5yOaj47SH48fOXYxEqIZjByJHQqBFcfjn88EPoRCIipYp2HvoEM/sP8HfgRufc9zHIlDjq1IGX\nX4YVK6BvX63zIiIJLdohl/bOuSOdc8c456bGKlRCOeUUP4Xx5ZfhxRdDpxERKZHuFC2PO+6A00/3\n+5Au2+MygYhIQlChl0f16vDSS37t9Msug59+Cp1IRGQPKvTyatwYRo2Cf/8bBg8OnUZEZA8q9Iro\n3BluvBGefBLeeSd0GhGRX1ChV9Tjj0N2NvTsCd98EzqNiMjPVOgVVaMGvPIKbNoE11yjzTBEJGGo\n0CNx5JHwxz/ClClaGkBEEoYKPVLXXeeX2h08GD79NHQaEREVesTM4NlnoWFDLQ0gIglBhR6NunX9\nHaRffOFnv4iIBKRCj1a7dn5pgJde0tIAIhKUCj0WBg+G007zSwPk54dOIyJVlAo9FqpX99vV7bOP\nH0/fsiV0IhGpglTosdK4MTz3HMyeraUBRCQIFXosdenih12eeALefTd0GhGpYlTosTZ0KLRuraUB\nRCTuVOixtnNpgB9+8KWupQFEJE5U6JXhqKP80gCTJ/uVGUVE4kCFXln69IFu3fxuR1oaQETiQIVe\nWXYuDXDwwX4q48aNoROJSIpToVemAw7Q0gAiEjcq9Mp26qlw771+WQAtDSAilSiqQjezW81skZkt\nNLOxZpYZq2Ap5c47/7c0wGefhU4jIikq4kI3s0bAzUCOc641UB24LFbBUkr16n7xrvR0LQ0gIpUm\n2iGXNKCGmaUBNYGvoo+Uopo08UsD5OXBXXeFTiMiKSjiQnfOrQaGAiuBr4ENzrnJsQqWkrp2hb59\n/UbTWhpARGIsmiGXukBnoBnQEKhlZlft5Xl9zCzPzPIKCwsjT5oqnngCsrOhe3d/ti4iEiPRDLmc\nA3zhnCt0zm0FXgdO2f1JzrmRzrkc51xOVlZWFIdLETVqwDvvQL16kJsLCxeGTiQiKSKaQl8JnGRm\nNc3MgLOBxbGJleIaNYL334eMDDj3XM18EZGYiGYMfRYwHpgDLCh+rZExypX6fvUrmDIFtm6Fc86B\ngoLQiUQkyUU1y8U5d69zrpVzrrVz7mrn3E+xClYlHHUUvPcerF/vS33NmtCJRCSJ6U7R0I47Dt56\nC1au9GPq338fOpGIJCkVeiJo3x5efx0WLYKOHWHTptCJRCQJqdATRYcOMHYszJrlt7IrKgqdSESS\njAo9kVx8MYwaBVOnwmWX+QumIiLlpEJPND17wlNPwaRJ0Lu3trATkXJLCx1A9qJfP78n6Z13Qu3a\n8MwzfsMMEZFSqNAT1R13+FJ/9FFf6o8+qlIXkVKp0BOVGTz8sC/1xx+H/ff3Z+wiIiVQoScyM3j6\naT+N8a67YL/94KabQqcSkQSlQk901ar5mS8bN8LNN/vhl169QqcSkQSkWS7JIC0NXnnFL+R17bUw\nfnzoRCKSgFToySIjAyZOhJNOgiuu0AYZIrIHFXoyqVXLr/vSujV06wbTp4dOJCIJRIWebOrU8Ss0\nHnoodOqkXY9E5Gcq9GSUleU3yNCuRyKyCxV6stp916Ply0MnEpHAVOjJTLseicguVOjJbueuR+vW\n+TN17XokUmWp0FPBzl2PVqzQrkciVZgKPVXs3PWoVSuoUSN0GhEJQLf+p5IOHfwfEamSdIYuIpIi\nIi50M2tpZnN3+fODmd0Sy3AiIlJ+EQ+5OOeWAm0AzKw6sBqYGKNcIiJSQbEacjkbWO6cWxGj1xMR\nkQqKVaFfBoyN0WuJiEgEoi50M9sHuAh4rYTP9zGzPDPLKywsjPZwIiJSglicoZ8PzHHOfbu3Tzrn\nRjrncpxzOVlZWTE4nIiI7E0sCv1yNNwiIhKcOeci/2KzWsBK4DDn3IZyPH8jsDTiA4ZxILA2dIgK\nSLa8oMzxkGx5IfkyV2beQ51zZQ5xRFXoFWVmec65nLgdMAaSLXOy5QVljodkywvJlzkR8upOURGR\nFKFCFxFJEfEu9JFxPl4sJFvmZMsLyhwPyZYXki9z8LxxHUMXEZHKoyEXEZEUEZdCN7MOZrbUzD4z\ns0HxOGY0zKyJmU0zs/+Y2SIz6x86U3mYWXUz+7eZvRk6S3mYWR0zG29mS8xssZmdHDpTWczs1uLv\niYVmNtbMMkNn2p2ZjTKzNWa2cJfHDjCzKWaWX/y2bsiMuysh8+PF3xvzzWyimdUJmXFXe8u7y+cG\nmJkzswPjnavSC714JcZh+DtKjwQuN7MjK/u4UdoGDHDOHQmcBNyYBJkB+gOLQ4eogD8B7zrnWgHH\nkODZzawRcDOQ45xrDVTHr2OUaEYDu+90MgiY6pxrDkwt/jiRjGbPzFOA1s65o4FlwB3xDlWK0eyZ\nFzNrApyHvz8n7uJxhn4C8Jlz7nPn3BbgFaBzHI4bMefc1865OcXvb8QXTaOwqUpnZo2BC4C/hs5S\nHma2P3Aa8ByAc26Lcy4ZNkNNA2qYWRpQE/gqcJ49OOemA+t3e7gz8ELx+y8AXeIaqgx7y+ycm+yc\n21b84SdA47gHK0EJ/8YAfwB+BwS5OBmPQm8ErNrl4wISvBx3ZWZNgbbArLBJyvRH/DfSjtBByqkZ\nUAg8XzxM9NfiO48TlnNuNTAUf/b1NbDBOTc5bKpyq++c+7r4/W+A+iHDRODXwDuhQ5TGzDoDq51z\n80Jl0EXRUpjZvsAE4Bbn3A+h85TEzDoBa5xzs0NnqYA04FhguHOuLfBfEm8Y4BeKx5074/8zagjU\nMrOrwqaqOOentiXN9DYzuxM/DDomdJaSmFlNYDBwT8gc8Sj01UCTXT5uXPxYQjOzdHyZj3HOvR46\nTxnaAReZ2Zf4Ia2zzOylsJHKVAAUOOd2/uYzHl/wiewc4AvnXKFzbivwOnBK4Ezl9a2ZHQxQ/HZN\n4DzlYma9gE7AlS6x51j/Cv8f/bzin8PGwBwzaxDPEPEo9E+B5mbWrHjt9MuAN+Jw3IiZmeHHdhc7\n554Mnacszrk7nHONnXNN8f++HzjnEvrM0Tn3DbDKzFoWP3Q28J+AkcpjJXCSmdUs/h45mwS/kLuL\nN4Cexe/3BCYFzFIuZtYBP4x4kXNuc+g8pXHOLXDOHeSca1r8c1gAHFv8fR43lV7oxRc1+gHv4b/5\nxznnFlUeYZjrAAAAlUlEQVT2caPUDrgaf6a7cxPsjqFDpaCbgDFmNh+/P+3vA+cpVfFvE+OBOcAC\n/M9P8LsDd2dmY4GZQEszKzCza4FHgHPNLB//m8YjITPuroTMTwO1gSnFP4MjgobcRQl5g9OdoiIi\nKUIXRUVEUoQKXUQkRajQRURShApdRCRFqNBFRFKECl1EJEWo0EVEUoQKXUQkRfw/+FKWLR3SOFsA\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x206031e7748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = mv_network(256,5)\n",
    "history = model.train_model()\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "plt.figure(1)\n",
    "plt.plot(train_loss, c='r', label='train_loss')\n",
    "plt.plot(val_loss, c='b', label='val_loss')\n",
    "plt.legend()\n",
    "plt.xlim([0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(model, to_file='model_2.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
