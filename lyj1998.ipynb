{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "sys.version_info(major=3, minor=6, micro=7, releaselevel='final', serial=0)\ntensorflow 2.1.0\nmatplotlib 3.2.0\nnumpy 1.18.1\npandas 1.0.1\nsklearn 0.22.2.post1\ntensorflow 2.1.0\ntensorflow_core.python.keras.api._v2.keras 2.2.4-tf\n"
    }
   ],
   "source": [
    "import os, sys, re, pickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow import keras\n",
    "\n",
    "from Data import Pro_data\n",
    "print(sys.version_info)\n",
    "for module in tf, mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ORIGIN_DATA_DIR = os.getcwd() + '\\\\data\\\\BX-CSV-Dump\\\\'\n",
    "FILTERED_DATA_DIR = os.getcwd() + '\\\\tmp\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   User-ID            Location   ISBN  \\\n0     2918  [1242, 7251, 5372]  32719   \n1     2918  [1242, 7251, 5372]  23709   \n2    24974  [4759, 1444, 5372]  23709   \n3    18233    [7245, 30, 1600]  23709   \n4      240  [4723, 6498, 1600]  23709   \n\n                                               Title  Author  Year  Publisher  \\\n0  [18416, 3305, 2794, 2794, 2794, 2794, 2794, 27...   13704    63       2346   \n1  [5042, 20096, 12301, 3565, 6702, 20408, 4956, ...    6881    71       2064   \n2  [5042, 20096, 12301, 3565, 6702, 20408, 4956, ...    6881    71       2064   \n3  [5042, 20096, 12301, 3565, 6702, 20408, 4956, ...    6881    71       2064   \n4  [5042, 20096, 12301, 3565, 6702, 20408, 4956, ...    6881    71       2064   \n\n                                               Blurb  \n0  [16743, 70936, 91023, 94647, 30725, 94201, 110...  \n1  [26888, 9440, 6976, 72713, 117446, 20931, 2329...  \n2  [26888, 9440, 6976, 72713, 117446, 20931, 2329...  \n3  [26888, 9440, 6976, 72713, 117446, 20931, 2329...  \n4  [26888, 9440, 6976, 72713, 117446, 20931, 2329...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>User-ID</th>\n      <th>Location</th>\n      <th>ISBN</th>\n      <th>Title</th>\n      <th>Author</th>\n      <th>Year</th>\n      <th>Publisher</th>\n      <th>Blurb</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2918</td>\n      <td>[1242, 7251, 5372]</td>\n      <td>32719</td>\n      <td>[18416, 3305, 2794, 2794, 2794, 2794, 2794, 27...</td>\n      <td>13704</td>\n      <td>63</td>\n      <td>2346</td>\n      <td>[16743, 70936, 91023, 94647, 30725, 94201, 110...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2918</td>\n      <td>[1242, 7251, 5372]</td>\n      <td>23709</td>\n      <td>[5042, 20096, 12301, 3565, 6702, 20408, 4956, ...</td>\n      <td>6881</td>\n      <td>71</td>\n      <td>2064</td>\n      <td>[26888, 9440, 6976, 72713, 117446, 20931, 2329...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>24974</td>\n      <td>[4759, 1444, 5372]</td>\n      <td>23709</td>\n      <td>[5042, 20096, 12301, 3565, 6702, 20408, 4956, ...</td>\n      <td>6881</td>\n      <td>71</td>\n      <td>2064</td>\n      <td>[26888, 9440, 6976, 72713, 117446, 20931, 2329...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>18233</td>\n      <td>[7245, 30, 1600]</td>\n      <td>23709</td>\n      <td>[5042, 20096, 12301, 3565, 6702, 20408, 4956, ...</td>\n      <td>6881</td>\n      <td>71</td>\n      <td>2064</td>\n      <td>[26888, 9440, 6976, 72713, 117446, 20931, 2329...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>240</td>\n      <td>[4723, 6498, 1600]</td>\n      <td>23709</td>\n      <td>[5042, 20096, 12301, 3565, 6702, 20408, 4956, ...</td>\n      <td>6881</td>\n      <td>71</td>\n      <td>2064</td>\n      <td>[26888, 9440, 6976, 72713, 117446, 20931, 2329...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "data = Pro_data.DataLoad()\n",
    "data.features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "200.0"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "length  = [len(i) for i in data.features['Blurb']]\n",
    "np.mean(np.array(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   User-ID            Location   ISBN  \\\n0     2918  [1242, 7251, 5372]  32719   \n1     2918  [1242, 7251, 5372]  23709   \n2    24974  [4759, 1444, 5372]  23709   \n3    18233    [7245, 30, 1600]  23709   \n4      240  [4723, 6498, 1600]  23709   \n\n                                               Title  Author  Year  Publisher  \\\n0  [18416, 3305, 2794, 2794, 2794, 2794, 2794, 27...   13704    63       2346   \n1  [5042, 20096, 12301, 3565, 6702, 20408, 4956, ...    6881    71       2064   \n2  [5042, 20096, 12301, 3565, 6702, 20408, 4956, ...    6881    71       2064   \n3  [5042, 20096, 12301, 3565, 6702, 20408, 4956, ...    6881    71       2064   \n4  [5042, 20096, 12301, 3565, 6702, 20408, 4956, ...    6881    71       2064   \n\n                                               Blurb  \n0  [16743, 70936, 91023, 94647, 30725, 94201, 110...  \n1  [26888, 9440, 6976, 72713, 117446, 20931, 2329...  \n2  [26888, 9440, 6976, 72713, 117446, 20931, 2329...  \n3  [26888, 9440, 6976, 72713, 117446, 20931, 2329...  \n4  [26888, 9440, 6976, 72713, 117446, 20931, 2329...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>User-ID</th>\n      <th>Location</th>\n      <th>ISBN</th>\n      <th>Title</th>\n      <th>Author</th>\n      <th>Year</th>\n      <th>Publisher</th>\n      <th>Blurb</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2918</td>\n      <td>[1242, 7251, 5372]</td>\n      <td>32719</td>\n      <td>[18416, 3305, 2794, 2794, 2794, 2794, 2794, 27...</td>\n      <td>13704</td>\n      <td>63</td>\n      <td>2346</td>\n      <td>[16743, 70936, 91023, 94647, 30725, 94201, 110...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2918</td>\n      <td>[1242, 7251, 5372]</td>\n      <td>23709</td>\n      <td>[5042, 20096, 12301, 3565, 6702, 20408, 4956, ...</td>\n      <td>6881</td>\n      <td>71</td>\n      <td>2064</td>\n      <td>[26888, 9440, 6976, 72713, 117446, 20931, 2329...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>24974</td>\n      <td>[4759, 1444, 5372]</td>\n      <td>23709</td>\n      <td>[5042, 20096, 12301, 3565, 6702, 20408, 4956, ...</td>\n      <td>6881</td>\n      <td>71</td>\n      <td>2064</td>\n      <td>[26888, 9440, 6976, 72713, 117446, 20931, 2329...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>18233</td>\n      <td>[7245, 30, 1600]</td>\n      <td>23709</td>\n      <td>[5042, 20096, 12301, 3565, 6702, 20408, 4956, ...</td>\n      <td>6881</td>\n      <td>71</td>\n      <td>2064</td>\n      <td>[26888, 9440, 6976, 72713, 117446, 20931, 2329...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>240</td>\n      <td>[4723, 6498, 1600]</td>\n      <td>23709</td>\n      <td>[5042, 20096, 12301, 3565, 6702, 20408, 4956, ...</td>\n      <td>6881</td>\n      <td>71</td>\n      <td>2064</td>\n      <td>[26888, 9440, 6976, 72713, 117446, 20931, 2329...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "data.features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "all_user_id_number= 28836\nlocation_length=3,all_location_words_number=7573 \nall_isbn_words_number  38036\ntitle_length=15,all_title_words_number=23730\nall_author_words_number  15196\nall_year_words_number  81\nall_publisher_words_number  2909\nblurb_length=200,all_blurb_words_number=127034\n"
    }
   ],
   "source": [
    "#User-ID number,max_index\n",
    "all_user_id_number = len(set(data.features['User-ID']))\n",
    "print('all_user_id_number=',all_user_id_number)\n",
    "\n",
    "#Location number index\n",
    "location_length = len(data.features['Location'][0])\n",
    "all_location_words_number = max([j for i in data.features['Location'] for j in i])\n",
    "print('location_length=%d,all_location_words_number=%d '% (location_length,all_location_words_number) )\n",
    "\n",
    "#ISBN numner\n",
    "all_isbn_words_number = len(set(data.features['ISBN']))\n",
    "print('all_isbn_words_number ',all_isbn_words_number)\n",
    "\n",
    "#Title\n",
    "title_length = len(data.features['Title'][0])\n",
    "all_title_words_number = max([j for i in data.features['Title'] for j in i])\n",
    "print('title_length=%d,all_title_words_number=%d'% (title_length,all_title_words_number))\n",
    "\n",
    "#Author\n",
    "all_author_words_number = len(set(data.features['Author']))\n",
    "print('all_author_words_number ',all_author_words_number)\n",
    "\n",
    "#Year\n",
    "all_year_words_number = len(set(data.features['Year']))\n",
    "print('all_year_words_number ',all_year_words_number)\n",
    "\n",
    "# Publisher\n",
    "all_publisher_words_number = len(set(data.features['Publisher']))\n",
    "print('all_publisher_words_number ',all_publisher_words_number)\n",
    "\n",
    "# Blurb\n",
    "blurb_length = len(data.features['Blurb'][0])\n",
    "all_blurb_words_number = max([j for i in data.features['Blurb'] for j in i])\n",
    "print('blurb_length=%d,all_blurb_words_number=%d'% (blurb_length,all_blurb_words_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "超参数\n",
    "\"\"\"\n",
    "# Number of Epochs\n",
    "num_epochs = 5\n",
    "# Batch_size\n",
    "batch_size = 256\n",
    "# dropout_keep\n",
    "dropout_keep = 0.5\n",
    "# Learning_rate\n",
    "learning_rate = 0.001\n",
    "# embedding_dim\n",
    "embedding_dim = 16\n",
    "# 文本卷积滑动的单词个数\n",
    "window_sizes = {2,3,4,5}\n",
    "# filter num\n",
    "filter_num = 8\n",
    "# dense dim\n",
    "dense_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "#     用户特征输入占位符\n",
    "    # 用户特征输入\n",
    "    user_id = keras.layers.Input(shape=(1,), dtype='int32', name='user_id_input')\n",
    "    user_location = keras.layers.Input(shape=(3,), dtype='int32', name='user_location_input')\n",
    "    \n",
    "    # 书籍特征输入\n",
    "    book_isbn = keras.layers.Input(shape=(1,),  dtype='int32', name='book_isbn_input')\n",
    "    book_author = keras.layers.Input(shape=(1,),  dtype='int32', name='book_author_input')\n",
    "    book_year = keras.layers.Input(shape=(1,),  dtype='int32', name='book_year_input')\n",
    "    book_publisher = keras.layers.Input(shape=(1,),  dtype='int32', name='book_publisher_input')  \n",
    "    book_title = keras.layers.Input(shape=(15, ), dtype='int32', name='book_title_input')\n",
    "    book_blurb = keras.layers.Input(shape=(200, ), dtype='int32', name='book_blurb_input')\n",
    "    return user_id, user_location, book_isbn, book_author, book_year, book_publisher, book_title, book_blurb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "构建User神经网络\n",
    "\"\"\"\n",
    "def get_user_embedding(user_id,user_location):\n",
    "    uid_embed_layer = tf.keras.layers.Embedding(all_user_id_number,embedding_dim,input_length=1,name='uid_embed_layer')(user_id)\n",
    "    location_embed_layer = tf.keras.layers.Embedding(all_location_words_number,embedding_dim,input_length=location_length,name='location_embed_layer')(user_location)\n",
    "    return uid_embed_layer, location_embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_user_feature_layer(uid_embed_layer,location_embed_layer):\n",
    "#     第一层全连接\n",
    "    uid_fc_layer = tf.keras.layers.Dense(dense_dim,name='uid_fc_layer',activation='relu')(uid_embed_layer)\n",
    "    location_fc_layer = tf.keras.layers.Dense(dense_dim,name='location_fc_layer',activation='relu')(location_embed_layer)\n",
    "#  对location进行Encoder提取特征\n",
    "    location_gru_layer = tf.keras.layers.GRU(units=dense_dim,dropout=0.5,name='location_gru_layer')(location_fc_layer)\n",
    "#     [None,32]\n",
    "    print(location_gru_layer.shape)\n",
    "    location_gru_expand_layer = tf.expand_dims(location_gru_layer,axis=1)\n",
    "    \n",
    "#     第二层全连接\n",
    "    user_combine_layer = tf.keras.layers.concatenate([uid_fc_layer,location_gru_expand_layer],2)\n",
    "    user_dense_layer = tf.keras.layers.Dense(200,activation='tanh',name='user_dense_layer')(user_combine_layer)\n",
    "    user_dense_layer_flat = tf.keras.layers.Reshape([200], name=\"user_combine_layer_flat\")(user_dense_layer)\n",
    "    return user_dense_layer,user_dense_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "构建book神经网络\n",
    "\"\"\"\n",
    "def get_book_embedding(book_isbn, book_author, book_year, book_publisher, book_title, book_blurb):\n",
    "    book_isbn_embed_layer = tf.keras.layers.Embedding(all_isbn_words_number,embedding_dim,input_length = 1,name='book_isbn_embed_layer')(book_isbn)\n",
    "    book_author_embed_layer = tf.keras.layers.Embedding(all_author_words_number,embedding_dim,input_length=1,name='book_author_embed_layer')(book_author)\n",
    "    book_year_embed_layer = tf.keras.layers.Embedding(all_year_words_number,embedding_dim,input_length=1,name='book_year_embed_layer')(book_year)\n",
    "    book_publisher_embed_layer = tf.keras.layers.Embedding(all_publisher_words_number,embedding_dim,input_length = 1,name='book_publisher_embed_layer')(book_publisher)\n",
    "    book_title_embed_layer = tf.keras.layers.Embedding(all_title_words_number,embedding_dim,input_length=title_length,name='book_title_embed_layer')(book_title)\n",
    "    book_blurb_embed_layer = tf.keras.layers.Embedding(all_blurb_words_number,embedding_dim,input_length = blurb_length,name='book_blurb_embed_layer')(book_blurb)\n",
    "    return book_isbn_embed_layer,book_author_embed_layer,book_year_embed_layer,book_publisher_embed_layer,book_title_embed_layer,book_blurb_embed_layer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_book_feature_layer(book_isbn_embed_layer,book_author_embed_layer,book_year_embed_layer,book_publisher_embed_layer,book_title_embed_layer,book_blurb_embed_layer):\n",
    "#     对isbn,author,year,publisher第一层全连接\n",
    "    book_isbn_dense_layer = tf.keras.layers.Dense(dense_dim,activation='relu',name='book_isbn_dense_layer')(book_isbn_embed_layer)\n",
    "    book_author_dense_layer = tf.keras.layers.Dense(dense_dim,activation='relu',name='book_author_dense_layer')(book_author_embed_layer)\n",
    "    book_year_dense_layer = tf.keras.layers.Dense(dense_dim,activation='relu',name='book_year_dense_layer')(book_year_embed_layer)\n",
    "    book_publisher_dense_layer = tf.keras.layers.Dense(dense_dim,activation='relu',name='book_publisher_dense_layer')(book_publisher_embed_layer)\n",
    "    book_title_embed_layer_expand = tf.expand_dims(book_title_embed_layer,axis=-1)\n",
    "#     对title进行文本卷积\n",
    "#     book_title_embed_layer_expand:[None,15,16,1]\n",
    "#     对文本嵌入层使用不同的卷积核做卷积核最大池化\n",
    "    pool_layer_list = []\n",
    "    for window_size in window_sizes:\n",
    "        title_conv_layer = tf.keras.layers.Conv2D(filters = filter_num,kernel_size = (window_size,embedding_dim),strides=1,activation='relu')(book_title_embed_layer_expand)\n",
    "        title_maxpool_layer = tf.keras.layers.MaxPooling2D(pool_size=(title_length-window_size+1,1),strides=1)(title_conv_layer)\n",
    "        pool_layer_list.append(title_maxpool_layer)\n",
    "    pool_layer_layer = tf.keras.layers.concatenate(pool_layer_list,axis=-1,name='title_pool_layer')\n",
    "    max_num = len(window_sizes)*filter_num\n",
    "    pool_layer_flat = tf.keras.layers.Reshape([1,max_num],name='pool_layer_flat')(pool_layer_layer)\n",
    "    dropout_layer = tf.keras.layers.Dropout(dropout_keep, name = \"dropout_layer\")(pool_layer_flat)\n",
    "\n",
    "    # 对简介进行Encoder特征提取\n",
    "    book_blurb_dense_layer = tf.keras.layers.Dense(dense_dim,activation='relu',name='book_blurb_dense_layer')(book_blurb_embed_layer)\n",
    "    book_blurb_gru_layer = tf.keras.layers.GRU(units=dense_dim,dropout=0.5,name='book_blurb_gru_layer')(book_blurb_dense_layer)\n",
    "    print('book_blurb_gru_layer=',book_blurb_gru_layer.shape)\n",
    "    book_blurb_gru_expand_layer = tf.expand_dims(book_blurb_gru_layer,axis=1)\n",
    "    book_combine_layer = tf.keras.layers.concatenate([book_isbn_dense_layer,book_author_dense_layer,book_year_dense_layer,book_publisher_dense_layer,dropout_layer,book_blurb_gru_expand_layer],axis=-1)\n",
    "    book_dense_layer = tf.keras.layers.Dense(200, activation='tanh')(book_combine_layer)\n",
    "    book_dense_layer_flat = tf.keras.layers.Reshape([200], name=\"book_dense_layer_flat\")(book_dense_layer)\n",
    "    return book_dense_layer,book_dense_layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rating(user_feature, book_feature):\n",
    "    multiply_layer = keras.layers.Lambda(lambda layer: tf.reduce_sum(layer[0]*layer[1],axis=1,keepdims=True), name = 'user_book_feature')(user_feature, book_feature)\n",
    "    print(multiply_layer.shape)\n",
    "    return multiply_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "TensorShape([2, 1])"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "a = tf.constant([[2,1,1],[1,3,4]])\n",
    "b = tf.constant([[0,1,1],[1,0,4]])\n",
    "c = inference = tf.keras.layers.Lambda(lambda layer:tf.reduce_sum(layer[0] * layer[1], axis=1,keepdims=True))((a,b))\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = data.features.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[2918, list([1242, 7251, 5372]), 32719,\n        list([18416, 3305, 2794, 2794, 2794, 2794, 2794, 2794, 2794, 2794, 2794, 2794, 2794, 2794, 2794]),\n        13704, 63, 2346,\n        list([16743, 70936, 91023, 94647, 30725, 94201, 110675, 90181, 124050, 43353, 17884, 63098, 25481, 55269, 5885, 41278, 4336, 123778, 68058, 98997, 76124, 102306, 1763, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630])],\n       [2918, list([1242, 7251, 5372]), 23709,\n        list([5042, 20096, 12301, 3565, 6702, 20408, 4956, 2954, 8688, 5445, 564, 2794, 2794, 2794, 2794]),\n        6881, 71, 2064,\n        list([26888, 9440, 6976, 72713, 117446, 20931, 23297, 84174, 4625, 70394, 7141, 113101, 26419, 47628, 55024, 112869, 112706, 20585, 106931, 92756, 109896, 23849, 65369, 36194, 114115, 80333, 73742, 28898, 48781, 111777, 2668, 82081, 83626, 109866, 27063, 109866, 111577, 44885, 86958, 75215, 12772, 60734, 115405, 80382, 89862, 25939, 106101, 39026, 19433, 126610, 76170, 109866, 27063, 116498, 64180, 95559, 97028, 27073, 98468, 63331, 47776, 27063, 126918, 19433, 52933, 82906, 56212, 5023, 14350, 73913, 5348, 21891, 16394, 44078, 26888, 1090, 88893, 1930, 110, 120855, 69218, 126918, 33914, 69359, 99901, 110390, 72713, 36255, 110675, 27063, 120937, 91617, 107752, 74255, 67900, 6092, 64565, 23297, 21891, 50271, 51224, 99901, 65369, 1270, 2914, 18718, 103600, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630, 3630])]],\n      dtype=object)"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "features[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets=data.labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get_Batch\n",
    "def get_batch(Xs,ys,batchsize):\n",
    "    for start in range(0,len(Xx,batchsize)):\n",
    "        end = min(start+batchsize,len(Xs))\n",
    "        yield xS[start:end],ys[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建计算图\n",
    "将用户特征和书籍特征作为输入，经过一个全连接，输出预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[1, 2, 5]"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# test\n",
    "a = np.array([[1, 2, 4, ([1, 2, 5])], \n",
    "             [3, 2, 6, ([6, 5, 1])],\n",
    "             [6, 9, 4, ([3, 7, 5])]])\n",
    "a.take(3,1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(None, 32)\nbook_blurb_gru_layer= (None, 32)\nuser_dense_layer_flat= (None, 200)\nbook_dense_layer_flat= (None, 200)\n"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Invalid reduction dimension 1 for input with 1 dimensions. for 'user_book_feature/Sum' (op: 'Sum') with input shapes: [200], [] and with computed input tensors: input[1] = <1>.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\envs\\rec_env\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1618\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1619\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1620\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Invalid reduction dimension 1 for input with 1 dimensions. for 'user_book_feature/Sum' (op: 'Sum') with input shapes: [200], [] and with computed input tensors: input[1] = <1>.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-9702bf0be87a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmv_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-9702bf0be87a>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"user_dense_layer_flat=\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muser_dense_layer_flat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"book_dense_layer_flat=\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbook_dense_layer_flat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0minference\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_rating\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_dense_layer_flat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_dense_layer_flat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# 将用户特征和电影特征作为输入，经过全连接，输出一个值的方案\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-962422bd952f>\u001b[0m in \u001b[0;36mget_rating\u001b[1;34m(user_feature, book_feature)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_rating\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_feature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmultiply_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'user_book_feature'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_feature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmultiply_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmultiply_layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\envs\\rec_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    771\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[0;32m    772\u001b[0m                   \u001b[1;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 773\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    774\u001b[0m                     \u001b[1;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m                     \u001b[1;31m# circular dependencies.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\envs\\rec_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, mask, training)\u001b[0m\n\u001b[0;32m    844\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwatch_accessed_variables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m         \u001b[0mvariable_scope\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_variable_creator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 846\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    847\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcreated_variables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwatched_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-962422bd952f>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(layer)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_rating\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_feature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmultiply_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'user_book_feature'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_feature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmultiply_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmultiply_layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\envs\\rec_env\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\envs\\rec_env\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mreduce_sum\u001b[1;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[0;32m   1593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1594\u001b[0m   return reduce_sum_with_dims(input_tensor, axis, keepdims, name,\n\u001b[1;32m-> 1595\u001b[1;33m                               _ReductionDims(input_tensor, axis))\n\u001b[0m\u001b[0;32m   1596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\envs\\rec_env\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mreduce_sum_with_dims\u001b[1;34m(input_tensor, axis, keepdims, name, dims)\u001b[0m\n\u001b[0;32m   1604\u001b[0m   return _may_reduce_to_scalar(\n\u001b[0;32m   1605\u001b[0m       \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1606\u001b[1;33m       gen_math_ops._sum(input_tensor, dims, keepdims, name=name))\n\u001b[0m\u001b[0;32m   1607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\envs\\rec_env\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m_sum\u001b[1;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[0;32m  10172\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0;32m  10173\u001b[0m         \u001b[1;34m\"Sum\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction_indices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_dims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep_dims\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10174\u001b[1;33m                name=name)\n\u001b[0m\u001b[0;32m  10175\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10176\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\envs\\rec_env\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    740\u001b[0m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[0;32m    741\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 742\u001b[1;33m                                  attrs=attr_protos, op_def=op_def)\n\u001b[0m\u001b[0;32m    743\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m     \u001b[1;31m# `outputs` is returned as a separate return value so that the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\envs\\rec_env\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    593\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0;32m    594\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m         compute_device)\n\u001b[0m\u001b[0;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\envs\\rec_env\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3320\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3321\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3322\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3323\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3324\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\envs\\rec_env\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1784\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   1785\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1786\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1787\u001b[0m       \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1788\u001b[0m     \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\envs\\rec_env\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1620\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1621\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1622\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1624\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid reduction dimension 1 for input with 1 dimensions. for 'user_book_feature/Sum' (op: 'Sum') with input shapes: [200], [] and with computed input tensors: input[1] = <1>."
     ]
    }
   ],
   "source": [
    "class mv_network(object):\n",
    "    def __init__(self, batch_size=256):\n",
    "        self.batch_size = batch_size\n",
    "        self.best_loss = 9999\n",
    "        self.losses = {'train': [], 'test': []}\n",
    "\n",
    "        # 获取输入占位符\n",
    "        user_id, user_location, book_isbn, book_author, book_year, book_publisher, book_title, book_blurb = get_inputs()\n",
    "        # 获取User的2个嵌入向量\n",
    "        uid_embed_layer, location_embed_layer = get_user_embedding(user_id,user_location)\n",
    "        # 得到用户特征\n",
    "        user_dense_layer,user_dense_layer_flat =get_user_feature_layer(uid_embed_layer,location_embed_layer)\n",
    "        # 获取书籍的嵌入向量\n",
    "        book_isbn_embed_layer,book_author_embed_layer,book_year_embed_layer,book_publisher_embed_layer,book_title_embed_layer,book_blurb_embed_layer=get_book_embedding(book_isbn, book_author, book_year, book_publisher, book_title, book_blurb)\n",
    "        # 获取书籍特征\n",
    "        book_dense_layer,book_dense_layer_flat=get_book_feature_layer(book_isbn_embed_layer,book_author_embed_layer,book_year_embed_layer,book_publisher_embed_layer,book_title_embed_layer,book_blurb_embed_layer)\n",
    "        \n",
    "        # 计算出评分\n",
    "        # 将用户特征和电影特征做矩阵乘法得到一个预测评分的方案\n",
    "        print(\"user_dense_layer_flat=\",user_dense_layer_flat.shape)\n",
    "        print(\"book_dense_layer_flat=\",book_dense_layer_flat.shape)\n",
    "        inference = get_rating(user_dense_layer_flat, book_dense_layer_flat)\n",
    "\n",
    "        # 将用户特征和电影特征作为输入，经过全连接，输出一个值的方案\n",
    "        # inference_layer = tf.keras.layers.concatenate([user_combine_layer_flat, movie_combine_layer_flat],1)  # (?, 400)\n",
    "        # # 你可以使用下面这个全连接层，试试效果\n",
    "        # inference_dense = tf.keras.layers.Dense(64, kernel_regularizer=tf.nn.l2_loss, activation='relu')(inference_layer)\n",
    "        # inference = tf.keras.layers.Dense(1, name=\"inference\")(inference_layer)  # inference_dense\n",
    "\n",
    "        self.model = tf.keras.Model(\n",
    "            inputs=[user_id, user_location, book_isbn, book_title, book_author, book_year, book_publisher, book_blurb],\n",
    "            outputs=[inference])\n",
    "\n",
    "        self.model.summary()\n",
    "model = mv_network()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}