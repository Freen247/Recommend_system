{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://imgconvert.csdnimg.cn/aHR0cDovL3N0YXRpYy5vc2NoaW5hLm5ldC91cGxvYWRzL3NwYWNlLzIwMTUvMTIxNC8xOTAwMTZfemo3Wl8yMzE1MjQ3LnBuZw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关键字怎么提取的，TF-IDF有改进么，怎么改进的\n",
    "\n",
    "TF（Term Frequency）词频，在文章中出现次数最多的词，然而文章中出现次数较多的词并不一定就是关键词，比如常见的对文章本身并没有多大意义的停用词。所以我们需要一个重要性调整系数来衡量一个词是不是常见词。该权重为IDF（Inverse Document Frequency）逆文档频率，它的大小与一个词的常见程度成反比。在我们得到词频（TF）和逆文档频率（IDF）以后，将两个值相乘，即可得到一个词的TF-IDF值，某个词对文章的重要性越高，其TF-IDF值就越大，所以排在最前面的几个词就是文章的关键词。\n",
    "\n",
    "TF-IDF的不足\n",
    "1、没有考虑特征词的位置因素对文本的区分度，词条出现在文档的不同位置时，对区分度的贡献大小是不一样的。\n",
    "\n",
    "2、按照传统TF-IDF函数标准，往往一些生僻词的IDF(反文档频率)会比较高、因此这些生僻词常会被误认为是文档关键词。(换句话说，如果一个特征项只在某一个类别中的个别文本中大量出现，在类内的其他大部分文本中出现的很少，那么不排除这些个别文本是这个类中的特例情况，因此这样的特征项不具有代表性。)\n",
    "\n",
    "3、传统TF-IDF函数中的IDF部分只考虑了特征词与它出现的文本数之间的关系，而忽略了特征项在一个类别中不同的类别间的分布情况。\n",
    "\n",
    "4、对于文档中出现次数较少的重要人名、地名信息提取效果不佳。\n",
    "\n",
    "TF部分的改进\n",
    "这里考虑将文档内的词频率更改为同一类文档内的词频率可以在一定程度上解决上面提到的第2项不足之处。\n",
    "IDF部分的改进\n",
    "\n",
    "传统的IDF通常可以写作：IDF=log(总文档数N/所有含特征词文档数n+0.01)\n",
    "\n",
    "在我查阅的所有论文中都提到了上面的第3项不足，这是TF-IDF应用于分类问题上的一个很明显的不足，针对这个不足，这些论文中也提到了不同的解决方法：\n",
    "\n",
    "①IDF=log(本类含特征词文档数m*总文档数N/所有含特征词文档数n+0.01)\n",
    "\n",
    "②用P（Mk）表示特征词Mk在当前类别中的频率，P（Mk）’表示特征词Mk在其他类别中的频率，对IDF计算改进如下\n",
    "\n",
    "$$IDF = log(1+\\frac{P(m_k)}{P(m_k)+{P(m_k)}'})$$\n",
    "\n",
    "![!image](https://img-blog.csdn.net/20150303100444956?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZnlmbWZvZg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost的原理\n",
    "XGBoost（eXtreme Gradient Boosting）全名叫极端梯度提升，XGBoost是集成学习方法的王牌，在Kaggle数据挖掘比赛中，大部分获胜者用了XGBoost，XGBoost在绝大多数的回归和分类问题上表现的十分顶尖，本文较详细的介绍了XGBoost的算法原理。\n",
    "\n",
    "CART － Classification and Regression Trees\n",
    "\n",
    "既然xgboost就是一个监督模型，那么我们的第一个问题就是：xgboost对应的模型是什么？\n",
    "答案就是一堆CART树。\n",
    "此时，可能我们又有疑问了，CART树是什么？这个问题请查阅其他资料，我的博客中也有相关文章涉及过。然后，一堆树如何做预测呢？答案非常简单，就是将每棵树的预测值加到一起作为最终的预测值，可谓简单粗暴。\n",
    "\n",
    "下图就是CART树和一堆CART树的示例，用来判断一个人是否会喜欢计算机游戏：\n",
    "\n",
    "![](https://upload-images.jianshu.io/upload_images/1371984-a90c565a27c9874d.PNG?imageMogr2/auto-orient/strip|imageView2/2/w/835/format/webp)\n",
    "![](https://upload-images.jianshu.io/upload_images/1371984-bbe17b3b253a6d1a.PNG?imageMogr2/auto-orient/strip|imageView2/2/w/901/format/webp)\n",
    "\n",
    "xgboost为什么使用CART树而不是用普通的决策树呢？\n",
    "简单讲，对于分类问题，由于CART树的叶子节点对应的值是一个实际的分数，而非一个确定的类别，这将有利于实现高效的优化算法。xgboost出名的原因一是准，二是快，之所以快，其中就有选用CART树的一份功劳。\n",
    "\n",
    "ID3、C4.5、CART的区别\n",
    "这三个是非常著名的决策树算法。简单粗暴来说，ID3 使用信息增益作为选择特征的准则；C4.5 使用信息增益比作为选择特征的准则；CART 使用 Gini 指数作为选择特征的准则。\n",
    "\n",
    "一、ID3\n",
    "熵表示的是数据中包含的信息量大小。熵越小，数据的纯度越高，也就是说数据越趋于一致，这是我们希望的划分之后每个子节点的样子。\n",
    "\n",
    "信息增益 = 划分前熵 - 划分后熵。信息增益越大，则意味着使用属性 a 来进行划分所获得的 “纯度提升” 越大 **。也就是说，用属性 a 来划分训练集，得到的结果中纯度比较高。\n",
    "\n",
    "ID3 仅仅适用于二分类问题。ID3 仅仅能够处理离散属性。\n",
    "\n",
    "二、C4.5\n",
    "\n",
    "C4.5 克服了 ID3 仅仅能够处理离散属性的问题，以及信息增益偏向选择取值较多特征的问题，使用信息增益比来选择特征。信息增益比 = 信息增益 / 划分前熵 选择信息增益比最大的作为最优特征。\n",
    "\n",
    "C4.5 处理连续特征是先将特征取值排序，以连续两个值中间值作为划分标准。尝试每一种划分，并计算修正后的信息增益，选择信息增益最大的分裂点作为该属性的分裂点。\n",
    "\n",
    "三、CART\n",
    "\n",
    "CART 与 ID3，C4.5 不同之处在于 CART 生成的树必须是二叉树。也就是说，无论是回归还是分类问题，无论特征是离散的还是连续的，无论属性取值有多个还是两个，内部节点只能根据属性值进行二分。\n",
    "\n",
    "CART 的全称是分类与回归树。从这个名字中就应该知道，CART 既可以用于分类问题，也可以用于回归问题。\n",
    "\n",
    "回归树中，使用平方误差最小化准则来选择特征并进行划分。每一个叶子节点给出的预测值，是划分到该叶子节点的所有样本目标值的均值，这样只是在给定划分的情况下最小化了平方误差。\n",
    "\n",
    "要确定最优化分，还需要遍历所有属性，以及其所有的取值来分别尝试划分并计算在此种划分情况下的最小平方误差，选取最小的作为此次划分的依据。由于回归树生成使用平方误差最小化准则，所以又叫做最小二乘回归树。\n",
    "\n",
    "分类树种，使用 Gini 指数最小化准则来选择特征并进行划分；\n",
    "\n",
    "Gini 指数表示集合的不确定性，或者是不纯度。基尼指数越大，集合不确定性越高，不纯度也越大。这一点和熵类似。另一种理解基尼指数的思路是，基尼指数是为了最小化误分类的概率。\n",
    "\n",
    "信息增益 vs 信息增益比\n",
    "\n",
    "之所以引入了信息增益比，是由于信息增益的一个缺点。那就是：信息增益总是偏向于选择取值较多的属性。信息增益比在此基础上增加了一个罚项，解决了这个问题。\n",
    "\n",
    "Gini 指数 vs 熵\n",
    "\n",
    "既然这两个都可以表示数据的不确定性，不纯度。那么这两个有什么区别那？\n",
    "\n",
    "Gini 指数的计算不需要对数运算，更加高效；\n",
    "Gini 指数更偏向于连续属性，熵更偏向于离散属性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SVM原始问题为什么要转化为对偶问题，为什么对偶问题就好求解，原始问题不能求解么\n",
    "- K-means 中我想聚成100类 结果发现只能聚成98类，为什么\n",
    "- 进程中的内存分段是怎样的\n",
    "- GBDT的原理，以及常用的调参的参数\n",
    "- xgboost的跟GBDT比优点都有哪些\n",
    "- L1、L2正则化，区别\n",
    "- Xgboost中的行抽样，可以起到哪些作用\n",
    "- 样本少了不是会过拟合么，为什么行抽样可以防止过拟合\n",
    "- 常用的损失函数和适用场景\n",
    "- LR和SVM原理\n",
    "- LR和SVM这两个应用起来有什么不同\n",
    "- PCA说一下\n",
    "- 你都会什么聚类方法\n",
    "- 模型的评价方法有哪些\n",
    "- ROC怎么画\n",
    "- 你知道SoftMax么\n",
    "- 特征选择方法都有用过哪些\n",
    "- 随机森林怎么进行特征选择\n",
    "- 用过哪些机器学习算法\n",
    "- 加密方法知道哪些\n",
    "- MD5可逆么\n",
    "- word2vec用过么\n",
    "- 极大似然估计是什么意思\n",
    "- 说一下随机森林和Adaboost，以及区别\n",
    "- 说一下GBDT和Adaboost，以及区别\n",
    "- 说一下LDA的原理\n",
    "- 对于PCA，会有第一主成分、第二主成分，怎么为什么第一主成分是第一，原因是什么？\n",
    "- PCA的主成分是怎么得到的\n",
    "- 说一下SVM\n",
    "- 面向对象的三要素\n",
    "- 对深度学习了解多少\n",
    "- 你觉得深度学习的方法和传统机器学习比，有什么大的优势\n",
    "- 当我们要求准确率很高，但是不在意召回率的时候，可以怎样处理。\n",
    "- 回归算法用于分类的阈值如何确定呢\n",
    "- xgboost，说一下原理，步长如何设定\n",
    "- k-means中的k如何确定呢？\n",
    "- 除了k-means，还可以用什么聚类方法，或者你还熟悉什么聚类方法\n",
    "- 层次聚类的话，你又如何判断聚成多少类合适？\n",
    "- 朴素贝叶斯原理\n",
    "- TF-IDF原理\n",
    "- 性能评价指标，准确率召回率是怎么回事，二分类 和多分类的评价方法\n",
    "- LDA你是怎么用的，LDA的表现如何，主题分的效果好不好\n",
    "- 你觉得基于内容的方法和协同过滤有什么不同\n",
    "- 常用的推荐算法都有什么\n",
    "- 集成学习为什么要用简单的基学习器，不用一个复杂一点的学习器\n",
    "- 非线性的数据，可以使用什么分类器进行分类\n",
    "- LDA的原理是什么？\n",
    "- 推荐的时候矩阵一定是稀疏的，对于这个稀疏矩阵应该如何处理？\n",
    "- 如何从文档中提取关键字？\n",
    "- 讲一讲tf-idf是什么意思\n",
    "- hashmap你用过么，底层是如何实现的？\n",
    "- 手撸代码，不用库函数求一个数的立方根，要求误差小于0.01\n",
    "\n",
    "\n",
    "## 专业知识归纳"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kaggle中的features 都是匿名的 如何才能利用上，或者说如何进行分析？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "情感指的是隐藏在句子中的观点，极性（polarity）定义句子中的消极性或积极性，主观性（subjectivity）暗示句子的表达的含糊的、还是肯定的。\n",
    "\n",
    "NLP 里面，最细粒度的是 词语，词语组成句子，句子再组成段落、篇章、文档。所以处理 NLP 的问题，首先就要拿词语开刀。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 知识图谱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备字节跳动的笔试和面试准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  面试官：你都熟悉哪些常用的机器学习算法？\n",
    "\n",
    "- 回归算法有：线性回归、Lasso回归、岭回归。分类算法有：逻辑回归、决策树、贝叶斯、SVM、神经网络、集成学习。聚类算法有：k-means、层次聚类、（密度聚类不敢说，因为切实忘了原理过程）。\n",
    "\n",
    "- 线性回归的原理么\n",
    "线性回归假设特征和结果满足线性关系。通过一个映射函数将特征变量与预测结果形成关系。这样就可以表达特征与结果之间的非线性关系。在Andrew Ng老师视频课程中用预测房价作为例子，比如x1=房间的面积，x2=房间的朝向，等等，这样可以写出一个估计函数：\n",
    "\n",
    "其中θ为权重参数，具体含义为所点成的特征变量在整个变量中所占的比重，比重越大，该特征的影响力越大，在实际应用中越值得考虑。\n",
    "\n",
    "- 线性回归的回归模型为 ，就是训练一条直线来拟合我们的训练数据，根据这个训练好的线性函数来预测数值。\n",
    "\n",
    "面试官：你讲解的线性回归是我们理解的形式，你能具体讲解一下线性回归的底层原理，比若说如何训练，如何得到参数，如何调整参数等？\n",
    "\n",
    "- 这个不是很清楚，没有做过具体的项目，但是我对逻辑回归做过深入的研究。\n",
    "\n",
    "面试官：那你就讲解一下逻辑回归的原理吧！\n",
    "\n",
    "- 把逻辑回归从头到尾，详详细细的讲解了一遍。\n",
    "\n",
    "3. 面试官：你再详细的讲解一下朴素贝叶斯的底层原理，比如说，如何选参数，如何训练模型，如何做分类？\n",
    "\n",
    "- 朴素贝叶斯的朴素其实是样例中特征之间相互独立。后边就说不出来了。。。\n",
    "\n",
    "4. 面试官：深度学习中的梯度消失是什么，如何解决？\n",
    "\n",
    "- 详详细细的把梯度消失的现象，造成的实质原因，和三种解决办法讲了一遍。\n",
    "\n",
    "三种解决办法：\n",
    "\n",
    "（1）修改激活函数。\n",
    "\n",
    "（2）用BN。\n",
    "\n",
    "（3）把传统的循环神经网络，换成GRU网络。\n",
    "\n",
    "5. 面试官：神经网络中的word2vec了解么？详细讲解一下它们的原理？\n",
    "\n",
    "- 把word2vec的两个模型：CBOW、skip-gram详细的讲解了一下。\n",
    "\n",
    "（1）面试官：你能详细的说一下CBOW和skip-garm它们的区别么？分别适用于什么场景？\n",
    "\n",
    "我：没回答出来。\n",
    "\n",
    "6. 面试官：数据结构中，什么是平衡二叉树？\n",
    "\n",
    "我：详细的把平衡二叉树概念说了一下。\n",
    "\n",
    "（1）面试官：什么是二叉树？\n",
    "\n",
    "我：一个结点没有子节点，或者是只有左孩子结点、或者只有右孩子结点、或者只有左孩子和右孩子结点。\n",
    "\n",
    "（2）面试官：平衡二叉树的应用都有哪些？\n",
    "\n",
    "我：二叉排序树。（错误的回答。）\n",
    "\n",
    "注意：正确的回答应该是B-树和B+树。\n",
    "\n",
    "7. 面试官：我们来写一道代码题。\n",
    "\n",
    "题目：给你一个很大的文件，文件里有很多行数据，每一行数据是一个用户的uid，表示这个用户点开过抖音，请你找出打开抖音次数最频繁的前10个用户。\n",
    "\n",
    "面试官接着解释题目：假如抖音里面有5亿用户，那么每个用户打开一次抖音就有5亿条记录，如果每个用户打开两次抖音，就有10亿条记录。也就是说，用户每打开一次抖音，就记录一下他的uid。请找出打开抖音次数最频繁的前10个用户。\n",
    "————————————————\n",
    "版权声明：本文为CSDN博主「Microstrong0305」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。\n",
    "原文链接：https://blog.csdn.net/program_developer/java/article/details/83042865"
   ]
  }
 ]
}