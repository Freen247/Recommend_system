{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36764bitrecenvvirtualenvc590a763a4564b6f98e8e2c90348aa96",
   "display_name": "Python 3.6.7 64-bit ('rec_env': virtualenv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "sys.version_info(major=3, minor=6, micro=7, releaselevel='final', serial=0)\ntensorflow 2.1.0\nmatplotlib 3.2.0\nnumpy 1.18.1\npandas 1.0.1\nsklearn 0.22.2.post1\ntensorflow 2.1.0\ntensorflow_core.keras 2.2.4-tf\n"
    }
   ],
   "source": [
    "import os, sys, re\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(sys.version_info)\n",
    "for module in tf, mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Index(['UserID', 'MovieID', 'Gender', 'Age', 'JobID', 'Title', 'Genres'], dtype='object')\nIndex(['ratings'], dtype='object')\n"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Load Dataset from File\n",
    "\"\"\"\n",
    "#读取User数据\n",
    "users_title = ['UserID', 'Gender', 'Age', 'JobID', 'Zip-code']\n",
    "users = pd.read_csv('./data/ml-1m/users.dat', sep='::', header=None, names=users_title, engine = 'python')\n",
    "users = users.filter(regex='UserID|Gender|Age|JobID')\n",
    "users_orig = users.values\n",
    "#改变User数据中性别和年龄\n",
    "gender_map = {'F':0, 'M':1}\n",
    "users['Gender'] = users['Gender'].map(gender_map)\n",
    "\n",
    "age_map = {val:ii for ii,val in enumerate(set(users['Age']))}\n",
    "users['Age'] = users['Age'].map(age_map)\n",
    "\n",
    "#读取Movie数据集\n",
    "movies_title = ['MovieID', 'Title', 'Genres']\n",
    "movies = pd.read_csv('./data/ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
    "movies_orig = movies.values\n",
    "#将Title中的年份去掉\n",
    "pattern = re.compile(r'^(.*)\\((\\d+)\\)$')\n",
    "\n",
    "title_map = {val:pattern.match(val).group(1) for ii,val in enumerate(set(movies['Title']))}\n",
    "movies['Title'] = movies['Title'].map(title_map)\n",
    "\n",
    "#电影类型转数字字典\n",
    "genres_set = set()\n",
    "for val in movies['Genres'].str.split('|'):\n",
    "    genres_set.update(val)\n",
    "\n",
    "genres_set.add('<PAD>')\n",
    "genres2int = {val:ii for ii, val in enumerate(genres_set)}\n",
    "\n",
    "#将电影类型转成等长数字列表，长度是18\n",
    "genres_map = {val:[genres2int[row] for row in val.split('|')] for ii,val in enumerate(set(movies['Genres']))}\n",
    "\n",
    "for key in genres_map:\n",
    "    for cnt in range(max(genres2int.values()) - len(genres_map[key])):\n",
    "        genres_map[key].insert(len(genres_map[key]) + cnt,genres2int['<PAD>'])\n",
    "\n",
    "movies['Genres'] = movies['Genres'].map(genres_map)\n",
    "\n",
    "#电影Title转数字字典\n",
    "title_set = set()\n",
    "for val in movies['Title'].str.split():\n",
    "    title_set.update(val)\n",
    "\n",
    "title_set.add('<PAD>')\n",
    "title2int = {val:ii for ii, val in enumerate(title_set)}\n",
    "\n",
    "#将电影Title转成等长数字列表，长度是15\n",
    "title_count = 15\n",
    "title_map = {val:[title2int[row] for row in val.split()] for ii,val in enumerate(set(movies['Title']))}\n",
    "\n",
    "for key in title_map:\n",
    "    for cnt in range(title_count - len(title_map[key])):\n",
    "        title_map[key].insert(len(title_map[key]) + cnt,title2int['<PAD>'])\n",
    "\n",
    "movies['Title'] = movies['Title'].map(title_map)\n",
    "\n",
    "#读取评分数据集\n",
    "ratings_title = ['UserID','MovieID', 'ratings', 'timestamps']\n",
    "ratings = pd.read_csv('./data/ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
    "ratings = ratings.filter(regex='UserID|MovieID|ratings')\n",
    "\n",
    "#合并三个表\n",
    "data = pd.merge(pd.merge(ratings, users), movies)\n",
    "\n",
    "#将数据分成X和y两张表\n",
    "target_fields = ['ratings']\n",
    "# features.index 为 ['UserID', 'MovieID', 'Gender', 'Age', 'JobID', 'Title', 'Genres']\n",
    "features_pd, targets_pd = data.drop(target_fields, axis=1), data[target_fields]\n",
    "print(features_pd.columns)\n",
    "print(targets_pd.columns)\n",
    "features = features_pd.values\n",
    "targets_values = targets_pd.values\n",
    "\n",
    "\n",
    "#嵌入矩阵的维度\n",
    "embed_dim = 32\n",
    "\n",
    "# features:Index(['UserID', 'MovieID', 'Gender', 'Age', 'JobID', 'Title', 'Genres'], dtype='object')\n",
    "#用户ID个数6040\n",
    "uid_max = max(features.take(0,1)) + 1\n",
    "#性别个数2\n",
    "gender_max = max(features.take(2,1)) + 1\n",
    "#年龄类别个数7\n",
    "age_max = max(features.take(3,1)) + 1 \n",
    "#职业个数21\n",
    "job_max = max(features.take(4,1)) + 1\n",
    "\n",
    "#电影ID个数3952\n",
    "movie_id_max = max(features.take(1,1)) + 1 \n",
    "#电影类型个数19\n",
    "movie_categories_max = max(genres2int.values()) + 1 \n",
    "#电影名单词个数5216\n",
    "movie_title_max = len(title_set) \n",
    "\n",
    "#对电影类型嵌入向量做加和操作的标志，考虑过使用mean做平均，但是没实现mean\n",
    "combiner = \"sum\"\n",
    "\n",
    "#电影名长度\n",
    "sentences_size = title_count # = 15\n",
    "#文本卷积滑动窗口，分别滑动2, 3, 4, 5个单词\n",
    "window_sizes = {2, 3, 4, 5}\n",
    "#文本卷积核数量\n",
    "filter_num = 8\n",
    "\n",
    "#电影ID转下标的字典，数据集中电影ID跟下标不一致，比如第5行的数据电影ID不一定是5\n",
    "movieid2idx = {val[0]:i for i, val in enumerate(movies.values)}\n",
    "\n",
    "\n",
    "\n",
    "# Number of Epochs\n",
    "num_epochs = 5\n",
    "# Batch Size\n",
    "batch_size = 256\n",
    "\n",
    "dropout_keep = 0.5\n",
    "# Learning Rate\n",
    "learning_rate = 0.0001\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 20\n",
    "\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"model_22\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nmovie_titles (InputLayer)       [(None, 15)]         0                                            \n__________________________________________________________________________________________________\nmovie_title_embed_layer (Embedd (None, 15, 32)       166880      movie_titles[0][0]               \n__________________________________________________________________________________________________\nreshape (Reshape)               (None, 15, 32, 1)    0           movie_title_embed_layer[0][0]    \n__________________________________________________________________________________________________\nconv2d (Conv2D)                 (None, 14, 1, 8)     520         reshape[0][0]                    \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 13, 1, 8)     776         reshape[0][0]                    \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 12, 1, 8)     1032        reshape[0][0]                    \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 11, 1, 8)     1288        reshape[0][0]                    \n__________________________________________________________________________________________________\nmovie_categories (InputLayer)   [(None, 18)]         0                                            \n__________________________________________________________________________________________________\nmax_pooling2d (MaxPooling2D)    (None, 1, 1, 8)      0           conv2d[0][0]                     \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 8)      0           conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 8)      0           conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 8)      0           conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nuid (InputLayer)                [(None, 1)]          0                                            \n__________________________________________________________________________________________________\nuser_gender (InputLayer)        [(None, 1)]          0                                            \n__________________________________________________________________________________________________\nuser_age (InputLayer)           [(None, 1)]          0                                            \n__________________________________________________________________________________________________\nuser_job (InputLayer)           [(None, 1)]          0                                            \n__________________________________________________________________________________________________\nmovie_id (InputLayer)           [(None, 1)]          0                                            \n__________________________________________________________________________________________________\nmovie_categories_embed_layer (E (None, 18, 32)       608         movie_categories[0][0]           \n__________________________________________________________________________________________________\npool_layer (Concatenate)        (None, 1, 1, 32)     0           max_pooling2d[0][0]              \n                                                                 max_pooling2d_1[0][0]            \n                                                                 max_pooling2d_2[0][0]            \n                                                                 max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\nuid_embed_layer (Embedding)     (None, 1, 32)        193312      uid[0][0]                        \n__________________________________________________________________________________________________\ngender_embed_layer (Embedding)  (None, 1, 16)        32          user_gender[0][0]                \n__________________________________________________________________________________________________\nage_embed_layer (Embedding)     (None, 1, 16)        112         user_age[0][0]                   \n__________________________________________________________________________________________________\njob_embed_layer (Embedding)     (None, 1, 16)        336         user_job[0][0]                   \n__________________________________________________________________________________________________\nmovie_id_embed_layer (Embedding (None, 1, 32)        126496      movie_id[0][0]                   \n__________________________________________________________________________________________________\nlambda (Lambda)                 (None, 1, 32)        0           movie_categories_embed_layer[0][0\n__________________________________________________________________________________________________\npool_layer_flat (Reshape)       (None, 1, 32)        0           pool_layer[0][0]                 \n__________________________________________________________________________________________________\nuid_fc_layer (Dense)            (None, 1, 32)        1056        uid_embed_layer[0][0]            \n__________________________________________________________________________________________________\ngender_fc_layer (Dense)         (None, 1, 32)        544         gender_embed_layer[0][0]         \n__________________________________________________________________________________________________\nage_fc_layer (Dense)            (None, 1, 32)        544         age_embed_layer[0][0]            \n__________________________________________________________________________________________________\njob_fc_layer (Dense)            (None, 1, 32)        544         job_embed_layer[0][0]            \n__________________________________________________________________________________________________\nmovie_id_fc_layer (Dense)       (None, 1, 32)        1056        movie_id_embed_layer[0][0]       \n__________________________________________________________________________________________________\nmovie_categories_fc_layer (Dens (None, 1, 32)        1056        lambda[0][0]                     \n__________________________________________________________________________________________________\ndropout_layer (Dropout)         (None, 1, 32)        0           pool_layer_flat[0][0]            \n__________________________________________________________________________________________________\nconcatenate_24 (Concatenate)    (None, 1, 128)       0           uid_fc_layer[0][0]               \n                                                                 gender_fc_layer[0][0]            \n                                                                 age_fc_layer[0][0]               \n                                                                 job_fc_layer[0][0]               \n__________________________________________________________________________________________________\nconcatenate_25 (Concatenate)    (None, 1, 96)        0           movie_id_fc_layer[0][0]          \n                                                                 movie_categories_fc_layer[0][0]  \n                                                                 dropout_layer[0][0]              \n__________________________________________________________________________________________________\ndense_24 (Dense)                (None, 1, 200)       25800       concatenate_24[0][0]             \n__________________________________________________________________________________________________\ndense_25 (Dense)                (None, 1, 200)       19400       concatenate_25[0][0]             \n__________________________________________________________________________________________________\nuser_combine_layer_flat (Reshap (None, 200)          0           dense_24[0][0]                   \n__________________________________________________________________________________________________\nmovie_combine_layer_flat (Resha (None, 200)          0           dense_25[0][0]                   \n__________________________________________________________________________________________________\ninference (Lambda)              (None,)              0           user_combine_layer_flat[0][0]    \n                                                                 movie_combine_layer_flat[0][0]   \n__________________________________________________________________________________________________\nlambda_1 (Lambda)               (None, 1)            0           inference[0][0]                  \n==================================================================================================\nTotal params: 541,392\nTrainable params: 541,392\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n"
    }
   ],
   "source": [
    "def get_inputs():\n",
    "    uid = tf.keras.layers.Input(shape=(1,), dtype='int32', name='uid')  \n",
    "    user_gender = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_gender')  \n",
    "    user_age = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_age') \n",
    "    user_job = tf.keras.layers.Input(shape=(1,), dtype='int32', name='user_job')\n",
    "    movie_id = tf.keras.layers.Input(shape=(1,), dtype='int32', name='movie_id') \n",
    "    movie_categories = tf.keras.layers.Input(shape=(18,), dtype='int32', name='movie_categories') \n",
    "    movie_titles = tf.keras.layers.Input(shape=(15,), dtype='int32', name='movie_titles') \n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles\n",
    "\n",
    "def get_user_embedding(uid, user_gender, user_age, user_job):\n",
    "    '''\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length)\n",
    "    1. define matrix : [vocab_size, embed_dim]\n",
    "    2. [1,2,3,4..], max_length * embedding_dim\n",
    "    3. batch_size * max_length * embedding_dim\n",
    "    '''\n",
    "    uid_embed_layer = tf.keras.layers.Embedding(uid_max, embed_dim, input_length=1, name='uid_embed_layer')(uid)\n",
    "    gender_embed_layer = tf.keras.layers.Embedding(gender_max, embed_dim // 2, input_length=1, name='gender_embed_layer')(user_gender)\n",
    "    age_embed_layer = tf.keras.layers.Embedding(age_max, embed_dim // 2, input_length=1, name='age_embed_layer')(user_age)\n",
    "    job_embed_layer = tf.keras.layers.Embedding(job_max, embed_dim // 2, input_length=1, name='job_embed_layer')(user_job)\n",
    "    return uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer\n",
    "\n",
    "def get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer):\n",
    "    #第一层全连接\n",
    "    uid_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"uid_fc_layer\", activation='relu')(uid_embed_layer)\n",
    "    gender_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"gender_fc_layer\", activation='relu')(gender_embed_layer)\n",
    "    age_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"age_fc_layer\", activation='relu')(age_embed_layer)\n",
    "    job_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"job_fc_layer\", activation='relu')(job_embed_layer)\n",
    "\n",
    "    #第二层全连接\n",
    "    user_combine_layer = tf.keras.layers.concatenate([uid_fc_layer, gender_fc_layer, age_fc_layer, job_fc_layer], 2)  #(?, 1, 128)\n",
    "    user_combine_layer = tf.keras.layers.Dense(200, activation='tanh')(user_combine_layer)  #(?, 1, 200)\n",
    "\n",
    "    user_combine_layer_flat = tf.keras.layers.Reshape([200], name=\"user_combine_layer_flat\")(user_combine_layer)\n",
    "    return user_combine_layer, user_combine_layer_flat\n",
    "\n",
    "def get_movie_id_embed_layer(movie_id):\n",
    "    movie_id_embed_layer = tf.keras.layers.Embedding(movie_id_max, embed_dim, input_length=1, name='movie_id_embed_layer')(movie_id)\n",
    "    return movie_id_embed_layer\n",
    "\n",
    "def get_movie_categories_layers(movie_categories):\n",
    "    movie_categories_embed_layer = tf.keras.layers.Embedding(movie_categories_max, embed_dim, input_length=18, name='movie_categories_embed_layer')(movie_categories)\n",
    "    movie_categories_embed_layer = tf.keras.layers.Lambda(lambda layer: tf.reduce_sum(layer, axis=1, keepdims=True))(movie_categories_embed_layer)\n",
    "#     movie_categories_embed_layer = tf.keras.layers.Reshape([1, 18 * embed_dim])(movie_categories_embed_layer)\n",
    "\n",
    "    return movie_categories_embed_layer\n",
    "\n",
    "def get_movie_cnn_layer(movie_titles):\n",
    "    #从嵌入矩阵中得到电影名对应的各个单词的嵌入向量\n",
    "    movie_title_embed_layer = tf.keras.layers.Embedding(movie_title_max, embed_dim, input_length=15, name='movie_title_embed_layer')(movie_titles)\n",
    "    sp=movie_title_embed_layer.shape\n",
    "    movie_title_embed_layer_expand = tf.keras.layers.Reshape([sp[1], sp[2], 1])(movie_title_embed_layer)\n",
    "    #对文本嵌入层使用不同尺寸的卷积核做卷积和最大池化window_sizes = {2, 3, 4, 5}， 文本卷积核数量filter_num = 8， 嵌入矩阵的维度 embed_dim = 32\n",
    "    pool_layer_lst = []\n",
    "    for window_size in window_sizes:\n",
    "        conv_layer = tf.keras.layers.Conv2D(filter_num, (window_size, embed_dim), 1, activation='relu')(movie_title_embed_layer_expand)\n",
    "        maxpool_layer = tf.keras.layers.MaxPooling2D(pool_size=(sentences_size - window_size + 1 ,1), strides=1)(conv_layer)\n",
    "        pool_layer_lst.append(maxpool_layer)\n",
    "    #Dropout层\n",
    "    pool_layer = tf.keras.layers.concatenate(pool_layer_lst, 3, name =\"pool_layer\")  \n",
    "    max_num = len(window_sizes) * filter_num\n",
    "    pool_layer_flat = tf.keras.layers.Reshape([1, max_num], name = \"pool_layer_flat\")(pool_layer)\n",
    "\n",
    "    dropout_layer = tf.keras.layers.Dropout(dropout_keep, name = \"dropout_layer\")(pool_layer_flat)\n",
    "    return pool_layer_flat, dropout_layer\n",
    "\n",
    "def get_movie_feature_layer(movie_id_embed_layer, movie_categories_embed_layer, dropout_layer):\n",
    "    #第一层全连接\n",
    "    movie_id_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"movie_id_fc_layer\", activation='relu')(movie_id_embed_layer)\n",
    "    movie_categories_fc_layer = tf.keras.layers.Dense(embed_dim, name=\"movie_categories_fc_layer\", activation='relu')(movie_categories_embed_layer)\n",
    "\n",
    "    #第二层全连接\n",
    "    movie_combine_layer = tf.keras.layers.concatenate([movie_id_fc_layer, movie_categories_fc_layer, dropout_layer], 2)  \n",
    "    movie_combine_layer = tf.keras.layers.Dense(200, activation='tanh')(movie_combine_layer)\n",
    "\n",
    "    movie_combine_layer_flat = tf.keras.layers.Reshape([200], name=\"movie_combine_layer_flat\")(movie_combine_layer)\n",
    "    return movie_combine_layer, movie_combine_layer_flat\n",
    "\n",
    "# 获取输入占位符\n",
    "uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles = get_inputs()\n",
    "# 获取User的4个嵌入向量\n",
    "uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer = get_user_embedding(uid, user_gender, user_age, user_job)\n",
    "# 得到用户特征\n",
    "user_combine_layer, user_combine_layer_flat = get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer)\n",
    "\n",
    "\n",
    "# 获取电影ID的嵌入向量\n",
    "movie_id_embed_layer = get_movie_id_embed_layer(movie_id)\n",
    "# 获取电影类型的嵌入向量\n",
    "movie_categories_embed_layer = get_movie_categories_layers(movie_categories)\n",
    "# 获取电影名的特征向量\n",
    "pool_layer_flat, dropout_layer = get_movie_cnn_layer(movie_titles)\n",
    "# 得到电影特征\n",
    "movie_combine_layer, movie_combine_layer_flat = get_movie_feature_layer(movie_id_embed_layer, movie_categories_embed_layer, dropout_layer)\n",
    "\n",
    "inference = tf.keras.layers.Lambda(lambda layer: tf.reduce_sum(layer[0] * layer[1], axis=1), name=\"inference\")((user_combine_layer_flat, movie_combine_layer_flat))\n",
    "inference = tf.keras.layers.Lambda(lambda layer: tf.expand_dims(layer, axis=1))(inference)\n",
    "model = tf.keras.Model(\n",
    "    inputs=[uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles],\n",
    "    outputs=[inference]\n",
    ")\n",
    "print(model.summary())\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# batch_size = batch_size\n",
    "# best_loss = 9999\n",
    "# losses = {'train': [], 'test': []}\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "# # MSE损失，将计算值回归到评分\n",
    "# ComputeLoss = tf.keras.losses.MeanSquaredError()\n",
    "# ComputeMetrics = tf.keras.metrics.MeanAbsoluteError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Index(['UserID', 'MovieID', 'Gender', 'Age', 'JobID', 'Title', 'Genres'], dtype='object')\nIndex(['ratings'], dtype='object')\n"
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected movie_categories to have shape (18,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-d1a583f20419>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# data_list = [features.take(_,1) for _ in range(7)].append(categories.astype(np.float32), titles.astype(np.float32))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# print(len(data_list))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mf:\\workspace\\rec_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mf:\\workspace\\rec_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\workspace\\rec_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    550\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m         steps=steps_per_epoch)\n\u001b[0m\u001b[0;32m    553\u001b[0m     (x, y, sample_weights,\n\u001b[0;32m    554\u001b[0m      \u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\workspace\\rec_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2383\u001b[1;33m         batch_size=batch_size)\n\u001b[0m\u001b[0;32m   2384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[1;32mf:\\workspace\\rec_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2408\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2409\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2410\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2412\u001b[0m     \u001b[1;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\workspace\\rec_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    580\u001b[0m                              \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m                              str(data_shape))\n\u001b[0m\u001b[0;32m    583\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected movie_categories to have shape (18,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "\n",
    "# uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles\n",
    "# uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles\n",
    "print(features_pd.columns)\n",
    "print(targets_pd.columns)\n",
    "# t = tf.convert_to_tensor(np.array())\n",
    "# print(t)\n",
    "# ['UserID', 'MovieID', 'Gender', 'Age', 'JobID', 'Title', 'Genres']\n",
    "# data_list = [features.take(_,1) for _ in range(7)].append(categories.astype(np.float32), titles.astype(np.float32))\n",
    "# print(len(data_list))\n",
    "history = model.fit([features.take(_,1) for _ in [0,2,3,4,1,6,5]], targets_values, epochs=30, batch_size=batch_size, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(labels, logits):\n",
    "    return tf.reduce_mean(tf.keras.losses.mse(labels, logits))\n",
    "\n",
    "def compute_metrics(self, labels, logits):\n",
    "    return tf.keras.metrics.mae(labels, logits)  #\n",
    "\n",
    "def get_batches(Xs, ys, batch_size):\n",
    "    for start in range(0, len(Xs), batch_size):\n",
    "        end = min(start + batch_size, len(Xs))\n",
    "        yield Xs[start:end], ys[start:end]\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    # Record the operations used to compute the loss, so that the gradient\n",
    "    # of the loss with respect to the variables can be computed.\n",
    "    #         metrics = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model([x[0],\n",
    "                                x[1],\n",
    "                                x[2],\n",
    "                                x[3],\n",
    "                                x[4],\n",
    "                                x[5],\n",
    "                                x[6]], training=True)\n",
    "        loss = ComputeLoss(y, logits)\n",
    "        # loss = compute_loss(labels, logits)\n",
    "        ComputeMetrics(y, logits)\n",
    "        # metrics = compute_metrics(labels, logits)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss, logits\n",
    "import time\n",
    "def training(features, targets_values, epochs=5, log_freq=50):\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        # 将数据集分成训练集和测试集，随机种子不固定\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        train_X, test_X, train_y, test_y = train_test_split(features, targets_values, test_size=0.2, random_state=0)\n",
    "\n",
    "        train_batches = get_batches(train_X, train_y, batch_size)\n",
    "        batch_num = (len(train_X) // batch_size)\n",
    "\n",
    "\n",
    "        train_start = time.time()\n",
    "        #             with train_summary_writer.as_default():\n",
    "        if True:\n",
    "            start = time.time()\n",
    "            # Metrics are stateful. They accumulate values and return a cumulative\n",
    "            # result when you call .result(). Clear accumulated values with .reset_states()\n",
    "            avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n",
    "            #                 avg_mae = tf.keras.metrics.Mean('mae', dtype=tf.float32)\n",
    "\n",
    "            # Datasets can be iterated over like any other Python iterable.\n",
    "            for batch_i in range(batch_num):\n",
    "                x, y = next(train_batches)\n",
    "                categories = np.zeros([batch_size, 18])\n",
    "                for i in range(batch_size):\n",
    "                    categories[i] = x.take(6, 1)[i]\n",
    "\n",
    "                titles = np.zeros([batch_size, sentences_size])\n",
    "                for i in range(batch_size):\n",
    "                    titles[i] = x.take(5, 1)[i]\n",
    "\n",
    "                loss, logits = train_step([np.reshape(x.take(0, 1), [batch_size, 1]).astype(np.float32),\n",
    "                                                np.reshape(x.take(2, 1), [batch_size, 1]).astype(np.float32),\n",
    "                                                np.reshape(x.take(3, 1), [batch_size, 1]).astype(np.float32),\n",
    "                                                np.reshape(x.take(4, 1), [batch_size, 1]).astype(np.float32),\n",
    "                                                np.reshape(x.take(1, 1), [batch_size, 1]).astype(np.float32),\n",
    "                                                categories.astype(np.float32),\n",
    "                                                titles.astype(np.float32)],\n",
    "                                                np.reshape(y, [batch_size, 1]).astype(np.float32))\n",
    "                avg_loss(loss)\n",
    "                #                     avg_mae(metrics)\n",
    "                losses['train'].append(loss)\n",
    "\n",
    "                if tf.equal(optimizer.iterations % log_freq, 0):\n",
    "                    #                         summary_ops_v2.scalar('loss', avg_loss.result(), step=optimizer.iterations)\n",
    "                    #                         summary_ops_v2.scalar('mae', ComputeMetrics.result(), step=optimizer.iterations)\n",
    "                    # summary_ops_v2.scalar('mae', avg_mae.result(), step=optimizer.iterations)\n",
    "\n",
    "                    rate = log_freq / (time.time() - start)\n",
    "                    print('Step #{}\\tEpoch {:>3} Batch {:>4}/{}   Loss: {:0.6f} mae: {:0.6f} ({} steps/sec)'.format(\n",
    "                        optimizer.iterations.numpy(),\n",
    "                        epoch_i,\n",
    "                        batch_i,\n",
    "                        batch_num,\n",
    "                        loss, (ComputeMetrics.result()), rate))\n",
    "                    # print('Step #{}\\tLoss: {:0.6f} mae: {:0.6f} ({} steps/sec)'.format(\n",
    "                    #     optimizer.iterations.numpy(), loss, (avg_mae.result()), rate))\n",
    "                    avg_loss.reset_states()\n",
    "                    ComputeMetrics.reset_states()\n",
    "                    # avg_mae.reset_states()\n",
    "                    start = time.time()\n",
    "\n",
    "\n",
    "        train_end = time.time()\n",
    "        print(\n",
    "            '\\nTrain time for epoch #{} ({} total steps): {}'.format(epoch_i + 1, optimizer.iterations.numpy(),\n",
    "                                                                        train_end - train_start))\n",
    "        #             with test_summary_writer.as_default():\n",
    "        testing((test_X, test_y), optimizer.iterations)\n",
    "        # checkpoint.save(checkpoint_prefix)\n",
    "    export_path = os.path.join(MODEL_DIR, 'export')\n",
    "    tf.saved_model.save(model, export_path)\n",
    "\n",
    "def testing(test_dataset, step_num):\n",
    "    test_X, test_y = test_dataset\n",
    "    test_batches = get_batches(test_X, test_y, batch_size)\n",
    "\n",
    "    \"\"\"Perform an evaluation of `model` on the examples from `dataset`.\"\"\"\n",
    "    avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n",
    "    #         avg_mae = tf.keras.metrics.Mean('mae', dtype=tf.float32)\n",
    "\n",
    "    batch_num = (len(test_X) // batch_size)\n",
    "    for batch_i in range(batch_num):\n",
    "        x, y = next(test_batches)\n",
    "        categories = np.zeros([batch_size, 18])\n",
    "        for i in range(batch_size):\n",
    "            categories[i] = x.take(6, 1)[i]\n",
    "\n",
    "        titles = np.zeros([batch_size, sentences_size])\n",
    "        for i in range(batch_size):\n",
    "            titles[i] = x.take(5, 1)[i]\n",
    "\n",
    "\n",
    "        logits = model([np.reshape(x.take(0, 1), [batch_size, 1]).astype(np.float32),\n",
    "                                np.reshape(x.take(2, 1), [batch_size, 1]).astype(np.float32),\n",
    "                                np.reshape(x.take(3, 1), [batch_size, 1]).astype(np.float32),\n",
    "                                np.reshape(x.take(4, 1), [batch_size, 1]).astype(np.float32),\n",
    "                                np.reshape(x.take(1, 1), [batch_size, 1]).astype(np.float32),\n",
    "                                categories.astype(np.float32),\n",
    "                                titles.astype(np.float32)], training=False)\n",
    "        test_loss = ComputeLoss(np.reshape(y, [batch_size, 1]).astype(np.float32), logits)\n",
    "        avg_loss(test_loss)\n",
    "        # 保存测试损失\n",
    "        losses['test'].append(test_loss)\n",
    "        ComputeMetrics(np.reshape(y, [batch_size, 1]).astype(np.float32), logits)\n",
    "        # avg_loss(compute_loss(labels, logits))\n",
    "        # avg_mae(compute_metrics(labels, logits))\n",
    "\n",
    "    print('Model test set loss: {:0.6f} mae: {:0.6f}'.format(avg_loss.result(), ComputeMetrics.result()))\n",
    "    # print('Model test set loss: {:0.6f} mae: {:0.6f}'.format(avg_loss.result(), avg_mae.result()))\n",
    "    #         summary_ops_v2.scalar('loss', avg_loss.result(), step=step_num)\n",
    "    #         summary_ops_v2.scalar('mae', ComputeMetrics.result(), step=step_num)\n",
    "    # summary_ops_v2.scalar('mae', avg_mae.result(), step=step_num)\n",
    "\n",
    "    if avg_loss.result() < best_loss:\n",
    "        best_loss = avg_loss.result()\n",
    "        print(\"best loss = {}\".format(best_loss))\n",
    "        checkpoint.save(checkpoint_prefix)\n",
    "\n",
    "def forward(xs):\n",
    "    predictions = model(xs)\n",
    "    # logits = tf.nn.softmax(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training(features, targets_values, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}