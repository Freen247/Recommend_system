{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "sys.version_info(major=3, minor=6, micro=7, releaselevel='final', serial=0)\n"
    }
   ],
   "source": [
    "import os, sys, re, pickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensorflow 2.1.0\nmatplotlib 3.2.0\nnumpy 1.18.1\npandas 1.0.1\nsklearn 0.22.2.post1\ntensorflow 2.1.0\ntensorflow_core.python.keras.api._v2.keras 2.2.4-tf\n"
    }
   ],
   "source": [
    "for module in tf, mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGIN_DATA_DIR = os.getcwd() + '/all_fearures/BX-CSV-Dump/'\n",
    "FILTERED_DATA_DIR = os.getcwd() + '/tmp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoad:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        books_with_blurbs.csv cloumns: ISBN,text,Author,Year,Publisher,Blurb\n",
    "        BX-Book-Ratings.csv cloumns: User-ID,ISBN,Book-Rating\n",
    "        BX-Books.csv cloumns: ISBN,Book-text,Book-Author,Year-Of-Publication,Publisher,Image-URL-S,Image-URL-M,Image-URL-L\n",
    "        BX-Users.csv cloumns: User-ID,Location,Age\n",
    "        '''\n",
    "        self.BX_Users = self.load_origin('BX-Users')\n",
    "        self.BX_Book_Ratings = self.load_origin('BX-Book-Ratings')\n",
    "        self.Books = self.load_origin('books_with_blurbs', ',')\n",
    "        #合并三个表\n",
    "        self.features, self.ISBN2int, self.UserID2int, self.Users = self.get_features()\n",
    "        self.labels = self.features.pop('Book-Rating')\n",
    "\n",
    "    def load_origin(self, \n",
    "        filename: \"根据文件名获取源文件，获取正确得columns、values等值\", \n",
    "        sep: \"因为源文件的分隔方式sep不同，所以通过传参改编分隔方式\"=\"\\\";\\\"\", \n",
    "        )->pd.DataFrame:\n",
    "        '''\n",
    "        获取原始数据，第一遍获取后将用pickle保存到本地，方便日后调用\n",
    "        '''\n",
    "        try:\n",
    "            # 从缓存的文件夹FILTERED_DATA_DIR获取基本被过滤后的文件\n",
    "            pickled_data = pickle.load(open(FILTERED_DATA_DIR+filename+'.p', mode='rb'))\n",
    "            return pickled_data\n",
    "        except FileNotFoundError:\n",
    "            # 如果缓存的文件不存在或者没有，则在源目录ORIGIN_DATA_DIR获取\n",
    "            all_fearures = pd.read_csv(ORIGIN_DATA_DIR+filename+'.csv', engine='python',sep=sep, encoding='utf-8')\n",
    "            # \\\";\\\"  初始过滤的文件\n",
    "            # ,      初始不需要过滤的文件\n",
    "            data_dict = {\"\\\";\\\"\":self.filtrator(all_fearures), ',':all_fearures}\n",
    "            # 因为没获得处理后的文件，所以我们在获取源文件后可以保存一下处理后的文件\n",
    "            pickle.dump((data_dict[sep]), open(FILTERED_DATA_DIR+filename+'.p', 'wb'))\n",
    "            return data_dict[sep]\n",
    "        except UnicodeDecodeError as e:\n",
    "            ''' 测试时经常会出现编码错误，如果尝试更换编码方式无效，可以将编码错误的部分位置重新复制粘贴就可以了，这里我们都默认UTF-8'''\n",
    "            print('UnicodeDecodeError:',e)\n",
    "        except pd.errors.ParserError as e:\n",
    "            print(\"connect error|pandas Error: %s\" % e)\n",
    "\n",
    "    def filtrator(self, \n",
    "        f_data: \"输入需要进行初步filter的数据\"\n",
    "        )->pd.DataFrame:\n",
    "        '''\n",
    "        源文件中的columns和各个值得第一列的第一个字符和最后一列的最后一个字符都带有双引号‘\"’,需要将其filter,Location字段当用户Age为null的时候，末尾会有\\\";NULL字符串 ，直接用切片调整\n",
    "        '''\n",
    "        Nonetype_age = 0\n",
    "        f_data = f_data.rename(columns={f_data.columns[0]:f_data.columns[0][1:], f_data.columns[-1]:f_data.columns[-1][:-1]})\n",
    "        f_data[f_data.columns[0]] = f_data[f_data.columns[0]].map(lambda v:v[1:] if v!=None else Nonetype_age)\n",
    "        f_data[f_data.columns[-1]] = f_data[f_data.columns[-1]].map(lambda v:v[:-1] if v!=None else Nonetype_age)\n",
    "        try:\n",
    "            f_data = f_data[f_data['Location'].notnull()][f_data[f_data['Location'].notnull()]['Location'].str.contains('\\\";NULL')]\n",
    "            f_data['Location'] = f_data['Location'].map(lambda location:location[:-6])\n",
    "        except:\n",
    "            pass\n",
    "        return f_data\n",
    "\n",
    "    def get_features(self):\n",
    "        '''\n",
    "        获取整个数据集的所有features，并对每个文本字段作xxxxx\n",
    "        User-ID、Location、ISBN、Book-Rating、Title、Author、Year、Publisher、Blurb\n",
    "        '''\n",
    "        try:\n",
    "            # 从缓存的文件夹FILTERED_DATA_DIR获取features的文件\n",
    "            all_fearures, ISBN2int, UserID2int, Users = pickle.load(open(FILTERED_DATA_DIR+'features.p', mode='rb'))\n",
    "            return all_fearures, ISBN2int, UserID2int, Users\n",
    "        except:\n",
    "            # 将所有的数据组成features大表\n",
    "            all_fearures = pd.merge(pd.merge(self.BX_Users, self.BX_Book_Ratings), self.Books)\n",
    "            Users = all_fearures\n",
    "            # 因为没获得处理后的文件，所以我们在获取源文件后可以保存一下处理后的文件\n",
    "            # isbn2index userid2index\n",
    "            all_fearures.pop('Age')\n",
    "            all_fearures['Title'] = self.feature2int(all_fearures['Title'], 'text')\n",
    "            all_fearures['Blurb'] = self.feature2int(all_fearures['Blurb'], 'text')\n",
    "            all_fearures['ISBN'], ISBN2int = self.feature2int(all_fearures['ISBN'], 'word')\n",
    "            all_fearures['Author'], X2int = self.feature2int(all_fearures['Author'], 'word')\n",
    "            all_fearures['Publisher'], X2int = self.feature2int(all_fearures['Publisher'], 'word')\n",
    "            all_fearures['Year'], X2int = self.feature2int(all_fearures['Year'], 'word')\n",
    "            all_fearures['User-ID'], UserID2int  = self.feature2int(all_fearures['User-ID'], 'word')\n",
    "            all_fearures['Location'] = self.feature2int(all_fearures['Location'], 'list')\n",
    "            all_fearures['Book-Rating'] = all_fearures['Book-Rating'].astype('float32')\n",
    "            pickle.dump((all_fearures, ISBN2int, UserID2int, Users), open(FILTERED_DATA_DIR+'features.p', 'wb'))\n",
    "            return all_fearures, ISBN2int, UserID2int, Users\n",
    "\n",
    "    def feature2int(self, \n",
    "        feature:'特征值',\n",
    "        feature_type:'text/word/list'):\n",
    "        '''\n",
    "        将文本字段比如title、blurb只取英文单词，并用空格为分隔符，做成一个带index值的集合，并用index值表示各个单词，作为文本得表示\n",
    "        '''\n",
    "        pattern = re.compile(r'[^a-zA-Z]')\n",
    "        filtered_map = {val:re.sub(pattern, ' ', str(val)) for ii,val in enumerate(set(feature)) }\n",
    "        letter_filter = lambda feature:feature.map({val:re.sub(pattern, ' ', str(val)) for ii,val in enumerate(set(feature)) })\n",
    "        text_words = set()\n",
    "        filtered_feature = letter_filter(feature)\n",
    "        for val in filtered_feature.str.split():\n",
    "            text_words.update(val)\n",
    "        text2int = {val:ii for ii, val in enumerate(text_words)}\n",
    "        text_map = {val:[text2int[row] for row in filtered_map[val].split()][:200] for ii,val in enumerate(set(feature))}\n",
    "        \n",
    "        word_map = {val:ii for ii,val in enumerate(set(feature))}\n",
    "        try:\n",
    "            cities = set()\n",
    "            for val in feature.str.split(','):\n",
    "                cities.update(val)\n",
    "            city2int = {val:ii for ii, val in enumerate(cities)}\n",
    "            list_map = {val:[city2int[row] for row in val.split(',')][:3] for ii,val in enumerate(set(feature))}\n",
    "        except AttributeError :\n",
    "            list_map = {}\n",
    "\n",
    "        feature_dict = {\n",
    "            'text':(feature.map(text_map)),\n",
    "            'word':(feature.map(word_map), word_map),\n",
    "            'list':(feature.map(list_map)),\n",
    "            }\n",
    "        return feature_dict[feature_type]\n",
    "\n",
    "    def __del__(self):\n",
    "        pass\n",
    "\n",
    "origin_DATA = DataLoad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_DATA = DataLoad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        User-ID            Location   ISBN  \\\n0         10848    [233, 6958, 126]  27711   \n1         10848    [233, 6958, 126]  26782   \n2         20979    [685, 3135, 126]  26782   \n3         17973  [5771, 3020, 5545]  26782   \n4          3664  [3317, 5469, 5545]  26782   \n...         ...                 ...    ...   \n172097    22010   [7222, 523, 5545]   5218   \n172098    23589  [3673, 2439, 5545]  23125   \n172099    23589  [3673, 2439, 5545]  14767   \n172100    23589  [3673, 2439, 5545]  17882   \n172101     3366  [6439, 3932, 1174]  22608   \n\n                                                    Title  Author  Year  \\\n0                                    [12058, 9896, 14641]    8308    63   \n1       [1415, 16421, 23247, 20764, 12094, 7631, 22863...    9858    71   \n2       [1415, 16421, 23247, 20764, 12094, 7631, 22863...    9858    71   \n3       [1415, 16421, 23247, 20764, 12094, 7631, 22863...    9858    71   \n4       [1415, 16421, 23247, 20764, 12094, 7631, 22863...    9858    71   \n...                                                   ...     ...   ...   \n172097  [4790, 20406, 16696, 8493, 3869, 2277, 8493, 3...    9355    70   \n172098  [4853, 12840, 2200, 21361, 16696, 18138, 14311...   13154    75   \n172099                                       [5478, 6884]   12470    75   \n172100                         [11475, 4193, 10880, 9234]   12306    76   \n172101                              [15532, 17750, 22475]     715    72   \n\n        Publisher                                              Blurb  \n0             939  [118142, 80291, 32356, 126011, 41964, 89996, 3...  \n1             652  [107313, 25552, 112071, 2624, 119018, 32356, 3...  \n2             652  [107313, 25552, 112071, 2624, 119018, 32356, 3...  \n3             652  [107313, 25552, 112071, 2624, 119018, 32356, 3...  \n4             652  [107313, 25552, 112071, 2624, 119018, 32356, 3...  \n...           ...                                                ...  \n172097       2763  [86013, 74054, 45479, 32356, 72651, 80940, 818...  \n172098        193  [103017, 93229, 52369, 111950, 101746, 45479, ...  \n172099       2889  [33424, 7818, 111090, 45479, 38754, 42924, 588...  \n172100       2366  [43101, 88468, 31146, 10384, 5881, 72843, 1658...  \n172101       2318  [14371, 92445, 44663, 76001, 6781, 82195, 3489...  \n\n[172102 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>User-ID</th>\n      <th>Location</th>\n      <th>ISBN</th>\n      <th>Title</th>\n      <th>Author</th>\n      <th>Year</th>\n      <th>Publisher</th>\n      <th>Blurb</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10848</td>\n      <td>[233, 6958, 126]</td>\n      <td>27711</td>\n      <td>[12058, 9896, 14641]</td>\n      <td>8308</td>\n      <td>63</td>\n      <td>939</td>\n      <td>[118142, 80291, 32356, 126011, 41964, 89996, 3...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10848</td>\n      <td>[233, 6958, 126]</td>\n      <td>26782</td>\n      <td>[1415, 16421, 23247, 20764, 12094, 7631, 22863...</td>\n      <td>9858</td>\n      <td>71</td>\n      <td>652</td>\n      <td>[107313, 25552, 112071, 2624, 119018, 32356, 3...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20979</td>\n      <td>[685, 3135, 126]</td>\n      <td>26782</td>\n      <td>[1415, 16421, 23247, 20764, 12094, 7631, 22863...</td>\n      <td>9858</td>\n      <td>71</td>\n      <td>652</td>\n      <td>[107313, 25552, 112071, 2624, 119018, 32356, 3...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>17973</td>\n      <td>[5771, 3020, 5545]</td>\n      <td>26782</td>\n      <td>[1415, 16421, 23247, 20764, 12094, 7631, 22863...</td>\n      <td>9858</td>\n      <td>71</td>\n      <td>652</td>\n      <td>[107313, 25552, 112071, 2624, 119018, 32356, 3...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3664</td>\n      <td>[3317, 5469, 5545]</td>\n      <td>26782</td>\n      <td>[1415, 16421, 23247, 20764, 12094, 7631, 22863...</td>\n      <td>9858</td>\n      <td>71</td>\n      <td>652</td>\n      <td>[107313, 25552, 112071, 2624, 119018, 32356, 3...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>172097</th>\n      <td>22010</td>\n      <td>[7222, 523, 5545]</td>\n      <td>5218</td>\n      <td>[4790, 20406, 16696, 8493, 3869, 2277, 8493, 3...</td>\n      <td>9355</td>\n      <td>70</td>\n      <td>2763</td>\n      <td>[86013, 74054, 45479, 32356, 72651, 80940, 818...</td>\n    </tr>\n    <tr>\n      <th>172098</th>\n      <td>23589</td>\n      <td>[3673, 2439, 5545]</td>\n      <td>23125</td>\n      <td>[4853, 12840, 2200, 21361, 16696, 18138, 14311...</td>\n      <td>13154</td>\n      <td>75</td>\n      <td>193</td>\n      <td>[103017, 93229, 52369, 111950, 101746, 45479, ...</td>\n    </tr>\n    <tr>\n      <th>172099</th>\n      <td>23589</td>\n      <td>[3673, 2439, 5545]</td>\n      <td>14767</td>\n      <td>[5478, 6884]</td>\n      <td>12470</td>\n      <td>75</td>\n      <td>2889</td>\n      <td>[33424, 7818, 111090, 45479, 38754, 42924, 588...</td>\n    </tr>\n    <tr>\n      <th>172100</th>\n      <td>23589</td>\n      <td>[3673, 2439, 5545]</td>\n      <td>17882</td>\n      <td>[11475, 4193, 10880, 9234]</td>\n      <td>12306</td>\n      <td>76</td>\n      <td>2366</td>\n      <td>[43101, 88468, 31146, 10384, 5881, 72843, 1658...</td>\n    </tr>\n    <tr>\n      <th>172101</th>\n      <td>3366</td>\n      <td>[6439, 3932, 1174]</td>\n      <td>22608</td>\n      <td>[15532, 17750, 22475]</td>\n      <td>715</td>\n      <td>72</td>\n      <td>2318</td>\n      <td>[14371, 92445, 44663, 76001, 6781, 82195, 3489...</td>\n    </tr>\n  </tbody>\n</table>\n<p>172102 rows × 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "origin_DATA.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(172102, 8)"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "origin_DATA.features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "all user id =  28836\nall location =  7574\n"
    }
   ],
   "source": [
    "# user-id的字典,总共有28836个用户\n",
    "all_user = len(set(origin_DATA.features['User-ID']))\n",
    "new_user_id = {val: i for i, val in enumerate(set(origin_DATA.features['User-ID']))}\n",
    "print('all user id = ', all_user)\n",
    "# location的数量=7573(从0开始的)\n",
    "all_location = max([j for i in origin_DATA.features.Location for j in i]) +1 \n",
    "print('all location = ', all_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "all isbn =  38036\nall author =  15196\nall year =  81\nall publisher =  2909\nall title words =  23815\nall blurb words =  127185\n"
    }
   ],
   "source": [
    "# ISBN总数\n",
    "all_isbn = len(set(origin_DATA.features['ISBN']))\n",
    "print('all isbn = ', all_isbn)\n",
    "# author总数\n",
    "all_author = len(set(origin_DATA.features['Author']))\n",
    "print('all author = ', all_author)\n",
    "# year总数\n",
    "all_year = len(set(origin_DATA.features['Year']))\n",
    "print('all year = ', all_year)\n",
    "# publish总数\n",
    "all_publisher = len(set(origin_DATA.features['Publisher']))\n",
    "print('all publisher = ', all_publisher)\n",
    "# title中所有单词总数\n",
    "all_title_words = max([j for i in origin_DATA.features.Title for j in i]) +1 \n",
    "print('all title words = ', all_title_words)\n",
    "# blurb中所有单词总数\n",
    "all_blurb_words = max([j for i in origin_DATA.features.Blurb for j in i]) +1 \n",
    "print('all blurb words = ', all_blurb_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    # 用户特征输入\n",
    "    user_id = keras.layers.Input(shape=(1,), dtype='int32', name='user_id_input')\n",
    "    user_location = keras.layers.Input(shape=(3,), dtype='int32', name='user_location_input')\n",
    "    \n",
    "    # 书籍特征输入\n",
    "    book_isbn = keras.layers.Input(shape=(1,),  dtype='int32', name='book_isbn_input')\n",
    "    book_author = keras.layers.Input(shape=(1,),  dtype='int32', name='book_author_input')\n",
    "    book_year = keras.layers.Input(shape=(1,),  dtype='int32', name='book_year_input')\n",
    "    book_publisher = keras.layers.Input(shape=(1,),  dtype='int32', name='book_publisher_input')  \n",
    "    book_title = keras.layers.Input(shape=(15, ), dtype='int32', name='book_title_input')\n",
    "    book_blurb = keras.layers.Input(shape=(200, ), dtype='int32', name='book_blurb_input')\n",
    "    return user_id, user_location, book_isbn, book_author, book_year, book_publisher, book_title, book_blurb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 嵌入矩阵的维度\n",
    "embed_dim = 16\n",
    "embed_dim_words = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_embed_layer(u_id, u_loca):\n",
    "    user_id_embedd = keras.layers.Embedding(all_user, embed_dim, name='user_id_embedding')(u_id)\n",
    "    user_loca_embedd = keras.layers.Embedding(all_location, embed_dim , name='user_loca_embedding')(u_loca)\n",
    "    return user_id_embedd, user_loca_embedd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_emded_layer(b_isbn, b_atuhor, b_year, b_publisher, b_title, b_blurb):\n",
    "    book_isbn_embedd = keras.layers.Embedding(all_isbn, embed_dim, name='book_isbn_embedding')(b_isbn)\n",
    "    book_author_embedd = keras.layers.Embedding(all_author, embed_dim, name='book_author_embedding')(b_atuhor)\n",
    "    book_year_embedd = keras.layers.Embedding(all_year, embed_dim, name='book_year_embedding')(b_year)\n",
    "    book_publisher_embedd = keras.layers.Embedding(all_publisher, embed_dim, name='book_publisher_embedding')(b_publisher)\n",
    "    \n",
    "    book_title_embedd = keras.layers.Embedding(all_title_words, embed_dim_words, name='book_title_embedding')(b_title)\n",
    "    book_blurb_embedd = keras.layers.Embedding(all_blurb_words, embed_dim_words, name='book_blurb_embedding')(b_blurb)\n",
    "    return book_isbn_embedd, book_author_embedd, book_year_embedd, book_publisher_embedd, book_title_embedd, book_blurb_embedd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_feature(u_id_embedd, u_loca_embedd):\n",
    "    u_id_layer = keras.layers.Dense(64, activation='relu', name='u_id_dense')(u_id_embedd)\n",
    "    # u_id_layer.shape = (?, 1, 64)\n",
    "    # u_loca_layer.shape = (?, 64)\n",
    "    # 这里可以再加个Dense\n",
    "    u_loca_layer = keras.layers.LSTM(32, go_backwards=False, name='u_loca_lstm')(u_loca_embedd)\n",
    "    u_loca_layer_lstm = keras.layers.Dense(64, activation='relu', name='u_loca_layer_lstm')(u_loca_layer)\n",
    "    u_id_reshape = keras.layers.Reshape([64])(u_id_layer)\n",
    "    u_combine = keras.layers.concatenate([u_id_reshape, u_loca_layer_lstm],axis=1, name='u_combine')\n",
    "    print(u_combine.shape)\n",
    "    # 这里能不能用激活函数\n",
    "    u_feature_layer = keras.layers.Dense(200, name='u_feature_layer')(u_combine)\n",
    "    print(u_feature_layer.shape)\n",
    "    return u_feature_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_dense = 16\n",
    "def get_book_feature(b_isbn_embedd, b_author_embedd, b_year_embedd, b_publisher_embedd, b_title_embedd, b_blurb_embedd):\n",
    "    # 首先对前4个特征连接Dense层\n",
    "    b_isbn_dense = keras.layers.Dense(b_dense, activation='relu', name='b_isbn_dense')(b_isbn_embedd)\n",
    "    b_author_dense = keras.layers.Dense(b_dense, activation='relu', name='b_author_dense')(b_author_embedd)\n",
    "    b_year_dense = keras.layers.Dense(b_dense, activation='relu', name='b_year_dense')(b_year_embedd)\n",
    "    b_publisher_dense = keras.layers.Dense(b_dense, activation='relu', name='b_publisher_dense')(b_publisher_embedd)\n",
    "    # 合并这四个特征,  b_combine_four shape = (?, 1, 64)\n",
    "    b_combine_four = keras.layers.concatenate([b_isbn_dense, b_author_dense, b_year_dense, b_publisher_dense], name='b_four_combine')\n",
    "    print('b_combine_four.shape', b_combine_four.shape)\n",
    "    # 对title进行卷积\n",
    "    b_title_reshape = keras.layers.Lambda(lambda layer: tf.expand_dims(layer, 3))(b_title_embedd)  # shape=(?,15, 32, 1)\n",
    "    print('b_title_reshape.shape = ', b_title_reshape.shape)\n",
    "    b_title_conv = keras.layers.Conv2D(filters=8, kernel_size=(2, embed_dim_words), strides=1)(b_title_reshape)# shape=(?, 14, 1, 8)\n",
    "    b_title_pool = keras.layers.MaxPool2D(pool_size=(14, 1), strides=1)(b_title_conv) # shape=(?,1, 1, 8)\n",
    "    print('b_title_conv.shape = ', b_title_conv)\n",
    "    print('b_title_pool.shape = ', b_title_pool)\n",
    "    \n",
    "    # 对blurb进行处理\n",
    "    b_blurb_lstm = keras.layers.LSTM(32, name='b_blurb_lstm')(b_blurb_embedd) # shape = (?, 32)\n",
    "    print('b_blurb_lstm.shape = ', b_blurb_lstm.shape)\n",
    "    # 将title和blurb合并\n",
    "    b_title_reshape = keras.layers.Reshape([b_title_pool.shape[3]])(b_title_pool)\n",
    "    # b_combine_blurb_title.shape = (?, 40)\n",
    "    b_combine_blurb_title = keras.layers.concatenate([b_title_reshape, b_blurb_lstm], axis=1, name='b_combine_blurb_title')\n",
    "    print('b_combine_blurb_title.shape', b_combine_blurb_title.shape)\n",
    "    b_blurb_title_dense = keras.layers.Dense(64, activation='relu', name='b_blurb_title_dense')(b_combine_blurb_title)\n",
    "    # b_combine_four_reshape shape = (?, 64)\n",
    "    b_combine_four_reshape = keras.layers.Reshape([b_combine_four.shape[2]], name='b_combine_four_reshape')(b_combine_four)\n",
    "    # 合并所有的书籍特征\n",
    "    b_combine_book = keras.layers.concatenate([b_combine_blurb_title, b_combine_four_reshape], axis=1, name='b_combine_book')\n",
    "    # 得到书籍矩阵\n",
    "    b_feature_layer = keras.layers.Dense(200, name='b_feature_layer')(b_combine_book)\n",
    "    return b_feature_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating(user_feature, book_feature):\n",
    "    multiply_layer = keras.layers.Lambda(lambda layer: tf.reduce_sum(layer[0]*layer[1], axis=1, keepdims=True), name = 'user_book_feature')((user_feature, book_feature))\n",
    "    print(multiply_layer.shape)\n",
    "    return multiply_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (3) into shape (15)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-9e41425bf5a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mloca\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morigin_DATA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Location'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtitle\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morigin_DATA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mblurb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morigin_DATA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Blurb'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloca\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (3) into shape (15)"
     ]
    }
   ],
   "source": [
    "m = len(origin_DATA.features['Location'])\n",
    "# 对location取3位数\n",
    "loca = np.zeros((m, 3))\n",
    "title = np.zeros((m, 15))\n",
    "blurb = np.zeros((m, 200))\n",
    "for i in range(m):\n",
    "    loca[i] = np.array(origin_DATA.features['Location'][i])\n",
    "    title[i] = np.array(origin_DATA.features['Title'][i])\n",
    "    blurb[i] = np.array(origin_DATA.features['Blurb'][i])\n",
    "print(loca[:-2])\n",
    "print(title[:-2])\n",
    "print(blurb[:-2])\n",
    "input_features = [origin_DATA.features['User-ID'].to_numpy(), loca, \n",
    "                  origin_DATA.features['ISBN'].to_numpy(), origin_DATA.features['Author'].to_numpy(),\n",
    "                 origin_DATA.features['Year'].to_numpy(), origin_DATA.features['Publisher'].to_numpy(), \n",
    "                 title, blurb]\n",
    "labels = origin_DATA.labels.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'input_features' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-2cd02072f7f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'input_features' is not defined"
     ]
    }
   ],
   "source": [
    "print(input_features[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './model/'\n",
    "\n",
    "class model_network():\n",
    "    def __init__(self):\n",
    "        self.batchsize = 256\n",
    "        self.epoch = 5\n",
    "        self.best_loss = 999\n",
    "    def creat_model(self):\n",
    "        user_id, user_location, book_isbn, book_author, book_year, book_publisher, book_title, book_blurb = get_inputs()\n",
    "        user_id_embedd, user_loca_embedd = user_embed_layer(user_id, user_location)\n",
    "        book_isbn_embedd, book_author_embedd, book_year_embedd, book_publisher_embedd, book_title_embedd, book_blurb_embedd = book_emded_layer(book_isbn, book_author, book_year, book_publisher, book_title, book_blurb)\n",
    "        u_feature_layer = get_user_feature(user_id_embedd, user_loca_embedd)\n",
    "        b_feature_layer = get_book_feature(book_isbn_embedd, book_author_embedd, book_year_embedd, book_publisher_embedd, book_title_embedd, book_blurb_embedd)\n",
    "        multiply_layer = get_rating(u_feature_layer, b_feature_layer)\n",
    "        model = keras.Model(inputs=[user_id, user_location, book_isbn, book_author, book_year, book_publisher, book_title, book_blurb],\n",
    "                    outputs=[multiply_layer])\n",
    "        return model\n",
    "    def train_model(self):\n",
    "        model = self.creat_model()\n",
    "        model.compile(optimizer='adam', loss=keras.losses.mae)\n",
    "        model.fit(input_features, labels, epochs=5, batch_size=512)\n",
    "        print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(None, 128)\n(None, 200)\nb_combine_four.shape (None, 1, 64)\nb_title_reshape.shape =  (None, 15, 32, 1)\nb_title_conv.shape =  Tensor(\"conv2d/Identity:0\", shape=(None, 14, 1, 8), dtype=float32)\nb_title_pool.shape =  Tensor(\"max_pooling2d/Identity:0\", shape=(None, 1, 1, 8), dtype=float32)\nb_blurb_lstm.shape =  (None, 32)\nb_combine_blurb_title.shape (None, 40)\n(None, 1)\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'input_features' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-efb01302d46c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-8db2d748cabc>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreat_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmae\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_features' is not defined"
     ]
    }
   ],
   "source": [
    "m = model_network()\n",
    "m.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(None, 128)\n(None, 200)\nb_combine_four.shape (None, 1, 64)\nb_title_reshape.shape =  (None, 15, 32, 1)\nb_title_conv.shape =  Tensor(\"conv2d_1/Identity:0\", shape=(None, 14, 1, 8), dtype=float32)\nb_title_pool.shape =  Tensor(\"max_pooling2d_1/Identity:0\", shape=(None, 1, 1, 8), dtype=float32)\nb_blurb_lstm.shape =  (None, 32)\nb_combine_blurb_title.shape (None, 40)\n(None, 1)\nModel: \"model_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nbook_title_input (InputLayer)   [(None, 15)]         0                                            \n__________________________________________________________________________________________________\nbook_title_embedding (Embedding (None, 15, 32)       762080      book_title_input[0][0]           \n__________________________________________________________________________________________________\nlambda_1 (Lambda)               (None, 15, 32, 1)    0           book_title_embedding[0][0]       \n__________________________________________________________________________________________________\nbook_isbn_input (InputLayer)    [(None, 1)]          0                                            \n__________________________________________________________________________________________________\nbook_author_input (InputLayer)  [(None, 1)]          0                                            \n__________________________________________________________________________________________________\nbook_year_input (InputLayer)    [(None, 1)]          0                                            \n__________________________________________________________________________________________________\nbook_publisher_input (InputLaye [(None, 1)]          0                                            \n__________________________________________________________________________________________________\nuser_id_input (InputLayer)      [(None, 1)]          0                                            \n__________________________________________________________________________________________________\nuser_location_input (InputLayer [(None, 3)]          0                                            \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 14, 1, 8)     520         lambda_1[0][0]                   \n__________________________________________________________________________________________________\nbook_blurb_input (InputLayer)   [(None, 200)]        0                                            \n__________________________________________________________________________________________________\nbook_isbn_embedding (Embedding) (None, 1, 16)        608576      book_isbn_input[0][0]            \n__________________________________________________________________________________________________\nbook_author_embedding (Embeddin (None, 1, 16)        243136      book_author_input[0][0]          \n__________________________________________________________________________________________________\nbook_year_embedding (Embedding) (None, 1, 16)        1296        book_year_input[0][0]            \n__________________________________________________________________________________________________\nbook_publisher_embedding (Embed (None, 1, 16)        46544       book_publisher_input[0][0]       \n__________________________________________________________________________________________________\nuser_id_embedding (Embedding)   (None, 1, 16)        461376      user_id_input[0][0]              \n__________________________________________________________________________________________________\nuser_loca_embedding (Embedding) (None, 3, 16)        121184      user_location_input[0][0]        \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 8)      0           conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nbook_blurb_embedding (Embedding (None, 200, 32)      4069920     book_blurb_input[0][0]           \n__________________________________________________________________________________________________\nb_isbn_dense (Dense)            (None, 1, 16)        272         book_isbn_embedding[0][0]        \n__________________________________________________________________________________________________\nb_author_dense (Dense)          (None, 1, 16)        272         book_author_embedding[0][0]      \n__________________________________________________________________________________________________\nb_year_dense (Dense)            (None, 1, 16)        272         book_year_embedding[0][0]        \n__________________________________________________________________________________________________\nb_publisher_dense (Dense)       (None, 1, 16)        272         book_publisher_embedding[0][0]   \n__________________________________________________________________________________________________\nu_id_dense (Dense)              (None, 1, 64)        1088        user_id_embedding[0][0]          \n__________________________________________________________________________________________________\nu_loca_lstm (LSTM)              (None, 32)           6272        user_loca_embedding[0][0]        \n__________________________________________________________________________________________________\nreshape_3 (Reshape)             (None, 8)            0           max_pooling2d_1[0][0]            \n__________________________________________________________________________________________________\nb_blurb_lstm (LSTM)             (None, 32)           8320        book_blurb_embedding[0][0]       \n__________________________________________________________________________________________________\nb_four_combine (Concatenate)    (None, 1, 64)        0           b_isbn_dense[0][0]               \n                                                                 b_author_dense[0][0]             \n                                                                 b_year_dense[0][0]               \n                                                                 b_publisher_dense[0][0]          \n__________________________________________________________________________________________________\nreshape_2 (Reshape)             (None, 64)           0           u_id_dense[0][0]                 \n__________________________________________________________________________________________________\nu_loca_layer_lstm (Dense)       (None, 64)           2112        u_loca_lstm[0][0]                \n__________________________________________________________________________________________________\nb_combine_blurb_title (Concaten (None, 40)           0           reshape_3[0][0]                  \n                                                                 b_blurb_lstm[0][0]               \n__________________________________________________________________________________________________\nb_combine_four_reshape (Reshape (None, 64)           0           b_four_combine[0][0]             \n__________________________________________________________________________________________________\nu_combine (Concatenate)         (None, 128)          0           reshape_2[0][0]                  \n                                                                 u_loca_layer_lstm[0][0]          \n__________________________________________________________________________________________________\nb_combine_book (Concatenate)    (None, 104)          0           b_combine_blurb_title[0][0]      \n                                                                 b_combine_four_reshape[0][0]     \n__________________________________________________________________________________________________\nu_feature_layer (Dense)         (None, 200)          25800       u_combine[0][0]                  \n__________________________________________________________________________________________________\nb_feature_layer (Dense)         (None, 200)          21000       b_combine_book[0][0]             \n__________________________________________________________________________________________________\nuser_book_feature (Lambda)      (None, 1)            0           u_feature_layer[0][0]            \n                                                                 b_feature_layer[0][0]            \n==================================================================================================\nTotal params: 6,380,312\nTrainable params: 6,380,312\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "model = m.creat_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
    }
   ],
   "source": [
    "keras.utils.plot_model(model, to_file='model_1.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}