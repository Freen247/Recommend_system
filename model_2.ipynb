{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved by word2vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改进思路：1\n",
    "## 加入停用词，blurb中的简介利用word2vec训练的向量替代，效果比原版提升了10%， 在15代的时候达到最好。loss=, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改进思路：2\n",
    "## 改变模型架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=7, micro=6, releaselevel='final', serial=0)\n",
      "tensorflow 2.0.0\n",
      "matplotlib 3.1.1\n",
      "numpy 1.16.4\n",
      "pandas 0.25.3\n",
      "sklearn 0.22.1\n",
      "tensorflow 2.0.0\n",
      "tensorflow_core.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import os, sys, re, pickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(sys.version_info)\n",
    "for module in tf, mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Location</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Year</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Blurb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>866</td>\n",
       "      <td>[6235, 579, 6140]</td>\n",
       "      <td>13770</td>\n",
       "      <td>[2732, 20340, 20672]</td>\n",
       "      <td>1909</td>\n",
       "      <td>64</td>\n",
       "      <td>2234</td>\n",
       "      <td>[100498, 62786, 1003, 86311, 97903, 118024, 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>866</td>\n",
       "      <td>[6235, 579, 6140]</td>\n",
       "      <td>13446</td>\n",
       "      <td>[1463, 21098, 16110, 16549, 329, 12926, 10988,...</td>\n",
       "      <td>11314</td>\n",
       "      <td>72</td>\n",
       "      <td>2867</td>\n",
       "      <td>[56570, 52866, 17228, 67961, 44141, 1003, 3674...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8127</td>\n",
       "      <td>[819, 1898, 6140]</td>\n",
       "      <td>13446</td>\n",
       "      <td>[1463, 21098, 16110, 16549, 329, 12926, 10988,...</td>\n",
       "      <td>11314</td>\n",
       "      <td>72</td>\n",
       "      <td>2867</td>\n",
       "      <td>[56570, 52866, 17228, 67961, 44141, 1003, 3674...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12958</td>\n",
       "      <td>[554, 2975, 3542]</td>\n",
       "      <td>13446</td>\n",
       "      <td>[1463, 21098, 16110, 16549, 329, 12926, 10988,...</td>\n",
       "      <td>11314</td>\n",
       "      <td>72</td>\n",
       "      <td>2867</td>\n",
       "      <td>[56570, 52866, 17228, 67961, 44141, 1003, 3674...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6447</td>\n",
       "      <td>[1494, 2532, 3542]</td>\n",
       "      <td>13446</td>\n",
       "      <td>[1463, 21098, 16110, 16549, 329, 12926, 10988,...</td>\n",
       "      <td>11314</td>\n",
       "      <td>72</td>\n",
       "      <td>2867</td>\n",
       "      <td>[56570, 52866, 17228, 67961, 44141, 1003, 3674...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID            Location   ISBN  \\\n",
       "0      866   [6235, 579, 6140]  13770   \n",
       "1      866   [6235, 579, 6140]  13446   \n",
       "2     8127   [819, 1898, 6140]  13446   \n",
       "3    12958   [554, 2975, 3542]  13446   \n",
       "4     6447  [1494, 2532, 3542]  13446   \n",
       "\n",
       "                                               Title  Author  Year  Publisher  \\\n",
       "0                               [2732, 20340, 20672]    1909    64       2234   \n",
       "1  [1463, 21098, 16110, 16549, 329, 12926, 10988,...   11314    72       2867   \n",
       "2  [1463, 21098, 16110, 16549, 329, 12926, 10988,...   11314    72       2867   \n",
       "3  [1463, 21098, 16110, 16549, 329, 12926, 10988,...   11314    72       2867   \n",
       "4  [1463, 21098, 16110, 16549, 329, 12926, 10988,...   11314    72       2867   \n",
       "\n",
       "                                               Blurb  \n",
       "0  [100498, 62786, 1003, 86311, 97903, 118024, 12...  \n",
       "1  [56570, 52866, 17228, 67961, 44141, 1003, 3674...  \n",
       "2  [56570, 52866, 17228, 67961, 44141, 1003, 3674...  \n",
       "3  [56570, 52866, 17228, 67961, 44141, 1003, 3674...  \n",
       "4  [56570, 52866, 17228, 67961, 44141, 1003, 3674...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ORIGIN_DATA_DIR = os.getcwd()+'/all_fearures/BX-CSV-Dump/'\n",
    "FILTERED_DATA_DIR = os.getcwd()+'/tmp/'\n",
    "class DataLoad:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        books_with_blurbs.csv cloumns: ISBN,text,Author,Year,Publisher,Blurb\n",
    "        BX-Book-Ratings.csv cloumns: User-ID,ISBN,Book-Rating\n",
    "        BX-Books.csv cloumns: ISBN,Book-text,Book-Author,Year-Of-Publication,Publisher,Image-URL-S,Image-URL-M,Image-URL-L\n",
    "        BX-Users.csv cloumns: User-ID,Location,Age\n",
    "        '''\n",
    "        self.BX_Users = self.load_origin('BX-Users')\n",
    "        self.BX_Book_Ratings = self.load_origin('BX-Book-Ratings')\n",
    "        self.Books = self.load_origin('books_with_blurbs', ',')\n",
    "        #合并三个表\n",
    "        self.features, self.ISBN2int, self.UserID2int, self.Users, self.blurb2int  = self.get_features()\n",
    "        self.labels = self.features.pop('Book-Rating')\n",
    "\n",
    "    def load_origin(self, \n",
    "        filename: \"根据文件名获取源文件，获取正确得columns、values等值\", \n",
    "        sep: \"因为源文件的分隔方式sep不同，所以通过传参改编分隔方式\"=\"\\\";\\\"\", \n",
    "        )->pd.DataFrame:\n",
    "        '''\n",
    "        获取原始数据，第一遍获取后将用pickle保存到本地，方便日后调用\n",
    "        '''\n",
    "        try:\n",
    "            # 从缓存的文件夹FILTERED_DATA_DIR获取基本被过滤后的文件\n",
    "            pickled_data = pickle.load(open(FILTERED_DATA_DIR+filename+'.p', mode='rb'))\n",
    "            return pickled_data\n",
    "        except FileNotFoundError:\n",
    "            # 如果缓存的文件不存在或者没有，则在源目录ORIGIN_DATA_DIR获取\n",
    "            all_fearures = pd.read_csv(ORIGIN_DATA_DIR+filename+'.csv', engine='python',sep=sep, encoding='utf-8')\n",
    "            # \\\";\\\"  初始过滤的文件\n",
    "            # ,      初始不需要过滤的文件\n",
    "            data_dict = {\"\\\";\\\"\":self.filtrator(all_fearures), ',':all_fearures}\n",
    "            # 因为没获得处理后的文件，所以我们在获取源文件后可以保存一下处理后的文件\n",
    "            pickle.dump((data_dict[sep]), open(FILTERED_DATA_DIR+filename+'.p', 'wb'))\n",
    "            return data_dict[sep]\n",
    "        except UnicodeDecodeError as e:\n",
    "            ''' 测试时经常会出现编码错误，如果尝试更换编码方式无效，可以将编码错误的部分位置重新复制粘贴就可以了，这里我们都默认UTF-8'''\n",
    "            print('UnicodeDecodeError:',e)\n",
    "        except pd.errors.ParserError as e:\n",
    "            print(\"connect error|pandas Error: %s\" % e)\n",
    "\n",
    "    def filtrator(self, \n",
    "        f_data: \"输入需要进行初步filter的数据\"\n",
    "        )->pd.DataFrame:\n",
    "        '''\n",
    "        源文件中的columns和各个值得第一列的第一个字符和最后一列的最后一个字符都带有双引号‘\"’,需要将其filter,Location字段当用户Age为null的时候，末尾会有\\\";NULL字符串 ，直接用切片调整\n",
    "        '''\n",
    "        Nonetype_age = 0\n",
    "        f_data = f_data.rename(columns={f_data.columns[0]:f_data.columns[0][1:], f_data.columns[-1]:f_data.columns[-1][:-1]})\n",
    "        f_data[f_data.columns[0]] = f_data[f_data.columns[0]].map(lambda v:v[1:] if v!=None else Nonetype_age)\n",
    "        f_data[f_data.columns[-1]] = f_data[f_data.columns[-1]].map(lambda v:v[:-1] if v!=None else Nonetype_age)\n",
    "        try:\n",
    "            f_data = f_data[f_data['Location'].notnull()][f_data[f_data['Location'].notnull()]['Location'].str.contains('\\\";NULL')]\n",
    "            f_data['Location'] = f_data['Location'].map(lambda location:location[:-6])\n",
    "        except:\n",
    "            pass\n",
    "        return f_data\n",
    "\n",
    "    def get_features(self):\n",
    "        '''\n",
    "        获取整个数据集的所有features，并对每个文本字段作xxxxx\n",
    "        User-ID、Location、ISBN、Book-Rating、Title、Author、Year、Publisher、Blurb\n",
    "        '''\n",
    "        try:\n",
    "            # 从缓存的文件夹FILTERED_DATA_DIR获取features的文件\n",
    "            all_fearures, ISBN2int, UserID2int, Users, blurb2int = pickle.load(open(FILTERED_DATA_DIR+'features.p', mode='rb'))\n",
    "            return all_fearures, ISBN2int, UserID2int, Users\n",
    "        except:\n",
    "            # 将所有的数据组成features大表\n",
    "            all_fearures = pd.merge(pd.merge(self.BX_Users, self.BX_Book_Ratings), self.Books)\n",
    "            Users = all_fearures\n",
    "            # 因为没获得处理后的文件，所以我们在获取源文件后可以保存一下处理后的文件\n",
    "            # isbn2index userid2index\n",
    "            all_fearures.pop('Age')\n",
    "            all_fearures['Title'], title2int = self.feature2int(all_fearures['Title'], 'text')\n",
    "            all_fearures['Blurb'], blurb2int  = self.feature2int(all_fearures['Blurb'], 'text')\n",
    "            all_fearures['ISBN'], ISBN2int = self.feature2int(all_fearures['ISBN'], 'word')\n",
    "            all_fearures['Author'], X2int = self.feature2int(all_fearures['Author'], 'word')\n",
    "            all_fearures['Publisher'], X2int = self.feature2int(all_fearures['Publisher'], 'word')\n",
    "            all_fearures['Year'], X2int = self.feature2int(all_fearures['Year'], 'word')\n",
    "            all_fearures['User-ID'], UserID2int  = self.feature2int(all_fearures['User-ID'], 'word')\n",
    "            all_fearures['Location'] = self.feature2int(all_fearures['Location'], 'list')\n",
    "            all_fearures['Book-Rating'] = all_fearures['Book-Rating'].astype('float32')\n",
    "            pickle.dump((all_fearures, ISBN2int, UserID2int, Users), open(FILTERED_DATA_DIR+'features.p', 'wb'))\n",
    "            return all_fearures, ISBN2int, UserID2int, Users, blurb2int\n",
    "\n",
    "    def feature2int(self, \n",
    "        feature:'特征值',\n",
    "        feature_type:'text/word/list'):\n",
    "        '''\n",
    "        将文本字段比如title、blurb只取英文单词，并用空格为分隔符，做成一个带index值的集合，并用index值表示各个单词，作为文本得表示\n",
    "        '''\n",
    "        pattern = re.compile(r'[^a-zA-Z]')\n",
    "        filtered_map = {val:re.sub(pattern, ' ', str(val)) for ii,val in enumerate(set(feature)) }\n",
    "        letter_filter = lambda feature:feature.map({val:re.sub(pattern, ' ', str(val)) for ii,val in enumerate(set(feature)) })\n",
    "        text_words = set()\n",
    "        filtered_feature = letter_filter(feature)\n",
    "        for val in filtered_feature.str.split():\n",
    "            text_words.update(val)\n",
    "        text2int = {val:ii for ii, val in enumerate(text_words)}\n",
    "        text_map = {val:[text2int[row] for row in filtered_map[val].split()][:200] for ii,val in enumerate(set(feature))}\n",
    "              \n",
    "        word_map = {val:ii for ii,val in enumerate(set(feature))}\n",
    "        try:\n",
    "            cities = set()\n",
    "            for val in feature.str.split(','):\n",
    "                cities.update(val)\n",
    "            city2int = {val:ii for ii, val in enumerate(cities)}\n",
    "            list_map = {val:[city2int[row] for row in val.split(',')][:3] for ii,val in enumerate(set(feature))}\n",
    "        except AttributeError :\n",
    "            list_map = {}\n",
    "\n",
    "        feature_dict = {\n",
    "            'text':(feature.map(text_map), text_map),\n",
    "            'word':(feature.map(word_map), word_map),\n",
    "            'list':(feature.map(list_map)),\n",
    "            }\n",
    "        return feature_dict[feature_type]\n",
    "\n",
    "    def __del__(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "origin_DATA = DataLoad()\n",
    "origin_DATA.features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepoces_blurb():\n",
    "    blurb_word2int = origin_DATA.blurb2int\n",
    "    blurb_int2word = {tuple(v): i for i,v in blurb_word2int.items()}\n",
    "    # 在这里我拿到了所有的句子\n",
    "    blurb_word_list = [blurb_int2word[tuple(val)] for val in origin_DATA.features.Blurb]\n",
    "    # 过滤停用词\n",
    "    stop_words = stopwords.words('english')\n",
    "    for sw in [',', ', ', ' ,', '«', '»', 'même', 'à', 'orée', '…', 'l', '', ' ', '``', '#']:\n",
    "        stop_words.append(sw)\n",
    "    new_sentences = []\n",
    "    for sentences in blurb_word_list:\n",
    "        sen_list = nltk.word_tokenize(sentences)\n",
    "        new_wrod_list = []\n",
    "        for word_ in sen_list:\n",
    "            if word_ not in stop_words:\n",
    "                new_wrod_list.append(word_)\n",
    "        new_sentences.append(new_wrod_list)\n",
    "    with open(\"./blurb.txt\", \"w\", encoding='utf-8') as f:\n",
    "        for sen in new_sentences:\n",
    "            for word_ in sen:\n",
    "                f.write(word_)\n",
    "                f.write(' ')\n",
    "            f.write('\\n')\n",
    "if not os.path.exists('./blurb.txt'):\n",
    "    prepoces_blurb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(172102, 1)\n",
      "                                               Blurb\n",
      "0  Here first time paperback outstanding military...\n",
      "1  The fascinating true story world 's deadliest ...\n",
      "2  The fascinating true story world 's deadliest ...\n",
      "3  The fascinating true story world 's deadliest ...\n",
      "4  The fascinating true story world 's deadliest ...\n",
      "5  Winnie Helen kept others worst secrets fifty y...\n",
      "6  Winnie Helen kept others worst secrets fifty y...\n",
      "7  Winnie Helen kept others worst secrets fifty y...\n",
      "8  Winnie Helen kept others worst secrets fifty y...\n",
      "9  Winnie Helen kept others worst secrets fifty y...\n"
     ]
    }
   ],
   "source": [
    "blurb_new = pd.read_csv('./blurb.txt', names=['Blurb'], sep='\\t')\n",
    "print(blurb_new.shape)\n",
    "print(blurb_new.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "blurb_word_list = []\n",
    "blurb_set = set()\n",
    "for sen in blurb_new.values:\n",
    "    sen_list = nltk.word_tokenize(sen[0])\n",
    "    new_word_list_ = []\n",
    "    # 对于某些异常的句子，应该将其处理掉\n",
    "    if len(sen_list) > 600:\n",
    "        sen_list = sen_list[:30]\n",
    "    for word_ in sen_list:\n",
    "        new_word_list_.append(word_)\n",
    "        blurb_set.add(word_)\n",
    "    blurb_word_list.append(new_word_list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.63560562921988\n"
     ]
    }
   ],
   "source": [
    "length_blurb = [len(i) for i in blurb_word_list]\n",
    "print(np.mean(np.array(length_blurb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172102\n"
     ]
    }
   ],
   "source": [
    "# 建立字典\n",
    "blurb_dict = {v:i for i, v in enumerate(blurb_set)}\n",
    "print(len(blurb_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159655\n",
      "159655\n",
      "[('serotonin', 0), ('T.C', 1), ('rummages', 2), ('Loved', 3), ('BRANDED', 4), ('Corbell', 5), ('masalas', 6), ('enemy-the', 7), ('THUD', 8), ('Columbian', 9), ('Adept', 10), ('heroin-addicted', 11), ('Belfry', 12), ('tercera', 13), ('Brandewyne', 14), ('poachers', 15), ('interessieren', 16), ('McIntyre', 17), ('scandals', 18), ('1590s', 19), ('mid-70s', 20), ('six-shooter', 21), ('double-deals', 22), ('job-hopping', 23), ('Bit', 24), ('hard.,2', 25), ('mastery', 26), ('entstellt', 27), ('McCammon', 28), ('teaser', 29), ('shrewishness', 30), ('Guilty—Until', 31), ('Memo', 32), ('Gates-Margolis', 33), ('Brethren', 34), ('Rapscallions', 35), ('damage', 36), ('espera', 37), ('high.Most', 38), ('uninfected', 39), ('sguardi', 40), ('forests', 41), ('Carnivores', 42), ('ElimiDATE', 43), ('marshmallow', 44), ('Nerone', 45), ('Zadie', 46), ('situada', 47), ('fertility', 48), ('Roberto', 49), ('murmurings—pulsing', 50), ('read-haired', 51), ('contamination', 52), ('Phar', 53), ('Reeds', 54), ('sunblasted', 55), ('half-truthful', 56), ('websites', 57), (\"d'appréhender\", 58), ('muto', 59), ('cobblers', 60), ('equilibrium', 61), ('Faites-nous', 62), ('Rainbows', 63), ('love-still', 64), ('tainting', 65), ('fall-out', 66), ('palates', 67), ('disparition', 68), ('Tomatoes', 69), ('Lozada-and', 70), ('Initially', 71), ('convictions', 72), ('Hokkaido', 73), ('Silvercloak', 74), ('wreath-making', 75), ('Eudes', 76), ('Ode', 77), ('infinitos', 78), ('Lupica', 79), ('parlay', 80), ('hunter-gatherers', 81), ('horror-stricken', 82), ('elicited', 83), ('believes-than', 84), ('head-to-toe', 85), ('highlighting/notes', 86), ('Außer', 87), ('mulberry', 88), ('centaines', 89), ('ENCHANTING', 90), ('Llys', 91), ('37', 92), ('heath', 93), ('deem', 94), ('spezialisiert', 95), ('*********', 96), ('Injured', 97), ('AWARD', 98), ('fair-mindedness', 99)]\n"
     ]
    }
   ],
   "source": [
    "w = []\n",
    "for i in blurb_word_list:\n",
    "    for word in i:\n",
    "        w.append(word)\n",
    "print(len(set(w)))\n",
    "print(len(blurb_dict))\n",
    "print(list(blurb_dict.items())[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainWord2Vec():#训练word2vec模型并存储\n",
    "    train_sentences = blurb_word_list\n",
    "    # 这里min_count会对字典做截断，如果min_coun=5,那么出现次数小于5的词会被丢弃，需要更新原本的字典\n",
    "    model=Word2Vec(sentences=train_sentences,size=100,min_count=0,window=5)\n",
    "    model.save('./blurb_word2vec.model')\n",
    "trainWord2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.9754843e-01  2.8008566e+00 -2.8137808e+00 -6.1239737e-01\n",
      "  3.9987683e+00 -1.1176540e+00  6.1179692e-01  4.6012250e-01\n",
      "  3.8250690e+00 -5.7861561e-01 -8.1807993e-02 -1.6565148e+00\n",
      " -1.1872522e+00 -1.2961287e+00 -3.4909885e+00  3.9388621e+00\n",
      "  8.2338262e-01 -5.0523543e-01 -1.8289630e+00  1.2028067e+00\n",
      " -2.5206101e-01  1.7424383e+00 -1.7303660e+00 -4.7074932e-01\n",
      "  1.2209388e+00  1.7551062e+00 -7.8166610e-01  3.3624601e-02\n",
      "  1.7001582e+00  1.2212100e+00 -3.5618749e-01  8.4452039e-01\n",
      "  1.6091290e+00  4.4228479e-01 -1.5335855e+00  5.4612851e-01\n",
      " -2.3034046e+00  3.8279355e+00  5.9133625e-01 -1.7215645e+00\n",
      " -1.2617992e+00 -3.0895033e+00 -1.6020856e+00 -1.2805055e+00\n",
      " -7.2305924e-01 -6.5627724e-01 -2.4727347e+00 -9.4625473e-01\n",
      " -1.3531775e+00  6.0299454e+00  1.3728358e+00  2.9687726e+00\n",
      " -5.1364779e+00  1.6492635e+00  1.9755188e+00  1.0195773e+00\n",
      " -6.9438386e-01 -1.7585691e+00  2.0761497e+00 -4.0419024e-01\n",
      "  1.7259640e+00  7.5109559e-01 -3.3816978e-01 -4.8804202e+00\n",
      "  3.7444451e-01 -2.4315641e+00  1.2264004e+00  2.1274900e+00\n",
      " -9.0729117e-01 -8.5645252e-01  2.5463662e+00 -3.2486260e+00\n",
      "  2.0317073e+00 -1.2055486e+00 -8.5953784e-01  3.1925366e+00\n",
      "  4.6952552e-01 -6.7687836e+00 -8.3947629e-01  1.8709424e-03\n",
      " -4.0751505e+00 -9.7145265e-01 -6.7212290e-01 -2.5338542e+00\n",
      "  2.2847047e+00  1.0824031e+00  1.0658301e+00  1.0688573e+00\n",
      " -9.6360052e-01  2.7473075e+00 -6.7850113e-01 -1.0605632e+00\n",
      " -2.3219852e+00  3.5953867e+00  1.1503769e-01 -3.1980065e-01\n",
      " -4.4974655e-01 -1.8959407e-02 -2.5009928e+00  3.2794011e+00]\n"
     ]
    }
   ],
   "source": [
    "model=Word2Vec.load('./blurb_word2vec.model')\n",
    "print(model.wv['computer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159655, 100)\n",
      "(159656, 100)\n"
     ]
    }
   ],
   "source": [
    "blurb_word2vec_martics = np.zeros((len(blurb_dict), 100))\n",
    "for i, value in blurb_dict.items():\n",
    "    try:\n",
    "        blurb_word2vec_martics[value, :] = model.wv[i]\n",
    "    except:\n",
    "        print(i ,value)\n",
    "        print(model.wv[i])\n",
    "        exit(1)\n",
    "print(blurb_word2vec_martics.shape)\n",
    "# 添加pad字符对于的向量\n",
    "blurb_word2vec_martics = np.insert(blurb_word2vec_martics, blurb_word2vec_martics.shape[0], axis=0, values=np.zeros(shape=(1, 100)))\n",
    "print(blurb_word2vec_martics.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_index =  159655\n",
      "(172102, 200)\n"
     ]
    }
   ],
   "source": [
    "# 将blurb_word_list变为数字列表\n",
    "# 增加一个pad字符\n",
    "blurb_dict['<padd>'] = len(blurb_dict)\n",
    "pad_index = blurb_dict['<padd>']\n",
    "print('pad_index = ', pad_index)\n",
    "blurb_int_list = []\n",
    "for one_blurb in blurb_word_list:\n",
    "    # 补齐和去长\n",
    "    one_blurb = one_blurb[:200]\n",
    "    if len(one_blurb)< 200:\n",
    "        one_blurb = one_blurb + ['<padd>'] * (200- len(one_blurb))\n",
    "    \n",
    "    # 转化为数字列表\n",
    "    temp_list = []\n",
    "    for word in one_blurb:\n",
    "        temp_list.append(blurb_dict[word])\n",
    "    blurb_int_list.append(temp_list)\n",
    "\n",
    "blurb = np.array(blurb_int_list)\n",
    "print(blurb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all user id =  28836\n",
      "all location =  7573\n"
     ]
    }
   ],
   "source": [
    "# user-id的字典,总共有28836个用户\n",
    "all_user = len(set(origin_DATA.features['User-ID']))\n",
    "new_user_id = {val: i for i, val in enumerate(set(origin_DATA.features['User-ID']))}\n",
    "print('all user id = ', all_user)\n",
    "# location的数量=7573(从0开始的)\n",
    "all_location = max([j for i in origin_DATA.features.Location for j in i]) +1 \n",
    "print('all location = ', all_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all isbn =  38036\n",
      "all author =  15196\n",
      "all year =  81\n",
      "all publisher =  2909\n",
      "all title words =  23815\n",
      "all blurb words =  159656\n"
     ]
    }
   ],
   "source": [
    "# ISBN总数\n",
    "all_isbn = len(set(origin_DATA.features['ISBN']))\n",
    "print('all isbn = ', all_isbn)\n",
    "# author总数\n",
    "all_author = len(set(origin_DATA.features['Author']))\n",
    "print('all author = ', all_author)\n",
    "# year总数\n",
    "all_year = len(set(origin_DATA.features['Year']))\n",
    "print('all year = ', all_year)\n",
    "# publish总数\n",
    "all_publisher = len(set(origin_DATA.features['Publisher']))\n",
    "print('all publisher = ', all_publisher)\n",
    "# title中所有单词总数\n",
    "all_title_words = max([j for i in origin_DATA.features.Title for j in i]) +1\n",
    "print('all title words = ', all_title_words)\n",
    "# blurb中所有单词总数\n",
    "all_blurb_words = len(blurb_dict)\n",
    "print('all blurb words = ', all_blurb_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(172102, 10)\n",
      "[[ 2732 20340 20672 23815 23815 23815 23815 23815 23815 23815]\n",
      " [ 1463 21098 16110 16549   329 12926 10988   623 16549  4583]\n",
      " [ 1463 21098 16110 16549   329 12926 10988   623 16549  4583]\n",
      " [ 1463 21098 16110 16549   329 12926 10988   623 16549  4583]\n",
      " [ 1463 21098 16110 16549   329 12926 10988   623 16549  4583]\n",
      " [21098  7472 10948 23415  9289 23815 23815 23815 23815 23815]\n",
      " [21098  7472 10948 23415  9289 23815 23815 23815 23815 23815]\n",
      " [21098  7472 10948 23415  9289 23815 23815 23815 23815 23815]\n",
      " [21098  7472 10948 23415  9289 23815 23815 23815 23815 23815]\n",
      " [21098  7472 10948 23415  9289 23815 23815 23815 23815 23815]]\n"
     ]
    }
   ],
   "source": [
    "m = len(origin_DATA.features['Location'])\n",
    "# 对title进行补齐\n",
    "\n",
    "title = []\n",
    "for ti in origin_DATA.features['Title'].values:\n",
    "    if len(ti) > 10:\n",
    "        ti = ti[:10]\n",
    "    if len(ti)<10:\n",
    "        ti = ti + [all_title_words] * (10 - len(ti))\n",
    "\n",
    "    title.append(ti)\n",
    "all_title_words += 1\n",
    "title = np.array(title)\n",
    "print(title.shape)\n",
    "print(title[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_train_val_test():  \n",
    "    # 对location取3位数\n",
    "    loca = np.zeros((m, 3))\n",
    "    for i in range(m):\n",
    "        loca[i] = np.array(origin_DATA.features['Location'][i])\n",
    "    print(loca[:-2])\n",
    "\n",
    "    input_features = [origin_DATA.features['User-ID'].to_numpy(), loca, \n",
    "                      origin_DATA.features['ISBN'].to_numpy(), origin_DATA.features['Author'].to_numpy(),\n",
    "                     origin_DATA.features['Year'].to_numpy(), origin_DATA.features['Publisher'].to_numpy(), \n",
    "                     title, blurb]\n",
    "    labels = origin_DATA.labels.to_numpy()\n",
    "    # 分割数据集以及shuffle\n",
    "    np.random.seed(100)\n",
    "    number_features = len(input_features)\n",
    "    shuffle_index = np.random.permutation(m)\n",
    "    shuffle_train_index = shuffle_index[:math.ceil(m*0.96)]\n",
    "    shuffle_val_index = shuffle_index[math.ceil(m*0.96): math.ceil(m*0.98)]\n",
    "    shuffle_test_index = shuffle_index[math.ceil(m*0.98):]\n",
    "    train_features = [input_features[i][shuffle_train_index] for i in range(number_features)]\n",
    "    train_labels = labels[shuffle_train_index]\n",
    "    val_features = [input_features[i][shuffle_val_index] for i in range(number_features)]\n",
    "    val_lables = labels[shuffle_val_index]\n",
    "    test_features = [input_features[i][shuffle_test_index] for i in range(number_features)]\n",
    "    test_lables = labels[shuffle_test_index]\n",
    "    return train_features, train_labels, val_features, val_lables, test_features, test_lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6235.  579. 6140.]\n",
      " [6235.  579. 6140.]\n",
      " [ 819. 1898. 6140.]\n",
      " ...\n",
      " [4974.  973. 3542.]\n",
      " [6524.  705. 3542.]\n",
      " [6524.  705. 3542.]]\n",
      "(165218,)\n",
      "(3442,)\n",
      "(3442,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_features, train_labels, val_features, val_lables, test_features, test_lables = get_train_val_test()\n",
    "print(train_features[0].shape)\n",
    "print(val_features[0].shape)\n",
    "print(test_features[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    # 用户特征输入\n",
    "    user_id = keras.layers.Input(shape=(1,), dtype='int32', name='user_id_input')\n",
    "    user_location = keras.layers.Input(shape=(3,), dtype='int32', name='user_location_input')\n",
    "    \n",
    "    # 书籍特征输入\n",
    "    book_isbn = keras.layers.Input(shape=(1,),  dtype='int32', name='book_isbn_input')\n",
    "    book_author = keras.layers.Input(shape=(1,),  dtype='int32', name='book_author_input')\n",
    "    book_year = keras.layers.Input(shape=(1,),  dtype='int32', name='book_year_input')\n",
    "    book_publisher = keras.layers.Input(shape=(1,),  dtype='int32', name='book_publisher_input')  \n",
    "    book_title = keras.layers.Input(shape=(10, ), dtype='int32', name='book_title_input')\n",
    "    book_blurb = keras.layers.Input(shape=(200, ), dtype='int32', name='book_blurb_input')\n",
    "    return user_id, user_location, book_isbn, book_author, book_year, book_publisher, book_title, book_blurb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 嵌入矩阵的维度\n",
    "embed_dim = 4\n",
    "embed_dim_title = 16\n",
    "# 这里的100跟上文的预训练模型size应一致\n",
    "embed_dim_blurb = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_embed_layer(u_id, u_loca):\n",
    "    user_id_embedd = keras.layers.Embedding(all_user, embed_dim, name='user_id_embedding')(u_id)\n",
    "    user_loca_embedd = keras.layers.Embedding(all_location, embed_dim , name='user_loca_embedding')(u_loca)\n",
    "    return user_id_embedd, user_loca_embedd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_emded_layer(b_isbn, b_atuhor, b_year, b_publisher, b_title, b_blurb):\n",
    "    book_isbn_embedd = keras.layers.Embedding(all_isbn, embed_dim, name='book_isbn_embedding')(b_isbn)\n",
    "    book_author_embedd = keras.layers.Embedding(all_author, embed_dim, name='book_author_embedding')(b_atuhor)\n",
    "    book_year_embedd = keras.layers.Embedding(all_year, embed_dim, name='book_year_embedding')(b_year)\n",
    "    book_publisher_embedd = keras.layers.Embedding(all_publisher, embed_dim, name='book_publisher_embedding')(b_publisher)\n",
    "    \n",
    "    book_title_embedd = keras.layers.Embedding(all_title_words, embed_dim_title, name='book_title_embedding')(b_title)\n",
    "    # 加载预训练模型\n",
    "    book_blurb_embedd = keras.layers.Embedding(all_blurb_words, embed_dim_blurb, name='book_blurb_embedding', weights=[blurb_word2vec_martics], trainable=True)(b_blurb)\n",
    "    return book_isbn_embedd, book_author_embedd, book_year_embedd, book_publisher_embedd, book_title_embedd, book_blurb_embedd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_feature(u_id_embedd, u_loca_embedd):\n",
    "    u_id_layer = keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.nn.l2_loss, name='u_id_dense')(u_id_embedd)\n",
    "#     u_id_layer_drop = keras.layers.Dropout(rate=0.5, name='u_id_layer_drop')(u_id_layer)\n",
    "    # u_id_layer.shape = (?, 1, 32)\n",
    "    # u_loca_layer.shape = (?, 32)\n",
    "    # 这里可以再加个Dense\n",
    "    u_loca_layer = keras.layers.Bidirectional(tf.keras.layers.LSTM(16, name='u_loca_lstm'), merge_mode='concat')(u_loca_embedd)\n",
    "    u_loca_lstm_dense = keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.nn.l2_loss, name='u_loca_lstm_dense')(u_loca_layer)\n",
    "    u_id_reshape = keras.layers.Reshape([32])(u_id_layer)\n",
    "    u_combine = keras.layers.concatenate([u_id_reshape, u_loca_lstm_dense],axis=1, name='u_combine')\n",
    "    print(u_combine.shape)\n",
    "    # 这里能不能用激活函数\n",
    "    u_feature_layer = keras.layers.Dense(100, activation='tanh', name='u_feature_layer')(u_combine)\n",
    "    print(u_feature_layer.shape)\n",
    "    return u_feature_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_dense = 4\n",
    "def get_book_feature(b_isbn_embedd, b_author_embedd, b_year_embedd, b_publisher_embedd, b_title_embedd, b_blurb_embedd):\n",
    "    # 首先对前4个特征连接Dense层\n",
    "    b_isbn_dense = keras.layers.Dense(b_dense, activation='relu', kernel_regularizer=tf.nn.l2_loss, name='b_isbn_dense')(b_isbn_embedd)\n",
    "    b_author_dense = keras.layers.Dense(b_dense, activation='relu', kernel_regularizer=tf.nn.l2_loss, name='b_author_dense')(b_author_embedd)\n",
    "    b_year_dense = keras.layers.Dense(b_dense, activation='relu', kernel_regularizer=tf.nn.l2_loss, name='b_year_dense')(b_year_embedd)\n",
    "    b_publisher_dense = keras.layers.Dense(b_dense, activation='relu', kernel_regularizer=tf.nn.l2_loss, name='b_publisher_dense')(b_publisher_embedd)\n",
    "    # 合并这四个特征,  b_combine_four shape = (?, 1, 16)\n",
    "    b_combine_four = keras.layers.concatenate([b_isbn_dense, b_author_dense, b_year_dense, b_publisher_dense], name='b_four_combine')\n",
    "    print('b_combine_four.shape', b_combine_four.shape)\n",
    "    # 对title进行卷积\n",
    "    b_title_reshape = keras.layers.Lambda(lambda layer: tf.expand_dims(layer, 3))(b_title_embedd)  # shape=(?,10, 16, 1)\n",
    "    print('b_title_reshape.shape = ', b_title_reshape.shape)\n",
    "    # b_title_conv.shape = \n",
    "    b_title_conv = keras.layers.Conv2D(filters=8, kernel_size=(2, embed_dim_title), kernel_regularizer=tf.nn.l2_loss, strides=1)(b_title_reshape)# shape=(?, 14, 1, 8)\n",
    "    # b-title_pool.shape =\n",
    "    b_title_pool = keras.layers.MaxPool2D(pool_size=(9, 1), strides=1)(b_title_conv) # shape=(?,1, 1, 8)\n",
    "    print('b_title_conv.shape = ', b_title_conv)\n",
    "    print('b_title_pool.shape = ', b_title_pool)\n",
    "    \n",
    "    # 对blurb进行处理\n",
    "    # shape = \n",
    "    b_blurb_lstm_1 = keras.layers.Bidirectional(tf.keras.layers.LSTM(16, name='b_blurb_lstm', dropout=0.5, return_sequences=True), merge_mode='concat')(b_blurb_embedd) \n",
    "    print('b_blurb_lstm_1.shape = ', b_blurb_lstm_1.shape)\n",
    "    b_blurb_lstm_2 = keras.layers.LSTM(32, name='b_blurb_lstm', dropout=0.5, return_sequences=False)(b_blurb_lstm_1) \n",
    "    print('b_blurb_lstm_2.shape = ', b_blurb_lstm_2.shape)\n",
    "    # 将title和blurb合并\n",
    "    b_title_reshape = keras.layers.Reshape([b_title_pool.shape[3]])(b_title_pool)\n",
    "    # b_combine_blurb_title.shape = \n",
    "    b_combine_blurb_title = keras.layers.concatenate([b_title_reshape, b_blurb_lstm_2], axis=1, name='b_combine_blurb_title')\n",
    "    print('b_combine_blurb_title.shape = ', b_combine_blurb_title)\n",
    "    b_blurb_title_dense = keras.layers.Dense(64, kernel_regularizer=tf.nn.l2_loss, activation='relu', name='b_blurb_title_dense')(b_combine_blurb_title)\n",
    "#     b_blurb_title_dense_drop = keras.layers.Dropout(rate=0.5, name='b_blurb_title_dense_drop')(b_blurb_title_dense)\n",
    "    b_blurb_title_dense = keras.layers.Dense(64, activation='relu', name='b_blurb_title_dense')(b_combine_blurb_title)\n",
    "    # b_combine_four_reshape shape = (?, 64)\n",
    "    b_combine_four_reshape = keras.layers.Reshape([b_combine_four.shape[2]], name='b_combine_four_reshape')(b_combine_four)\n",
    "    # 合并所有的书籍特征\n",
    "#     b_combine_book = keras.layers.concatenate([b_combine_blurb_title, b_combine_four_reshape], axis=1, name='b_combine_book')\n",
    "    b_combine_book = keras.layers.concatenate([b_blurb_title_dense, b_combine_four_reshape], axis=1, name='b_combine_book')\n",
    "\n",
    "    # 得到书籍矩阵\n",
    "    b_feature_layer = keras.layers.Dense(100, name='b_feature_layer', activation='tanh')(b_combine_book)\n",
    "    return b_feature_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating(user_feature, book_feature):\n",
    "#     multiply_layer = keras.layers.Lambda(lambda layer: tf.reduce_sum(layer[0]+layer[1], axis=1, keepdims=True), name = 'user_book_feature')((user_feature, book_feature))\n",
    "    inference_layer = keras.layers.concatenate([user_feature, book_feature], axis=1, name='user_book_feature')\n",
    "    inference_dense = tf.keras.layers.Dense(64, kernel_regularizer=tf.nn.l2_loss, activation='relu')(inference_layer)\n",
    "    multiply_layer = tf.keras.layers.Dense(1, name=\"inference\")(inference_layer)  # inference_dense\n",
    "    print(multiply_layer.shape)\n",
    "    return multiply_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './model/'\n",
    "\n",
    "class model_network():\n",
    "    def __init__(self):\n",
    "        self.batchsize = 512\n",
    "        self.epoch = 20\n",
    "    def creat_model(self):\n",
    "        user_id, user_location, book_isbn, book_author, book_year, book_publisher, book_title, book_blurb = get_inputs()\n",
    "        user_id_embedd, user_loca_embedd = user_embed_layer(user_id, user_location)\n",
    "        book_isbn_embedd, book_author_embedd, book_year_embedd, book_publisher_embedd, book_title_embedd, book_blurb_embedd = book_emded_layer(book_isbn, book_author, book_year, book_publisher, book_title, book_blurb)\n",
    "        u_feature_layer = get_user_feature(user_id_embedd, user_loca_embedd)\n",
    "        b_feature_layer = get_book_feature(book_isbn_embedd, book_author_embedd, book_year_embedd, book_publisher_embedd, book_title_embedd, book_blurb_embedd)\n",
    "        multiply_layer = get_rating(u_feature_layer, b_feature_layer)\n",
    "        model = keras.Model(inputs=[user_id, user_location, book_isbn, book_author, book_year, book_publisher, book_title, book_blurb],\n",
    "                    outputs=[multiply_layer])\n",
    "        return model\n",
    "    def train_model(self):\n",
    "        weights_path = './model_weights/model_2.hdf5'\n",
    "#         checkpoint = keras.callbacks.ModelCheckpoint(filepath=weights_path, monitor='val_loss', mode='min', save_weights_only=True)\n",
    "        model_optimizer = keras.optimizers.Adamax()\n",
    "        model = self.creat_model()\n",
    "        model.compile(optimizer=model_optimizer, loss=keras.losses.mse)\n",
    "        history = model.fit(train_features, train_labels, validation_data=(val_features, val_lables), epochs=self.epoch, batch_size=self.batchsize, verbose=1)\n",
    "        model.summary()\n",
    "        return model, history\n",
    "    def predict_model(self, model):\n",
    "        test_loss = model.evaluate(test_features, test_lables, batch_size = 512, verbose=1)\n",
    "        return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 64)\n",
      "(None, 100)\n",
      "b_combine_four.shape (None, 1, 16)\n",
      "b_title_reshape.shape =  (None, 10, 16, 1)\n",
      "b_title_conv.shape =  Tensor(\"conv2d/Identity:0\", shape=(None, 9, 1, 8), dtype=float32)\n",
      "b_title_pool.shape =  Tensor(\"max_pooling2d/Identity:0\", shape=(None, 1, 1, 8), dtype=float32)\n",
      "b_blurb_lstm_1.shape =  (None, 200, 32)\n",
      "b_blurb_lstm_2.shape =  (None, 32)\n",
      "b_combine_blurb_title.shape =  Tensor(\"b_combine_blurb_title/Identity:0\", shape=(None, 40), dtype=float32)\n",
      "(None, 1)\n",
      "Train on 165218 samples, validate on 3442 samples\n",
      "Epoch 1/20\n",
      "165218/165218 [==============================] - 62s 377us/sample - loss: 26.3266 - val_loss: 17.0480\n",
      "Epoch 2/20\n",
      "165218/165218 [==============================] - 52s 313us/sample - loss: 15.1689 - val_loss: 13.2845\n",
      "Epoch 3/20\n",
      "165218/165218 [==============================] - 52s 312us/sample - loss: 13.2525 - val_loss: 12.4625\n",
      "Epoch 4/20\n",
      "165218/165218 [==============================] - 52s 315us/sample - loss: 12.6880 - val_loss: 12.1982\n",
      "Epoch 5/20\n",
      "165218/165218 [==============================] - 52s 314us/sample - loss: 12.4112 - val_loss: 12.0957\n",
      "Epoch 6/20\n",
      "165218/165218 [==============================] - 52s 316us/sample - loss: 12.2045 - val_loss: 11.9542\n",
      "Epoch 7/20\n",
      "165218/165218 [==============================] - 52s 314us/sample - loss: 12.0240 - val_loss: 11.8784\n",
      "Epoch 8/20\n",
      "165218/165218 [==============================] - 51s 311us/sample - loss: 11.8545 - val_loss: 11.8272\n",
      "Epoch 9/20\n",
      "165218/165218 [==============================] - 51s 310us/sample - loss: 11.6861 - val_loss: 11.8143\n",
      "Epoch 10/20\n",
      "165218/165218 [==============================] - 51s 312us/sample - loss: 11.5291 - val_loss: 11.7483\n",
      "Epoch 11/20\n",
      "165218/165218 [==============================] - 52s 313us/sample - loss: 11.3839 - val_loss: 11.7743\n",
      "Epoch 12/20\n",
      "165218/165218 [==============================] - 51s 308us/sample - loss: 11.2479 - val_loss: 11.7431\n",
      "Epoch 13/20\n",
      " 76288/165218 [============>.................] - ETA: 26s - loss: 11.1033"
     ]
    }
   ],
   "source": [
    "net_work = model_network()\n",
    "model, history = net_work.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "plt.figure(1)\n",
    "plt.plot(train_loss, c='r', label='train_loss')\n",
    "plt.plot(val_loss, c='b', label='val_loss')\n",
    "plt.legend()\n",
    "plt.xlim([1, 50])\n",
    "plt.ylim([0, 20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画出模型图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, to_file='model_2.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
