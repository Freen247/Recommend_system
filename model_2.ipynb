{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved by word2vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改进思路：1\n",
    "## 加入停用词，blurb中的简介利用word2vec训练的向量替代，效果比原版提升了10%， 在15代的时候达到最好。loss=10.7, val_loss=11.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改进思路：2\n",
    "## 改变模型架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "sys.version_info(major=3, minor=6, micro=7, releaselevel='final', serial=0)\ntensorflow 2.1.0\nmatplotlib 3.2.0\nnumpy 1.18.1\npandas 1.0.1\nsklearn 0.22.2.post1\ntensorflow 2.1.0\ntensorflow_core.python.keras.api._v2.keras 2.2.4-tf\n"
    }
   ],
   "source": [
    "import os, sys, re, pickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "print(sys.version_info)\n",
    "for module in tf, mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)\n",
    "\n",
    "from Data import Pro_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   User-ID            Location   ISBN  \\\n0     2918  [1242, 7251, 5372]  32719   \n1     2918  [1242, 7251, 5372]  23709   \n2    24974  [4759, 1444, 5372]  23709   \n3    18233    [7245, 30, 1600]  23709   \n4      240  [4723, 6498, 1600]  23709   \n\n                                               Title  Author  Year  Publisher  \\\n0  [18416, 3305, 2794, 2794, 2794, 2794, 2794, 27...   13704    63       2346   \n1  [5042, 20096, 12301, 3565, 6702, 20408, 4956, ...    6881    71       2064   \n2  [5042, 20096, 12301, 3565, 6702, 20408, 4956, ...    6881    71       2064   \n3  [5042, 20096, 12301, 3565, 6702, 20408, 4956, ...    6881    71       2064   \n4  [5042, 20096, 12301, 3565, 6702, 20408, 4956, ...    6881    71       2064   \n\n                                               Blurb  \n0  [16743, 70936, 91023, 94647, 30725, 94201, 110...  \n1  [26888, 9440, 6976, 72713, 117446, 20931, 2329...  \n2  [26888, 9440, 6976, 72713, 117446, 20931, 2329...  \n3  [26888, 9440, 6976, 72713, 117446, 20931, 2329...  \n4  [26888, 9440, 6976, 72713, 117446, 20931, 2329...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>User-ID</th>\n      <th>Location</th>\n      <th>ISBN</th>\n      <th>Title</th>\n      <th>Author</th>\n      <th>Year</th>\n      <th>Publisher</th>\n      <th>Blurb</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2918</td>\n      <td>[1242, 7251, 5372]</td>\n      <td>32719</td>\n      <td>[18416, 3305, 2794, 2794, 2794, 2794, 2794, 27...</td>\n      <td>13704</td>\n      <td>63</td>\n      <td>2346</td>\n      <td>[16743, 70936, 91023, 94647, 30725, 94201, 110...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2918</td>\n      <td>[1242, 7251, 5372]</td>\n      <td>23709</td>\n      <td>[5042, 20096, 12301, 3565, 6702, 20408, 4956, ...</td>\n      <td>6881</td>\n      <td>71</td>\n      <td>2064</td>\n      <td>[26888, 9440, 6976, 72713, 117446, 20931, 2329...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>24974</td>\n      <td>[4759, 1444, 5372]</td>\n      <td>23709</td>\n      <td>[5042, 20096, 12301, 3565, 6702, 20408, 4956, ...</td>\n      <td>6881</td>\n      <td>71</td>\n      <td>2064</td>\n      <td>[26888, 9440, 6976, 72713, 117446, 20931, 2329...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>18233</td>\n      <td>[7245, 30, 1600]</td>\n      <td>23709</td>\n      <td>[5042, 20096, 12301, 3565, 6702, 20408, 4956, ...</td>\n      <td>6881</td>\n      <td>71</td>\n      <td>2064</td>\n      <td>[26888, 9440, 6976, 72713, 117446, 20931, 2329...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>240</td>\n      <td>[4723, 6498, 1600]</td>\n      <td>23709</td>\n      <td>[5042, 20096, 12301, 3565, 6702, 20408, 4956, ...</td>\n      <td>6881</td>\n      <td>71</td>\n      <td>2064</td>\n      <td>[26888, 9440, 6976, 72713, 117446, 20931, 2329...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "origin_DATA = Pro_data.DataLoad()\n",
    "origin_DATA.features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0         [Here, first, time, paperback, outstanding, mi...\n1         [The, fascinating, true, story, world, deadlie...\n2         [The, fascinating, true, story, world, deadlie...\n3         [The, fascinating, true, story, world, deadlie...\n4         [The, fascinating, true, story, world, deadlie...\n                                ...                        \n172097    [Tory, Bauer, quintessential, diner, waitress,...\n172098    [Faberville, bookstore, owner, Claire, Malloy,...\n172099    [Eric, Jerome, Dickey, back, sexy, fast, paced...\n172100    [Lieutenant, Horatio, Caine, leads, crack, tea...\n172101    [C, est, au, travers, de, leur, correspondance...\nName: Blurb, Length: 172102, dtype: object\n"
    }
   ],
   "source": [
    "print(origin_DATA.blurb2vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0\n"
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "\"word '' not in vocabulary\"",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-7c65c32e7359>\u001b[0m in \u001b[0;36mload_word2vec_martics\u001b[1;34m(size)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mblurb_word2vec_martics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\workspace\\rec_env\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, entities)\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[1;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\workspace\\rec_env\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\workspace\\rec_env\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word '' not in vocabulary\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-7c65c32e7359>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mblurb_word2vec_martics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mblurb_word2vec_martics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_word2vec_martics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-80-7c65c32e7359>\u001b[0m in \u001b[0;36mload_word2vec_martics\u001b[1;34m(size)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblurb_word2vec_martics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\workspace\\rec_env\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, entities)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[1;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\workspace\\rec_env\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\workspace\\rec_env\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word '' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "blurb_embedding_dim = 50\n",
    "def load_word2vec_martics(size=blurb_embedding_dim):\n",
    "    # 这里min_count会对字典做截断，如果min_coun=5,那么出现次数小于5的词会被丢弃，需要更新原本的字典\n",
    "    model=Word2Vec(sentences=origin_DATA.blurb2vect,size=size,min_count=0,window=5)\n",
    "    # 得到向量矩阵\n",
    "    blurb_word2vec_martics = np.zeros((len(origin_DATA.blurb2int), size))\n",
    "    for i, value in origin_DATA.blurb2int.items():\n",
    "        try:\n",
    "            blurb_word2vec_martics[value, :] = model.wv[i]\n",
    "        except:\n",
    "            print(i ,value)\n",
    "            print(model.wv[i])\n",
    "            exit(1)\n",
    "    print(blurb_word2vec_martics.shape)\n",
    "    # 添加pad字符对应的向量\n",
    "    blurb_word2vec_martics = np.insert(blurb_word2vec_martics, blurb_word2vec_martics.shape[0], axis=0, values=np.zeros(shape=(1, size)))\n",
    "    print(blurb_word2vec_martics.shape)\n",
    "    return blurb_word2vec_martics\n",
    "\n",
    "blurb_word2vec_martics = load_word2vec_martics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增加一个pad字符\n",
    "print(len(blurb_word_list))\n",
    "blurb_dict['<padd>'] = len(blurb_dict)\n",
    "print(len(blurb_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'blurb_dict' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-14072d582612>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mblurb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mblurb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpre_blurb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-42-14072d582612>\u001b[0m in \u001b[0;36mpre_blurb\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpre_blurb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# 将blurb转换为数字矩阵，并处理长度\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mpad_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblurb_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'<padd>'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pad_index = '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mblurb_int_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'blurb_dict' is not defined"
     ]
    }
   ],
   "source": [
    "def pre_blurb():\n",
    "    # 将blurb转换为数字矩阵，并处理长度\n",
    "    pad_index = blurb_dict['<padd>']\n",
    "    print('pad_index = ', pad_index)\n",
    "    blurb_int_list = []\n",
    "    for one_blurb in blurb_word_list:\n",
    "        # 补齐和去长\n",
    "        one_blurb = one_blurb[:200]\n",
    "        if len(one_blurb)< 200:\n",
    "            one_blurb = one_blurb + ['<padd>'] * (200- len(one_blurb))\n",
    "\n",
    "        # 转化为数字列表\n",
    "        temp_list = []\n",
    "        for word in one_blurb:\n",
    "            temp_list.append(blurb_dict[word])\n",
    "        blurb_int_list.append(temp_list)\n",
    "\n",
    "    blurb = np.array(blurb_int_list)\n",
    "    print(blurb.shape)\n",
    "    return blurb\n",
    "\n",
    "blurb = pre_blurb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user-id的字典,总共有28836个用户\n",
    "all_user = len(set(origin_DATA.features['User-ID']))\n",
    "new_user_id = {val: i for i, val in enumerate(set(origin_DATA.features['User-ID']))}\n",
    "print('all user id = ', all_user)\n",
    "# location的数量=7573(从0开始的)\n",
    "all_location = max([j for i in origin_DATA.features.Location for j in i]) +1 \n",
    "print('all location = ', all_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "all isbn =  38036\nall author =  15196\nall year =  81\nall publisher =  2909\nall title words =  23731\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'blurb_dict' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-44480f8f7c09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'all title words = '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_title_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# blurb中所有单词总数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mall_blurb_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblurb_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'all blurb words = '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_blurb_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'blurb_dict' is not defined"
     ]
    }
   ],
   "source": [
    "# ISBN总数\n",
    "all_isbn = len(set(origin_DATA.features['ISBN']))\n",
    "print('all isbn = ', all_isbn)\n",
    "# author总数\n",
    "all_author = len(set(origin_DATA.features['Author']))\n",
    "print('all author = ', all_author)\n",
    "# year总数\n",
    "all_year = len(set(origin_DATA.features['Year']))\n",
    "print('all year = ', all_year)\n",
    "# publish总数\n",
    "all_publisher = len(set(origin_DATA.features['Publisher']))\n",
    "print('all publisher = ', all_publisher)\n",
    "# title中所有单词总数\n",
    "all_title_words = max([j for i in origin_DATA.features.Title for j in i]) +1\n",
    "print('all title words = ', all_title_words)\n",
    "# blurb中所有单词总数\n",
    "all_blurb_words = len(blurb_dict)\n",
    "print('all blurb words = ', all_blurb_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_title(all_title_words=all_title_words):\n",
    "    # 将title转换为数字矩阵，并处理长度\n",
    "    # 对title进行补齐\n",
    "    title = []\n",
    "    for ti in origin_DATA.features['Title'].values:\n",
    "        if len(ti) > 10:\n",
    "            ti = ti[:10]\n",
    "        if len(ti) < 10:\n",
    "            ti = ti + [all_title_words] * (10 - len(ti))\n",
    "\n",
    "        title.append(ti)\n",
    "    all_title_words += 1\n",
    "    title = np.array(title)\n",
    "    return title, all_title_words\n",
    "\n",
    "title,all_title_words = pre_title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_test(): \n",
    "    m = len(origin_DATA.features['Location'])\n",
    "    # 对location取3位数\n",
    "    loca = np.zeros((m, 3))\n",
    "    for i in range(m):\n",
    "        loca[i] = np.array(origin_DATA.features['Location'][i])\n",
    "\n",
    "    input_features = [origin_DATA.features['User-ID'].to_numpy(), loca, \n",
    "                      origin_DATA.features['ISBN'].to_numpy(), origin_DATA.features['Author'].to_numpy(),\n",
    "                     origin_DATA.features['Year'].to_numpy(), origin_DATA.features['Publisher'].to_numpy(), \n",
    "                     title, blurb]\n",
    "    labels = origin_DATA.labels.to_numpy()\n",
    "    # 分割数据集以及shuffle\n",
    "    np.random.seed(100)\n",
    "    number_features = len(input_features)\n",
    "    shuffle_index = np.random.permutation(m)\n",
    "    shuffle_train_index = shuffle_index[:math.ceil(m*0.96)]\n",
    "    shuffle_val_index = shuffle_index[math.ceil(m*0.96): math.ceil(m*0.98)]\n",
    "    shuffle_test_index = shuffle_index[math.ceil(m*0.98):]\n",
    "    train_features = [input_features[i][shuffle_train_index] for i in range(number_features)]\n",
    "    train_labels = labels[shuffle_train_index]\n",
    "    val_features = [input_features[i][shuffle_val_index] for i in range(number_features)]\n",
    "    val_lables = labels[shuffle_val_index]\n",
    "    test_features = [input_features[i][shuffle_test_index] for i in range(number_features)]\n",
    "    test_lables = labels[shuffle_test_index]\n",
    "    return train_features, train_labels, val_features, val_lables, test_features, test_lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'blurb' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-8f16d5e0c676>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_lables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_lables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_train_val_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-51-7c1e1b484222>\u001b[0m in \u001b[0;36mget_train_val_test\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m                       \u001b[0morigin_DATA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ISBN'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morigin_DATA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Author'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                      \u001b[0morigin_DATA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Year'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morigin_DATA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Publisher'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                      title, blurb]\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0morigin_DATA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# 分割数据集以及shuffle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'blurb' is not defined"
     ]
    }
   ],
   "source": [
    "train_features, train_labels, val_features, val_lables, test_features, test_lables = get_train_val_test()\n",
    "print(train_features[0].shape)\n",
    "print(val_features[0].shape)\n",
    "print(test_features[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    # 用户特征输入\n",
    "    user_id = keras.layers.Input(shape=(1,), dtype='int32', name='user_id_input')\n",
    "    user_location = keras.layers.Input(shape=(3,), dtype='int32', name='user_location_input')\n",
    "    \n",
    "    # 书籍特征输入\n",
    "    book_isbn = keras.layers.Input(shape=(1,),  dtype='int32', name='book_isbn_input')\n",
    "    book_author = keras.layers.Input(shape=(1,),  dtype='int32', name='book_author_input')\n",
    "    book_year = keras.layers.Input(shape=(1,),  dtype='int32', name='book_year_input')\n",
    "    book_publisher = keras.layers.Input(shape=(1,),  dtype='int32', name='book_publisher_input')  \n",
    "    book_title = keras.layers.Input(shape=(10, ), dtype='int32', name='book_title_input')\n",
    "    book_blurb = keras.layers.Input(shape=(200, ), dtype='int32', name='book_blurb_input')\n",
    "    return user_id, user_location, book_isbn, book_author, book_year, book_publisher, book_title, book_blurb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 嵌入矩阵的维度\n",
    "embed_dim = 4\n",
    "embed_dim_title = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_embed_layer(u_id, u_loca):\n",
    "    user_id_embedd = keras.layers.Embedding(all_user, embed_dim, name='user_id_embedding')(u_id)\n",
    "    user_loca_embedd = keras.layers.Embedding(all_location, embed_dim , name='user_loca_embedding')(u_loca)\n",
    "    return user_id_embedd, user_loca_embedd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_emded_layer(b_isbn, b_atuhor, b_year, b_publisher, b_title, b_blurb):\n",
    "    book_isbn_embedd = keras.layers.Embedding(all_isbn, embed_dim, name='book_isbn_embedding')(b_isbn)\n",
    "    book_author_embedd = keras.layers.Embedding(all_author, embed_dim, name='book_author_embedding')(b_atuhor)\n",
    "    book_year_embedd = keras.layers.Embedding(all_year, embed_dim, name='book_year_embedding')(b_year)\n",
    "    book_publisher_embedd = keras.layers.Embedding(all_publisher, embed_dim, name='book_publisher_embedding')(b_publisher)\n",
    "    \n",
    "    book_title_embedd = keras.layers.Embedding(all_title_words, embed_dim_title, name='book_title_embedding')(b_title)\n",
    "    # 加载预训练模型\n",
    "    book_blurb_embedd = keras.layers.Embedding(all_blurb_words, blurb_embedding_dim, name='book_blurb_embedding', weights=[blurb_word2vec_martics], trainable=True)(b_blurb)\n",
    "    return book_isbn_embedd, book_author_embedd, book_year_embedd, book_publisher_embedd, book_title_embedd, book_blurb_embedd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_feature(u_id_embedd, u_loca_embedd):\n",
    "    u_id_layer = keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.nn.l2_loss, name='u_id_dense')(u_id_embedd)\n",
    "#     u_id_layer_drop = keras.layers.Dropout(rate=0.5, name='u_id_layer_drop')(u_id_layer)\n",
    "    # u_id_layer.shape = (?, 1, 32)\n",
    "    # u_loca_layer.shape = (?, 32)\n",
    "    # 这里可以再加个Dense\n",
    "    u_loca_layer = keras.layers.Bidirectional(tf.keras.layers.LSTM(16, name='u_loca_bilstm'), merge_mode='concat')(u_loca_embedd)\n",
    "    u_loca_lstm_dense = keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.nn.l2_loss, name='u_loca_lstm_dense')(u_loca_layer)\n",
    "    u_id_reshape = keras.layers.Reshape([32])(u_id_layer)\n",
    "    u_combine = keras.layers.concatenate([u_id_reshape, u_loca_lstm_dense],axis=1, name='u_combine')\n",
    "    print(u_combine.shape)\n",
    "    # 这里能不能用激活函数\n",
    "    u_feature_layer = keras.layers.Dense(100, activation='tanh', name='u_feature_layer')(u_combine)\n",
    "    print(u_feature_layer.shape)\n",
    "    return u_feature_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_dense = 4\n",
    "def get_book_feature(b_isbn_embedd, b_author_embedd, b_year_embedd, b_publisher_embedd, b_title_embedd, b_blurb_embedd):\n",
    "    # 首先对前4个特征连接Dense层\n",
    "    b_isbn_dense = keras.layers.Dense(b_dense, activation='relu', kernel_regularizer=tf.nn.l2_loss, name='b_isbn_dense')(b_isbn_embedd)\n",
    "    b_author_dense = keras.layers.Dense(b_dense, activation='relu', kernel_regularizer=tf.nn.l2_loss, name='b_author_dense')(b_author_embedd)\n",
    "    b_year_dense = keras.layers.Dense(b_dense, activation='relu', kernel_regularizer=tf.nn.l2_loss, name='b_year_dense')(b_year_embedd)\n",
    "    b_publisher_dense = keras.layers.Dense(b_dense, activation='relu', kernel_regularizer=tf.nn.l2_loss, name='b_publisher_dense')(b_publisher_embedd)\n",
    "    # 合并这四个特征,  b_combine_four shape = (?, 1, 16)\n",
    "    b_combine_four = keras.layers.concatenate([b_isbn_dense, b_author_dense, b_year_dense, b_publisher_dense], name='b_four_combine')\n",
    "    print('b_combine_four.shape', b_combine_four.shape)\n",
    "    # 对title进行卷积\n",
    "    b_title_reshape = keras.layers.Lambda(lambda layer: tf.expand_dims(layer, 3))(b_title_embedd)  # shape=(?,10, 16, 1)\n",
    "    print('b_title_reshape.shape = ', b_title_reshape.shape)\n",
    "    # b_title_conv.shape = \n",
    "    b_title_conv = keras.layers.Conv2D(filters=8, kernel_size=(2, embed_dim_title), kernel_regularizer=tf.nn.l2_loss, strides=1)(b_title_reshape)# shape=(?, 14, 1, 8)\n",
    "    # b-title_pool.shape =\n",
    "    b_title_pool = keras.layers.MaxPool2D(pool_size=(9, 1), strides=1)(b_title_conv) # shape=(?,1, 1, 8)\n",
    "    print('b_title_conv.shape = ', b_title_conv)\n",
    "    print('b_title_pool.shape = ', b_title_pool)\n",
    "    \n",
    "    # 对blurb进行处理\n",
    "    # shape = \n",
    "    b_blurb_lstm_1 = keras.layers.Bidirectional(tf.keras.layers.LSTM(16, name='b_blurb_bilstm', dropout=0.5, return_sequences=True), merge_mode='concat')(b_blurb_embedd) \n",
    "    print('b_blurb_lstm_1.shape = ', b_blurb_lstm_1.shape)\n",
    "    b_blurb_lstm_2 = keras.layers.LSTM(32, name='b_blurb_lstm', dropout=0.5, return_sequences=False)(b_blurb_lstm_1) \n",
    "    print('b_blurb_lstm_2.shape = ', b_blurb_lstm_2.shape)\n",
    "    # 将title和blurb合并\n",
    "    b_title_reshape = keras.layers.Reshape([b_title_pool.shape[3]])(b_title_pool)\n",
    "    # b_combine_blurb_title.shape = \n",
    "    b_combine_blurb_title = keras.layers.concatenate([b_title_reshape, b_blurb_lstm_2], axis=1, name='b_combine_blurb_title')\n",
    "    print('b_combine_blurb_title.shape = ', b_combine_blurb_title)\n",
    "    b_blurb_title_dense = keras.layers.Dense(64, kernel_regularizer=tf.nn.l2_loss, activation='relu', name='b_blurb_title_dense')(b_combine_blurb_title)\n",
    "#     b_blurb_title_dense_drop = keras.layers.Dropout(rate=0.5, name='b_blurb_title_dense_drop')(b_blurb_title_dense)\n",
    "    b_blurb_title_dense = keras.layers.Dense(64, activation='relu', name='b_blurb_title_dense')(b_combine_blurb_title)\n",
    "    # b_combine_four_reshape shape = (?, 64)\n",
    "    b_combine_four_reshape = keras.layers.Reshape([b_combine_four.shape[2]], name='b_combine_four_reshape')(b_combine_four)\n",
    "    # 合并所有的书籍特征\n",
    "#     b_combine_book = keras.layers.concatenate([b_combine_blurb_title, b_combine_four_reshape], axis=1, name='b_combine_book')\n",
    "    b_combine_book = keras.layers.concatenate([b_blurb_title_dense, b_combine_four_reshape], axis=1, name='b_combine_book')\n",
    "\n",
    "    # 得到书籍矩阵\n",
    "    b_feature_layer = keras.layers.Dense(100, name='b_feature_layer', activation='tanh')(b_combine_book)\n",
    "    return b_feature_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating(user_feature, book_feature):\n",
    "#     multiply_layer = keras.layers.Lambda(lambda layer: tf.reduce_sum(layer[0]+layer[1], axis=1, keepdims=True), name = 'user_book_feature')((user_feature, book_feature))\n",
    "    inference_layer = keras.layers.concatenate([user_feature, book_feature], axis=1, name='user_book_feature')\n",
    "    inference_dense = tf.keras.layers.Dense(64, kernel_regularizer=tf.nn.l2_loss, activation='relu')(inference_layer)\n",
    "    multiply_layer = tf.keras.layers.Dense(1, name=\"inference\")(inference_layer)  # inference_dense\n",
    "    print(multiply_layer.shape)\n",
    "    return multiply_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './model/'\n",
    "\n",
    "class model_network():\n",
    "    def __init__(self):\n",
    "        self.batchsize = 512\n",
    "        self.epoch = 15\n",
    "    def creat_model(self):\n",
    "        user_id, user_location, book_isbn, book_author, book_year, book_publisher, book_title, book_blurb = get_inputs()\n",
    "        user_id_embedd, user_loca_embedd = user_embed_layer(user_id, user_location)\n",
    "        book_isbn_embedd, book_author_embedd, book_year_embedd, book_publisher_embedd, book_title_embedd, book_blurb_embedd = book_emded_layer(book_isbn, book_author, book_year, book_publisher, book_title, book_blurb)\n",
    "        u_feature_layer = get_user_feature(user_id_embedd, user_loca_embedd)\n",
    "        b_feature_layer = get_book_feature(book_isbn_embedd, book_author_embedd, book_year_embedd, book_publisher_embedd, book_title_embedd, book_blurb_embedd)\n",
    "        multiply_layer = get_rating(u_feature_layer, b_feature_layer)\n",
    "        model = keras.Model(inputs=[user_id, user_location, book_isbn, book_author, book_year, book_publisher, book_title, book_blurb],\n",
    "                    outputs=[multiply_layer])\n",
    "        return model\n",
    "    def train_model(self):\n",
    "        weights_path = './model_weights/model_2.hdf5'\n",
    "#         checkpoint = keras.callbacks.ModelCheckpoint(filepath=weights_path, monitor='val_loss', mode='min', save_weights_only=True)\n",
    "        model_optimizer = keras.optimizers.Adamax()\n",
    "        model = self.creat_model()\n",
    "        model.compile(optimizer=model_optimizer, loss=keras.losses.mse)\n",
    "        history = model.fit(train_features, train_labels, validation_data=(val_features, val_lables), epochs=self.epoch, batch_size=self.batchsize, verbose=1)\n",
    "        print(model.summary())\n",
    "        return model, history\n",
    "    def predict_model(self, model):\n",
    "        test_loss = model.evaluate(test_features, test_lables, verbose=0)\n",
    "        return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'all_blurb_words' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-5a2c89edb1c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnet_work\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet_work\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-70-395fdf962e9a>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#         checkpoint = keras.callbacks.ModelCheckpoint(filepath=weights_path, monitor='val_loss', mode='min', save_weights_only=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mmodel_optimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreat_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_lables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-70-395fdf962e9a>\u001b[0m in \u001b[0;36mcreat_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0muser_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_isbn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_author\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_year\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_publisher\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_title\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_blurb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0muser_id_embedd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_loca_embedd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_embed_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_location\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mbook_isbn_embedd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_author_embedd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_year_embedd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_publisher_embedd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_title_embedd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_blurb_embedd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbook_emded_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbook_isbn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_author\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_year\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_publisher\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_title\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_blurb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mu_feature_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_user_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_id_embedd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_loca_embedd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mb_feature_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_book_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbook_isbn_embedd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_author_embedd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_year_embedd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_publisher_embedd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_title_embedd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_blurb_embedd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-62-ce984e297d2f>\u001b[0m in \u001b[0;36mbook_emded_layer\u001b[1;34m(b_isbn, b_atuhor, b_year, b_publisher, b_title, b_blurb)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mbook_title_embedd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_title_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_dim_title\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'book_title_embedding'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_title\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# 加载预训练模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mbook_blurb_embedd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_blurb_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblurb_embedding_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'book_blurb_embedding'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mblurb_word2vec_martics\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_blurb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbook_isbn_embedd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_author_embedd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_year_embedd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_publisher_embedd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_title_embedd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbook_blurb_embedd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_blurb_words' is not defined"
     ]
    }
   ],
   "source": [
    "net_work = model_network()\n",
    "model, history = net_work.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "plt.figure(1)\n",
    "plt.plot(train_loss, c='r', label='train_loss')\n",
    "plt.plot(val_loss, c='b', label='val_loss')\n",
    "plt.legend()\n",
    "plt.xlim([0, 15])\n",
    "# plt.ylim([0, 20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-15d62dfdf105>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet_work\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "test_loss = net_work.predict_model(model)\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画出模型图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-13264eab90b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'model_2.png'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(model, to_file='model_2.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}