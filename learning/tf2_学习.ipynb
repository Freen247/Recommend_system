{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 包的导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "sys.version_info(major=3, minor=6, micro=7, releaselevel='final', serial=0)\ntensorflow 2.1.0\nmatplotlib 3.2.0\nnumpy 1.18.1\npandas 1.0.1\nsklearn 0.22.2.post1\ntensorflow 2.1.0\ntensorflow_core.python.keras.api._v2.keras 2.2.4-tf\n"
    }
   ],
   "source": [
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "import os, sys, time \n",
    "\n",
    "\n",
    "print(sys.version_info)\n",
    "for module in tf, mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF data模块处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<TensorSliceDataset shapes: (), types: tf.float32>\ntf.Tensor(0.0, shape=(), dtype=float32)\ntf.Tensor(1.0, shape=(), dtype=float32)\ntf.Tensor(2.0, shape=(), dtype=float32)\ntf.Tensor(3.0, shape=(), dtype=float32)\ntf.Tensor(4.0, shape=(), dtype=float32)\ntf.Tensor(5.0, shape=(), dtype=float32)\ntf.Tensor(6.0, shape=(), dtype=float32)\ntf.Tensor(7.0, shape=(), dtype=float32)\ntf.Tensor(8.0, shape=(), dtype=float32)\ntf.Tensor(9.0, shape=(), dtype=float32)\n"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(np.arange(10,dtype=np.float32))\n",
    "print(dataset)\n",
    "for _ in dataset:\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tf.Tensor([0. 1. 2. 3. 4. 5. 6.], shape=(7,), dtype=float32)\ntf.Tensor([7. 8. 9. 0. 1. 2. 3.], shape=(7,), dtype=float32)\ntf.Tensor([4. 5. 6. 7. 8. 9. 0.], shape=(7,), dtype=float32)\ntf.Tensor([1. 2. 3. 4. 5. 6. 7.], shape=(7,), dtype=float32)\ntf.Tensor([8. 9.], shape=(2,), dtype=float32)\n"
    }
   ],
   "source": [
    "'''\n",
    "数据操作：\n",
    "'''\n",
    "# repeat epoch+get batch\n",
    "dataset = dataset.repeat(3).batch(7)\n",
    "for _ in dataset:\n",
    "    print(_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tf.Tensor(0.0, shape=(), dtype=float32)\ntf.Tensor(1.0, shape=(), dtype=float32)\ntf.Tensor(2.0, shape=(), dtype=float32)\ntf.Tensor(3.0, shape=(), dtype=float32)\ntf.Tensor(4.0, shape=(), dtype=float32)\ntf.Tensor(7.0, shape=(), dtype=float32)\ntf.Tensor(8.0, shape=(), dtype=float32)\ntf.Tensor(9.0, shape=(), dtype=float32)\ntf.Tensor(0.0, shape=(), dtype=float32)\ntf.Tensor(1.0, shape=(), dtype=float32)\ntf.Tensor(4.0, shape=(), dtype=float32)\ntf.Tensor(5.0, shape=(), dtype=float32)\ntf.Tensor(6.0, shape=(), dtype=float32)\ntf.Tensor(7.0, shape=(), dtype=float32)\ntf.Tensor(8.0, shape=(), dtype=float32)\ntf.Tensor(1.0, shape=(), dtype=float32)\ntf.Tensor(2.0, shape=(), dtype=float32)\ntf.Tensor(3.0, shape=(), dtype=float32)\ntf.Tensor(4.0, shape=(), dtype=float32)\ntf.Tensor(5.0, shape=(), dtype=float32)\ntf.Tensor(8.0, shape=(), dtype=float32)\ntf.Tensor(9.0, shape=(), dtype=float32)\ntf.Tensor(5.0, shape=(), dtype=float32)\ntf.Tensor(6.0, shape=(), dtype=float32)\ntf.Tensor(2.0, shape=(), dtype=float32)\ntf.Tensor(3.0, shape=(), dtype=float32)\ntf.Tensor(9.0, shape=(), dtype=float32)\ntf.Tensor(0.0, shape=(), dtype=float32)\ntf.Tensor(6.0, shape=(), dtype=float32)\ntf.Tensor(7.0, shape=(), dtype=float32)\n"
    }
   ],
   "source": [
    "# interleave\n",
    "# case ：文件dataset -> 具体数据集\n",
    "dataset2 = dataset.interleave(\n",
    "    # map_fn\n",
    "    lambda x: tf.data.Dataset.from_tensor_slices(x),\n",
    "    # cycle_length\n",
    "    cycle_length = 5,\n",
    "    # block_length\n",
    "    block_length = 5,\n",
    ")\n",
    "for _ in dataset2:\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "value: \"machine learning\"\nvalue: \"cc150\"\n\nvalue: 15.5\nvalue: 9.5\nvalue: 7.0\nvalue: 8.0\n\nvalue: 42\n\nfeature {\n  key: \"age\"\n  value {\n    int64_list {\n      value: 42\n    }\n  }\n}\nfeature {\n  key: \"favorite_book\"\n  value {\n    bytes_list {\n      value: \"machine learning\"\n      value: \"cc150\"\n    }\n  }\n}\nfeature {\n  key: \"hours\"\n  value {\n    float_list {\n      value: 15.5\n      value: 9.5\n      value: 7.0\n      value: 8.0\n    }\n  }\n}\n\n"
    }
   ],
   "source": [
    "# tfrecord 文件格式\n",
    "# -> tf.train.Example\n",
    "# -> tf.train.Feature -> {'key':tf.train.Feature}\n",
    "# -> tf.train.Feature - >tf.train.ByteList/FloatList/Int64List\n",
    "favorite_books = [name.encode('utf-8') for name in ['machine learning', 'cc150']]\n",
    "favorite_books_bytelist = tf.train.BytesList(value=favorite_books)\n",
    "print(favorite_books_bytelist)\n",
    "\n",
    "hours_floatlist = tf.train.FloatList(value=[15.5, 9.5, 7.0, 8.0])\n",
    "print(hours_floatlist)\n",
    "\n",
    "age_int64list = tf.train.Int64List(value=[42])\n",
    "print(age)\n",
    "\n",
    "features = tf.train.Features(\n",
    "    feature = {\n",
    "        'favorite_book': tf.train.Feature(bytes_list = favorite_books_bytelist),\n",
    "        'hours': tf.train.Feature(float_list = hours_floatlist),\n",
    "        'age': tf.train.Feature(int64_list = age_int64list),\n",
    "    }\n",
    ")\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF 骚操作\n",
    "将自己写的function转化成tf.function\n",
    "优势：快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.function and auto-graph\n",
    "def scaled_elu(z, scale=1.0, alpha=1.0):\n",
    "    # z>=0? scale * z : scale:alpha * tf.nn.eu(z)\n",
    "    is_postive = tf.greater_equal(z, 0.0)\n",
    "    return scale * tf.where(is_postive, z, alpha * tf.nn.elu(z))\n",
    "print(scaled_elu(tf.constant(-1.)))\n",
    "print(scaled_elu(tf.constant([-3., -2.5])))\n",
    "# 用tf.function 将函数转化为tf函数\n",
    "# 方式1\n",
    "tf_scale = tf.function(scaled_elu)\n",
    "print(tf_scale(tf.constant(-1.)))\n",
    "print(tf_scale(tf.constant([-3., -2.5])))\n",
    "# 方式2\n",
    "@tf.function\n",
    "def tf_scale2(z, scale=1.0, alpha=1.0):\n",
    "    # z>=0? scale * z : scale:alpha * tf.nn.eu(z)\n",
    "    is_postive = tf.greater_equal(z, 0.0)\n",
    "    return scale * tf.where(is_postive, z, alpha * tf.nn.elu(z))\n",
    "print(tf_scale2(tf.constant(-1.)))\n",
    "print(tf_scale2(tf.constant([-3., -2.5])))\n",
    "\n",
    "# 将展转化后的函数展示出来\n",
    "def display_tf_code(func):\n",
    "    code = tf.autograph.to_code(func)\n",
    "    from IPython.display import display, Markdown\n",
    "    display(Markdown('```python\\n{}\\n```'.format(code)))\n",
    "# 展示源码\n",
    "display_tf_code(scaled_elu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = tf.Variable(0.)\n",
    "\n",
    "@tf.function\n",
    "def add_21():\n",
    "    return var.assign_add(21)\n",
    "print(add_21())\n",
    "\n",
    "# 如果将var放在函数里面?,会报错!所以在我们定义使用function之前我们需要将var = tf.Variable(0.)定义在函数初始化之前\n",
    "@tf.function\n",
    "def add_21():\n",
    "    var = tf.Variable(0.)\n",
    "    return var.assign_add(21)\n",
    "# print(add_21()) 打印会报错\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def cube(z):\n",
    "    return tf.pow(z,3)\n",
    "# 我们输入的数据类型不一样 输出的类型也不一样\n",
    "print(cube(tf.constant([1.,2.,3.])))\n",
    "print(cube(tf.constant([1,2,3])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们给函数加上一个变量限制\n",
    "@tf.function(input_signature=[tf.TensorSpec([None], tf.int32, name='x')])\n",
    "def cube(z):\n",
    "    return tf.pow(z,3)\n",
    "try:\n",
    "    print(cube(tf.constant([1.,2.,3.])))\n",
    "    print(cube(tf.constant([1,2,3])))\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_func_int32 = cube.get_concrete_function(tf.TensorSpec([None],tf.int32, name='x'))\n",
    "print(cube_func_int32)\n",
    "print(cube_func_int32 is cube.get_concrete_function(tf.TensorSpec([5], tf.int32, name='x')))\n",
    "print(cube_func_int32 is cube.get_concrete_function(tf.constant([1,2,3])))\n",
    "print(cube_func_int32.graph)\n",
    "print(cube_func_int32.graph.get_operations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pow_op =  cube_func_int32.graph.get_operations()[2]\n",
    "# 打印操作的过程\n",
    "print(pow_op)\n",
    "# 其中的值都是可以取出来的\n",
    "print(pow_op.name)\n",
    "print(list(pow_op.inputs))\n",
    "print(list(pow_op.outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求近似导数\n",
    "f = lambda x:3. *x **2 +2.*x-1\n",
    "approximate_derivative = lambda f, x, eps=1e-4 : (f(x+eps) - f(x-eps))/(2. * eps)\n",
    "print(approximate_derivative(f,1.))\n",
    "\n",
    "g = lambda x1,x2:(x1+5)*(x2**2)\n",
    "def approximate_gradient(g, x1, x2, eps=1e-3):\n",
    "    dg_x1 = approximate_derivative(lambda x:g(x,x2),x1,eps)\n",
    "    dg_x2 = approximate_derivative(lambda x:g(x2,x),x2,eps)\n",
    "    return dg_x1,dg_x2\n",
    "print(approximate_gradient(g,2.,3.))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.Variable(2.0)\n",
    "x2 = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = g(x1,x2)\n",
    "# 对z函数求x1的偏导\n",
    "# tape只能调用一次\n",
    "dz_x1 = tape.gradient(z,x1)\n",
    "print(dz_x1)\n",
    "\n",
    "try:\n",
    "    dz_x2 = tape.gradient(z,x2)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们加上persistent后就可以求两次偏导了，结果为9和42，和上面的结果误差比较大\n",
    "with tf.GradientTape(persistent = True) as tape:\n",
    "    z = g(x1,x2)\n",
    "\n",
    "dz_x1 = tape.gradient(z,x1)\n",
    "print(dz_x1)\n",
    "\n",
    "try:\n",
    "    dz_x2 = tape.gradient(z,x2)\n",
    "    print(dz_x2)\n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "del(tape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一次性求出两个偏导\n",
    "with tf.GradientTape() as tape:\n",
    "    z = g(x1,x2)\n",
    "\n",
    "dz = tape.gradient(z,[x1,x2])\n",
    "print(dz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x1 = tf.constant(2.0)\n",
    "x2 = tf.constant(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x1)\n",
    "    tape.watch(x2)\n",
    "    z = g(x1,x2)\n",
    "print(tape.gradient(z, [x1,x2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(5.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z1 = 3*x\n",
    "    z2 = x**2\n",
    "tape.gradient([z1,z2],x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟一次梯度下降\n",
    "f = lambda x:3. *x **2 +2.*x-1\n",
    "learning_rate = 0.1\n",
    "x = tf.Variable(0.0)\n",
    "print(x)\n",
    "for _ in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        z = f(x)\n",
    "    dz_dx = tape.gradient(z,x)\n",
    "    # x.assign_sub(learning_rate*dz_dx) 表示x= x-(learning_rate*dz_dx)\n",
    "    x.assign_sub(learning_rate*dz_dx)\n",
    "    # print(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟一次梯度下降\n",
    "f = lambda x:3. *x **2 +2.*x-1\n",
    "learning_rate = 0.1\n",
    "x = tf.Variable(0.0)\n",
    "optimizer = keras.optimizers.SGD(lr = learning_rate)\n",
    "print(x)\n",
    "for _ in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        z = f(x)\n",
    "    dz_dx = tape.gradient(z,x)\n",
    "    optimizer.apply_gradients([(dz_dx,x)])\n",
    "    # print(x)\n",
    "    # x.assign_sub(learning_rate*dz_dx) 表示x= x-(learning_rate*dz_dx)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 数据集的准备\n",
    "Fashion-MNIST是一个替代MNIST手写数字集的图像数据集。 它是由Zalando（一家德国的时尚科技公司）旗下的研究部门提供。其涵盖了来自10种类别的共7万个不同商品的正面图片。Fashion-MNIST的大小、格式和训练集/测试集划分与原始的MNIST完全一致。60000/10000的训练测试数据划分，28x28的灰度图片。你可以直接用它来测试你的机器学习和深度学习算法性能，且不需要改动任何的代码。\n",
    "- 60000张训练图像和对应Label；\n",
    "- 10000张测试图像和对应Label；\n",
    "- 10个类别；\n",
    "- 每张图像28x28的分辨率；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(x_train_all, y_train_all), (x_test, y_test) = fashion_mnist.load_data()\n",
    "# fashion_mnist 训练集共有60000，将前5000作为验证集，后55000作为训练集\n",
    "x_valid, x_train = x_train_all[:5000], x_train_all[5000:]\n",
    "y_valid, y_train = y_train_all[:5000], y_train_all[5000:]\n",
    "\n",
    "print(\"x_valid.shape, y_valid.shape:\",x_valid.shape, y_valid.shape)\n",
    "print(\"x_train.shape, y_train.shape\",x_train.shape, y_train.shape)\n",
    "print(\"x_test.shape, y_test.shape\",x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 数据集样本展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集样本展示\n",
    "def show_single_img(img_arr):\n",
    "    plt.imshow(img_arr, cmap=\"binary\")\n",
    "    plt.show()\n",
    "# show_single_img(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 对数据进行归一化处理\n",
    "> 定义：把数据经过处理后使之限定在一定的范围内。比如通常限制在区间`[0, 1]`或者`[-1, 1]`\n",
    "\n",
    "常用归一化法：\n",
    "- 最大-最小标准化: $$\\frac{x-x_{min}}{x_{max}-x_{min}}$$\n",
    "- Z-score标准化: $$\\frac{x-\\mu }{std} \\left ( \\mu为标准差 ,std 为方差 \\right ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "对数据进行归一化处理\n",
    "定义：把数据经过处理后使之限定在一定的范围内。比如通常限制在区间[0, 1]或者[-1, 1]\n",
    "Z-score归一化\n",
    "'''\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# x_train:[None, 28, 28] -> [None, 784]\n",
    "# 我们np里的数据是int类型，所以我们需要x_train.astype(np.float32)将数据转化成float32\n",
    "# fit_transform 不仅有数据转化为归一化的功能，还有fit（将数据存储下来）的功能.\n",
    "Z_score = lambda d:scaler.fit_transform(d.astype(np.float32).reshape(-1,1)).reshape(-1,28,28)\n",
    "x_train_scaled = Z_score(x_train)\n",
    "x_valid_scaled = Z_score(x_valid)\n",
    "x_test_scaled = Z_score(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = keras.metrics.MeanSquaredError()\n",
    "print(metric([5.],[2.,1],))\n",
    "metric.reset_states\n",
    "print(metric([0.],[1.]))\n",
    "\n",
    "print(metric.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch : 遍历训练集metric\n",
    "- 自动求导\n",
    "epoch结束:验证集 metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "steps_per_epoch = len(x_train_scaled) // batch_size\n",
    "print(steps_per_epoch)\n",
    "optimizer = keras.optimizers.SGD()\n",
    "\n",
    "def random_batch(x,y,batch_size=32):\n",
    "    idx = np.random.randint(0, len(x), size=batch_size)\n",
    "    return x[idx], y[idx]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = 'relu', input_shape=x_train.shape[1:]),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "metric = keras.metrics.MeanSquaredError()\n",
    "for epoch in range(epochs):\n",
    "    metric.reset_states()\n",
    "    for step in range(steps_per_epoch):\n",
    "        x_batch, y_batch = random_batch(x_train_scaled, y_train, batch_size)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x_batch)\n",
    "            loss = tf.reduce_mean(keras.losses.mean_squared_error(y_batch, y_pred))\n",
    "            metric(y_batch, y_pred)\n",
    "        grads = tape.gradient(loss, model.variables)\n",
    "        grads_and_vars = zip(grads, model.variables)\n",
    "        optimizer.apply_gradients(grads_and_vars)\n",
    "        print('\\rEpoch', epoch, 'train mse:',metric.result().numpy(), end='')\n",
    "    y_valid_pred = model(x_valid_scaled)\n",
    "    # 将数据转化成float32格式，不然会报错\n",
    "    y_valid = tf.dtypes.cast(y_valid,tf.float32)\n",
    "    valid_loss = tf.reduce_mean(keras.losses.mean_squared_error(y_valid_pred, y_valid))\n",
    "    print('\\t', 'valid mse:', valid_loss.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 模型的构建\n",
    "tf.keras.models.sequential()\n",
    "普通的模型构建：\n",
    "```\n",
    "model = keras.models.Sequential([\n",
    "    # 第一层输入成，每一个组数据为[28,28]的二维矩阵，通过keras.layers.Flatten压缩成一维矩阵\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300, activation = \"relu\"),\n",
    "    keras.layers.Dense(100, activation = \"relu\"),\n",
    "    keras.layers.Dense(10, activation = \"softmax\"),\n",
    "])\n",
    "```\n",
    "批归一化加激活函数模型构建：\n",
    "\n",
    "```\n",
    "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,activation='relu'))\n",
    "    # 批归一化\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    '''\n",
    "    model.add(keras.layers.Dense(100))\n",
    "    # 将批归一化放在激活函数之前\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    # 我们还可以将激活函数设置为一个层次\n",
    "    model.add(keras.layers.Activation('selu'))\n",
    "    model.add(keras.layers.Dense(10,activation='softmax'))\n",
    "    '''\n",
    "```\n",
    "\n",
    "添加dropout层，一般情况下我们只在最后几层添加dropout：\n",
    "\n",
    "```\n",
    "# deep_neural_network模型的构建(20层)\n",
    "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "for _ in range(20):\n",
    "    # 其中selu是一个自带归一化的激活函数,会在\n",
    "    model.add(keras.layers.Dense(100,activation='selu'))\n",
    "# AlphaDropout相比 强大在：1，均值和方差不变 2.归一化的性质不变，分布不会发生变化，可以和批归一化一起使用\n",
    "model.add(keras.layers.AlphaDropout(rate=0.5,))\n",
    "# model.add(keras.layers.Dropout(rate=0.5))\n",
    "model.add(keras.layers.Dense(10,activation='softmax'))\n",
    "```\n",
    "wide&deep模型的构建：\n",
    "```\n",
    "# 使用API实现wide&deep模型\n",
    "class WideDeepModel(keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(WideDeepModel, self).__init__()\n",
    "        '''定义模型的层次'''\n",
    "        self.hidden1_layer = keras.layers.Dense(30, activation='relu')\n",
    "        self.hidden2_layer = keras.layers.Dense(30, activation='relu')\n",
    "        self.output_layer = keras.layers.Dense(1)\n",
    "        \n",
    "\n",
    "    def call(self, input):\n",
    "        '''完成模型的正向计算'''\n",
    "        hidden1 = self.hidden1_layer(input)\n",
    "        hidden2 = self.hidden2_layer(input)\n",
    "        concat = keras.layers.concatenate([input, hidden2])\n",
    "        output = self.output_layer(concat)\n",
    "        return output\n",
    "# wd模型的建立方式1,这样可以展示更多的细节\n",
    "'''\n",
    "# none 是样本数目，8对应的是features的数目\n",
    "Model: \"wide_deep_model_14\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_42 (Dense)             multiple                  270       \n",
    "_________________________________________________________________\n",
    "dense_43 (Dense)             multiple                  270       \n",
    "_________________________________________________________________\n",
    "dense_44 (Dense)             multiple                  39        \n",
    "=================================================================\n",
    "'''\n",
    "model = WideDeepModel()\n",
    "# wd模型的建立方式2\n",
    "# model = keras.models.Sequential([\n",
    "#     WideDeepModel(),\n",
    "# ])\n",
    "\n",
    "'''\n",
    "# none 是样本数目，8对应的是features的数目\n",
    "Model: \"sequential_1\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "wide_deep_model_13 (WideDeep multiple                  579       \n",
    "=================================================================\n",
    "'''\n",
    "model.build(input_shape=(None, 8))\n",
    "```\n",
    "对于多输入的wide&deep模型的构建：\n",
    "```\n",
    "# 前五个feature当作wide模型的输入\n",
    "input_wide = keras.layers.Input(shape=[5])\n",
    "# 后六个feature当作deep模型的输入\n",
    "input_deep = keras.layers.Input(shape=[6])\n",
    "# deep_model有两个隐含层\n",
    "hidden1 = keras.layers.Dense(30,activation='relu')(input_deep)\n",
    "hidden2 = keras.layers.Dense(30,activation='relu')(hidden1)\n",
    "# 将构建的deep层和wide给拼接起来\n",
    "concat = keras.layers.concatenate([input_wide, hidden2])\n",
    "# 定义输出层\n",
    "output = keras.layers.Dense(1)\n",
    "# 模型定义完成\n",
    "model = keras.model.Model(inputs=[input_wide, input_deep], outputs=[output]))\n",
    "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "# 对于deep&and模型多输入的时候，我们的的训练数据也需要进行拆分\n",
    "x_train_scaled_wide = x_train_scaled[:,:5]\n",
    "x_train_scaled_deep = x_train_scaled[:,2:]\n",
    "x_valid_scaled_wide = x_valid_scaled[:,:5]\n",
    "x_valid_scaled_deep = x_valid_scaled[:,2:]\n",
    "x_test_scaled_wide = x_test_scaled[:,:5]\n",
    "x_test_scaled_deep = x_test_scaled[:,2:]\n",
    "# 数据的训练\n",
    "history = model.fit([x_train_scaled_wide,x_train_scaled_deep], \n",
    "                        y_train,\n",
    "                        epochs=10,\n",
    "                        validation_data=([x_valid_scaled_wide,x_valid_scaled_deep],y_valid))\n",
    "# mdhistory.history\n",
    "```\n",
    "对于多输入多输出的wide&deep模型的构建：\n",
    "```\n",
    "# 前五个feature当作wide模型的输入\n",
    "input_wide = keras.layers.Input(shape=[5])\n",
    "# 后六个feature当作deep模型的输入\n",
    "input_deep = keras.layers.Input(shape=[6])\n",
    "# deep_model有两个隐含层\n",
    "hidden1 = keras.layers.Dense(30,activation='relu')(input_deep)\n",
    "hidden2 = keras.layers.Dense(30,activation='relu')(hidden1)\n",
    "# 将构建的deep层和wide给拼接起来\n",
    "concat = keras.layers.concatenate([input_wide, hidden2])\n",
    "# 定义输出层\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "# 在hidden2层后我们定义一次输出\n",
    "output2 = keras.layers.Dense(1)(hidden2)\n",
    "# 模型定义完成\n",
    "model = keras.models.Model(inputs=[input_wide, input_deep], \n",
    "                            outputs=[output,output2])\n",
    "\n",
    "# reason for sparse : y->index, y->one_hot->[],我们需要将y处理成one_hot向量所以用sparse\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer='sgd', metrics= ['accuracy'])\n",
    "# 定义的模型中的所有层\n",
    "model.layers\n",
    "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "# 对于deep&and模型我们的的训练数据也需要进行拆分\n",
    "x_train_scaled_wide = x_train_scaled[:,:5]\n",
    "x_train_scaled_deep = x_train_scaled[:,2:]\n",
    "x_valid_scaled_wide = x_valid_scaled[:,:5]\n",
    "x_valid_scaled_deep = x_valid_scaled[:,2:]\n",
    "x_test_scaled_wide = x_test_scaled[:,:5]\n",
    "x_test_scaled_deep = x_test_scaled[:,2:]\n",
    "# 数据的训练, 当有多个输出的时候，我们的y_train也需要定义两份\n",
    "history = model.fit([x_train_scaled_wide,x_train_scaled_deep], \n",
    "                        [y_train,y_train],\n",
    "                        epochs=10,\n",
    "                        validation_data=([x_valid_scaled_wide,x_valid_scaled_deep],[y_valid,y_valid]))\n",
    "# mdhistory.history\n",
    "```\n",
    "超参：\n",
    "神经网络有很多训练过程中不变的参数\n",
    "- 网络结构参数：几层，每层宽度，每层激活函数等\n",
    "- 训练参数：batch_size，学习率，学习率衰减算法等\n",
    "手工去试的话比较耗费人力\n",
    "超参搜索策略：\n",
    "- 网络搜索\n",
    "- 随机搜索\n",
    "- 遗传算法搜索\n",
    "- 启发式搜索\n",
    "使用sklearn封装keras模型：\n",
    "```\n",
    "'''\n",
    "RandomizedSearchCV\n",
    "转化为skleran的model\n",
    "定义参数集合\n",
    "搜索参数\n",
    "'''\n",
    "def build_model(hidden_layers = 1,\n",
    "                            layer_size = 30,\n",
    "                            learning_rate = 3e-3):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(layer_size, activation='relu',inpute_shape=x_train.shape[1:]))\n",
    "    for _ in range(hidden_layers - 1):\n",
    "        model.add(keras.layers.Dense(layer_size,activation = 'relu'))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(leraning_rate)\n",
    "    mode.compile(loss='mse',optimizer=optimizer)\n",
    "    return model\n",
    "skleran_model = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n",
    "sklearn_model.fit(x_train_scaled, y_train, epochs = 100, validation_data = (x_valid_scaled,y_valid))\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu'),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "```\n",
    "自定义层的两种方式\n",
    "```\n",
    "# 自定义层的两种方式\n",
    "Customized_softplus = keras.layers.Lambda(lambda x:tf.nn.softplus(x))\n",
    "\n",
    "class CustomizedDenseLayer(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        # 层次的输出：units\n",
    "        self.units = units\n",
    "        self.activation = keras.layers.Activation(activation)\n",
    "        super(CustomizedDenseLayer, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        '''构建所需要的参数'''\n",
    "        # x*w+b input_shape:[none,a], w;[a,b],output_shape:[none,b]\n",
    "        self.kernel = self.add_weight(name='kernel', shape=(input_shape[1],self.units),initializer='uniform',trainable=True)\n",
    "        self.bias = self.add_weight(name='bias', shape=(self.units,),initializer='zeros',trainable=True)\n",
    "        super(CustomizedDenseLayer, self).build(input_shape)\n",
    "    def call(self,x):\n",
    "        '''完整计算向量'''\n",
    "        return self.activation(x @ self.kernel + self.bias)\n",
    "model = keras.models.Sequential([\n",
    "    CustomizedDenseLayer(30, activation='relu'),\n",
    "    CustomizedDenseLayer(1),\n",
    "    # Customized_softplus 和 keras.layers.Dense(1,activation='softplus')是等价的\n",
    "    Customized_softplus,\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型的构建\n",
    "# tf.keras.models.sequential()\n",
    "# 将 28*28 的矩阵展开为一维向量\n",
    "print('训练集中第0个数据：',x_train[0].shape)\n",
    "\n",
    "model.build(input_shape=(None, 8))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer='sgd', metrics= ['accuracy'])\n",
    "# 定义的模型中的所有层\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 超参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于deep&and模型我们的的训练数据也需要进行拆分\n",
    "x_train_scaled_wide = x_train_scaled[:,:5]\n",
    "x_train_scaled_deep = x_train_scaled[:,2:]\n",
    "x_valid_scaled_wide = x_valid_scaled[:,:5]\n",
    "x_valid_scaled_deep = x_valid_scaled[:,2:]\n",
    "x_test_scaled_wide = x_test_scaled[:,:5]\n",
    "x_test_scaled_deep = x_test_scaled[:,2:]\n",
    "# 数据的训练, 当有多个输出的时候，我们的y_train也需要定义两份\n",
    "history = model.fit([x_train_scaled_wide,x_train_scaled_deep], \n",
    "                        [y_train,y_train],\n",
    "                        epochs=10,\n",
    "                        validation_data=([x_valid_scaled_wide,x_valid_scaled_deep],[y_valid,y_valid]))\n",
    "# mdhistory.history"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 数据的训练\n",
    "### [添加回调函数pg](https://tensorflow.google.cn/api_docs/python/tf/keras/callbacks?hl=zh_cn)\n",
    "- Tensorboard: \n",
    "    - Metrics summary plots 指标摘要图\n",
    "    - Training graph visualization 训练图可视化\n",
    "    - Activation histograms 激活直方图\n",
    "    - Sampled profiling 采样分析\n",
    "- EarlyStopping \n",
    "    - 关注某个指标，比如超参\n",
    "        超参数之一是定型周期（epoch）的数量：亦即应当完整遍历数据集多少次（一次为一个epoch）？如果epoch数量太少，网络有可能发生欠拟合（即对于定型数据的学习不够充分）；如果epoch数量太多，则有可能发生过拟合（即网络对定型数据中的“噪声”而非信号拟合）。\n",
    "\n",
    "    早停法旨在解决epoch数量需要手动设置的问题。它也可以被视为一种能够避免网络发生过拟合的正则化方法（与L1/L2权重衰减和丢弃法类似）。\n",
    "\n",
    "    根本原因就是因为继续训练会导致测试集上的准确率下降。\n",
    "    那继续训练导致测试准确率下降的原因猜测可能是1. 过拟合 2. 学习率过大导致不收敛\n",
    "\n",
    "### 对layers进行数据训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回调函数\n",
    "logdir = './callbacks'\n",
    "if not os.path.exists(logdir):\n",
    "    os.mkdir(logdir)\n",
    "output_modle_file = os.path.join(logdir, \"fashion_mnist_model.h5\")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(logdir),\n",
    "    keras.callbacks.ModelCheckpoint(output_modle_file, save_best_only = True),\n",
    "    # 由于epochs设置的比较小，可能不会触发，可以将epochs调大点，看看EarlyStopping的运行情况\n",
    "    keras.callbacks.EarlyStopping(patience=5, min_delta=1e-3),\n",
    "]\n",
    "\n",
    "# 数据的训练\n",
    "history = model.fit(x_train_scaled, y_train, epochs=10,validation_data=(x_valid_scaled,y_valid))\n",
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## matloplib可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matloplib可视化\n",
    "def plot_learning_cruves(history):\n",
    "    pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0,1)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加上Z_scores归一化,dnn后的损失图像：\n",
    "plot_learning_cruves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "### 关于梯度消失问题\n",
    "深度模型中可能出现的问题：参数众多，倒只训练不充分\n",
    "梯度消失问题：首先梯度下降是指，一个数按照其此点最大导数的反方向更新。\n",
    "对于一个多层次的神经网络来说，比目标函数比较远的但是梯度比较微小的现象。\n",
    "什么情况会导致？ 一般发生在深度模型中，根据链式法则：符合函数{f(g(x))}\n",
    "梯度下降的时候我们需要对每一个嵌套的复合函数进行求导再相乘，最后如果求出来的导数小于1，多此相乘就会导致梯度消失。\n",
    "1.01^99=37.8\n",
    "0.99^99=0.03\n",
    "\n",
    "批归一化可以在一定程度上缓解梯度消失~\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test_scaled, y_test)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit ('rec_env': virtualenv)",
   "language": "python",
   "name": "python36764bitrecenvvirtualenvc590a763a4564b6f98e8e2c90348aa96"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}