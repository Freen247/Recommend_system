{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 包的导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "sys.version_info(major=3, minor=6, micro=7, releaselevel='final', serial=0)\ntensorflow 2.1.0\nmatplotlib 3.2.0\nnumpy 1.18.1\npandas 1.0.1\nsklearn 0.22.2.post1\ntensorflow 2.1.0\ntensorflow_core.keras 2.2.4-tf\n"
    }
   ],
   "source": [
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "import os, sys, time \n",
    "\n",
    "\n",
    "print(sys.version_info)\n",
    "for module in tf, mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF 骚操作\n",
    "将自己写的function转化成tf.function\n",
    "优势：快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tf.Tensor(-0.63212055, shape=(), dtype=float32)\ntf.Tensor([-0.95021296 -0.917915  ], shape=(2,), dtype=float32)\ntf.Tensor(-0.63212055, shape=(), dtype=float32)\ntf.Tensor([-0.95021296 -0.917915  ], shape=(2,), dtype=float32)\ntf.Tensor(-0.63212055, shape=(), dtype=float32)\ntf.Tensor([-0.95021296 -0.917915  ], shape=(2,), dtype=float32)\n"
    },
    {
     "data": {
      "text/markdown": "```python\ndef tf__scaled_elu(z, scale=None, alpha=None):\n  do_return = False\n  retval_ = ag__.UndefinedReturnValue()\n  with ag__.FunctionScope('scaled_elu', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n    is_postive = ag__.converted_call(tf.greater_equal, (z, 0.0), None, fscope)\n    do_return = True\n    retval_ = fscope.mark_return_value(scale * ag__.converted_call(tf.where, (is_postive, z, alpha * ag__.converted_call(tf.nn.elu, (z,), None, fscope)), None, fscope))\n  do_return,\n  return ag__.retval(retval_)\n\n```",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tf.function and auto-graph\n",
    "def scaled_elu(z, scale=1.0, alpha=1.0):\n",
    "    # z>=0? scale * z : scale:alpha * tf.nn.eu(z)\n",
    "    is_postive = tf.greater_equal(z, 0.0)\n",
    "    return scale * tf.where(is_postive, z, alpha * tf.nn.elu(z))\n",
    "print(scaled_elu(tf.constant(-1.)))\n",
    "print(scaled_elu(tf.constant([-3., -2.5])))\n",
    "# 用tf.function 将函数转化为tf函数\n",
    "# 方式1\n",
    "tf_scale = tf.function(scaled_elu)\n",
    "print(tf_scale(tf.constant(-1.)))\n",
    "print(tf_scale(tf.constant([-3., -2.5])))\n",
    "# 方式2\n",
    "@tf.function\n",
    "def tf_scale2(z, scale=1.0, alpha=1.0):\n",
    "    # z>=0? scale * z : scale:alpha * tf.nn.eu(z)\n",
    "    is_postive = tf.greater_equal(z, 0.0)\n",
    "    return scale * tf.where(is_postive, z, alpha * tf.nn.elu(z))\n",
    "print(tf_scale2(tf.constant(-1.)))\n",
    "print(tf_scale2(tf.constant([-3., -2.5])))\n",
    "\n",
    "# 将展转化后的函数展示出来\n",
    "def display_tf_code(func):\n",
    "    code = tf.autograph.to_code(func)\n",
    "    from IPython.display import display, Markdown\n",
    "    display(Markdown('```python\\n{}\\n```'.format(code)))\n",
    "# 展示源码\n",
    "display_tf_code(scaled_elu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tf.Tensor(21.0, shape=(), dtype=float32)\n"
    }
   ],
   "source": [
    "var = tf.Variable(0.)\n",
    "\n",
    "@tf.function\n",
    "def add_21():\n",
    "    return var.assign_add(21)\n",
    "print(add_21())\n",
    "\n",
    "# 如果将var放在函数里面?,会报错!所以在我们定义使用function之前我们需要将var = tf.Variable(0.)定义在函数初始化之前\n",
    "@tf.function\n",
    "def add_21():\n",
    "    var = tf.Variable(0.)\n",
    "    return var.assign_add(21)\n",
    "# print(add_21()) 打印会报错\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tf.Tensor([ 1.  8. 27.], shape=(3,), dtype=float32)\ntf.Tensor([ 1  8 27], shape=(3,), dtype=int32)\n"
    }
   ],
   "source": [
    "@tf.function()\n",
    "def cube(z):\n",
    "    return tf.pow(z,3)\n",
    "# 我们输入的数据类型不一样 输出的类型也不一样\n",
    "print(cube(tf.constant([1.,2.,3.])))\n",
    "print(cube(tf.constant([1,2,3])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们给函数加上一个变量限制\n",
    "@tf.function(input_signature=[tf.TensorSpec([None], tf.int32, name='x')])\n",
    "def cube(z):\n",
    "    return tf.pow(z,3)\n",
    "try:\n",
    "    print(cube(tf.constant([1.,2.,3.])))\n",
    "    print(cube(tf.constant([1,2,3])))\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<tensorflow.python.eager.function.ConcreteFunction object at 0x0000025520B237F0>\nFalse\nFalse\nFuncGraph(name=cube, id=2564634359456)\n[<tf.Operation 'x' type=Placeholder>, <tf.Operation 'Pow/y' type=Const>, <tf.Operation 'Pow' type=Pow>, <tf.Operation 'Identity' type=Identity>]\n"
    }
   ],
   "source": [
    "cube_func_int32 = cube.get_concrete_function(tf.TensorSpec([None],tf.int32, name='x'))\n",
    "print(cube_func_int32)\n",
    "print(cube_func_int32 is cube.get_concrete_function(tf.TensorSpec([5], tf.int32, name='x')))\n",
    "print(cube_func_int32 is cube.get_concrete_function(tf.constant([1,2,3])))\n",
    "print(cube_func_int32.graph)\n",
    "print(cube_func_int32.graph.get_operations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "name: \"Pow\"\nop: \"Pow\"\ninput: \"x\"\ninput: \"Pow/y\"\nattr {\n  key: \"T\"\n  value {\n    type: DT_INT32\n  }\n}\n\nPow\n[<tf.Tensor 'x:0' shape=(None,) dtype=int32>, <tf.Tensor 'Pow/y:0' shape=() dtype=int32>]\n[<tf.Tensor 'Pow:0' shape=(None,) dtype=int32>]\n"
    }
   ],
   "source": [
    "pow_op =  cube_func_int32.graph.get_operations()[2]\n",
    "# 打印操作的过程\n",
    "print(pow_op)\n",
    "# 其中的值都是可以取出来的\n",
    "print(pow_op.name)\n",
    "print(list(pow_op.inputs))\n",
    "print(list(pow_op.outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "7.999999999994678\n(8.999999999993236, 47.999999999994714)\n"
    }
   ],
   "source": [
    "# 求近似导数\n",
    "f = lambda x:3. *x **2 +2.*x-1\n",
    "approximate_derivative = lambda f, x, eps=1e-4 : (f(x+eps) - f(x-eps))/(2. * eps)\n",
    "print(approximate_derivative(f,1.))\n",
    "\n",
    "g = lambda x1,x2:(x1+5)*(x2**2)\n",
    "def approximate_gradient(g, x1, x2, eps=1e-3):\n",
    "    dg_x1 = approximate_derivative(lambda x:g(x,x2),x1,eps)\n",
    "    dg_x2 = approximate_derivative(lambda x:g(x2,x),x2,eps)\n",
    "    return dg_x1,dg_x2\n",
    "print(approximate_gradient(g,2.,3.))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tf.Tensor(9.0, shape=(), dtype=float32)\nGradientTape.gradient can only be called once on non-persistent tapes.\n"
    }
   ],
   "source": [
    "x1 = tf.Variable(2.0)\n",
    "x2 = tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = g(x1,x2)\n",
    "# 对z函数求x1的偏导\n",
    "# tape只能调用一次\n",
    "dz_x1 = tape.gradient(z,x1)\n",
    "print(dz_x1)\n",
    "\n",
    "try:\n",
    "    dz_x2 = tape.gradient(z,x2)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tf.Tensor(9.0, shape=(), dtype=float32)\ntf.Tensor(42.0, shape=(), dtype=float32)\n"
    }
   ],
   "source": [
    "# 我们加上persistent后就可以求两次偏导了，结果为9和42，和上面的结果误差比较大\n",
    "with tf.GradientTape(persistent = True) as tape:\n",
    "    z = g(x1,x2)\n",
    "\n",
    "dz_x1 = tape.gradient(z,x1)\n",
    "print(dz_x1)\n",
    "\n",
    "try:\n",
    "    dz_x2 = tape.gradient(z,x2)\n",
    "    print(dz_x2)\n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "del(tape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[<tf.Tensor: shape=(), dtype=float32, numpy=9.0>, <tf.Tensor: shape=(), dtype=float32, numpy=42.0>]\n"
    }
   ],
   "source": [
    "# 一次性求出两个偏导\n",
    "with tf.GradientTape() as tape:\n",
    "    z = g(x1,x2)\n",
    "\n",
    "dz = tape.gradient(z,[x1,x2])\n",
    "print(dz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[<tf.Tensor: shape=(), dtype=float32, numpy=9.0>, <tf.Tensor: shape=(), dtype=float32, numpy=42.0>]\n"
    }
   ],
   "source": [
    "\n",
    "x1 = tf.constant(2.0)\n",
    "x2 = tf.constant(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x1)\n",
    "    tape.watch(x2)\n",
    "    z = g(x1,x2)\n",
    "print(tape.gradient(z, [x1,x2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=13.0>"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(5.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z1 = 3*x\n",
    "    z2 = x**2\n",
    "tape.gradient([z1,z2],x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>\ntf.Tensor(2.0, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.2>\ntf.Tensor(0.79999995, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.28>\ntf.Tensor(0.31999993, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.312>\ntf.Tensor(0.12800002, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3248>\ntf.Tensor(0.05120015, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.32992002>\ntf.Tensor(0.020480037, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.33196804>\ntf.Tensor(0.008191943, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.33278725>\ntf.Tensor(0.0032767057, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.33311492>\ntf.Tensor(0.0013104677, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.33324596>\ntf.Tensor(0.00052440166, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.33329841>\ntf.Tensor(0.00020968914, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333194>\ntf.Tensor(8.34465e-05, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.33332774>\ntf.Tensor(3.33786e-05, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.33333108>\ntf.Tensor(1.3589859e-05, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.33333245>\ntf.Tensor(5.2452087e-06, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.333333>\ntf.Tensor(2.1457672e-06, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333332>\ntf.Tensor(8.34465e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.33333328>\ntf.Tensor(2.3841858e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\ntf.Tensor(1.1920929e-07, shape=(), dtype=float32)\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.3333333>\n"
    }
   ],
   "source": [
    "f = lambda x:3. *x **2 +2.*x-1\n",
    "learning_rate = 0.1\n",
    "x = tf.Variable(0.0)\n",
    "print(x)\n",
    "for _ in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        z = f(x)\n",
    "    dz_dx = tape.gradient(z,x)\n",
    "    print(dz_dx)\n",
    "    # x.assign_sub(learning_rate*dz_dx) 表示x= x-(learning_rate*dz_dx)\n",
    "    x.assign_sub(learning_rate*dz_dx)\n",
    "    print(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 数据集的准备\n",
    "Fashion-MNIST是一个替代MNIST手写数字集的图像数据集。 它是由Zalando（一家德国的时尚科技公司）旗下的研究部门提供。其涵盖了来自10种类别的共7万个不同商品的正面图片。Fashion-MNIST的大小、格式和训练集/测试集划分与原始的MNIST完全一致。60000/10000的训练测试数据划分，28x28的灰度图片。你可以直接用它来测试你的机器学习和深度学习算法性能，且不需要改动任何的代码。\n",
    "- 60000张训练图像和对应Label；\n",
    "- 10000张测试图像和对应Label；\n",
    "- 10个类别；\n",
    "- 每张图像28x28的分辨率；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(x_train_all, y_train_all), (x_test, y_test) = fashion_mnist.load_data()\n",
    "# fashion_mnist 训练集共有60000，将前5000作为验证集，后55000作为训练集\n",
    "x_valid, x_train = x_train_all[:5000], x_train_all[5000:]\n",
    "y_valid, y_train = y_train_all[:5000], y_train_all[5000:]\n",
    "\n",
    "print(\"x_valid.shape, y_valid.shape:\",x_valid.shape, y_valid.shape)\n",
    "print(\"x_train.shape, y_train.shape\",x_train.shape, y_train.shape)\n",
    "print(\"x_test.shape, y_test.shape\",x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 数据集样本展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集样本展示\n",
    "def show_single_img(img_arr):\n",
    "    plt.imshow(img_arr, cmap=\"binary\")\n",
    "    plt.show()\n",
    "# show_single_img(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 对数据进行归一化处理\n",
    "> 定义：把数据经过处理后使之限定在一定的范围内。比如通常限制在区间`[0, 1]`或者`[-1, 1]`\n",
    "\n",
    "常用归一化法：\n",
    "- 最大-最小标准化: $$\\frac{x-x_{min}}{x_{max}-x_{min}}$$\n",
    "- Z-score标准化: $$\\frac{x-\\mu }{std} \\left ( \\mu为标准差 ,std 为方差 \\right ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "对数据进行归一化处理\n",
    "定义：把数据经过处理后使之限定在一定的范围内。比如通常限制在区间[0, 1]或者[-1, 1]\n",
    "Z-score归一化\n",
    "'''\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# x_train:[None, 28, 28] -> [None, 784]\n",
    "# 我们np里的数据是int类型，所以我们需要x_train.astype(np.float32)将数据转化成float32\n",
    "# fit_transform 不仅有数据转化为归一化的功能，还有fit（将数据存储下来）的功能.\n",
    "Z_score = lambda d:scaler.fit_transform(d.astype(np.float32).reshape(-1,1)).reshape(-1,28,28)\n",
    "x_train_scaled = Z_score(x_train)\n",
    "x_valid_scaled = Z_score(x_valid)\n",
    "x_test_scaled = Z_score(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 模型的构建\n",
    "tf.keras.models.sequential()\n",
    "普通的模型构建：\n",
    "```\n",
    "model = keras.models.Sequential([\n",
    "    # 第一层输入成，每一个组数据为[28,28]的二维矩阵，通过keras.layers.Flatten压缩成一维矩阵\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300, activation = \"relu\"),\n",
    "    keras.layers.Dense(100, activation = \"relu\"),\n",
    "    keras.layers.Dense(10, activation = \"softmax\"),\n",
    "])\n",
    "```\n",
    "批归一化加激活函数模型构建：\n",
    "\n",
    "```\n",
    "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,activation='relu'))\n",
    "    # 批归一化\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    '''\n",
    "    model.add(keras.layers.Dense(100))\n",
    "    # 将批归一化放在激活函数之前\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    # 我们还可以将激活函数设置为一个层次\n",
    "    model.add(keras.layers.Activation('selu'))\n",
    "    model.add(keras.layers.Dense(10,activation='softmax'))\n",
    "    '''\n",
    "```\n",
    "\n",
    "添加dropout层，一般情况下我们只在最后几层添加dropout：\n",
    "\n",
    "```\n",
    "# deep_neural_network模型的构建(20层)\n",
    "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "for _ in range(20):\n",
    "    # 其中selu是一个自带归一化的激活函数,会在\n",
    "    model.add(keras.layers.Dense(100,activation='selu'))\n",
    "# AlphaDropout相比 强大在：1，均值和方差不变 2.归一化的性质不变，分布不会发生变化，可以和批归一化一起使用\n",
    "model.add(keras.layers.AlphaDropout(rate=0.5,))\n",
    "# model.add(keras.layers.Dropout(rate=0.5))\n",
    "model.add(keras.layers.Dense(10,activation='softmax'))\n",
    "```\n",
    "wide&deep模型的构建：\n",
    "```\n",
    "# 使用API实现wide&deep模型\n",
    "class WideDeepModel(keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(WideDeepModel, self).__init__()\n",
    "        '''定义模型的层次'''\n",
    "        self.hidden1_layer = keras.layers.Dense(30, activation='relu')\n",
    "        self.hidden2_layer = keras.layers.Dense(30, activation='relu')\n",
    "        self.output_layer = keras.layers.Dense(1)\n",
    "        \n",
    "\n",
    "    def call(self, input):\n",
    "        '''完成模型的正向计算'''\n",
    "        hidden1 = self.hidden1_layer(input)\n",
    "        hidden2 = self.hidden2_layer(input)\n",
    "        concat = keras.layers.concatenate([input, hidden2])\n",
    "        output = self.output_layer(concat)\n",
    "        return output\n",
    "# wd模型的建立方式1,这样可以展示更多的细节\n",
    "'''\n",
    "# none 是样本数目，8对应的是features的数目\n",
    "Model: \"wide_deep_model_14\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_42 (Dense)             multiple                  270       \n",
    "_________________________________________________________________\n",
    "dense_43 (Dense)             multiple                  270       \n",
    "_________________________________________________________________\n",
    "dense_44 (Dense)             multiple                  39        \n",
    "=================================================================\n",
    "'''\n",
    "model = WideDeepModel()\n",
    "# wd模型的建立方式2\n",
    "# model = keras.models.Sequential([\n",
    "#     WideDeepModel(),\n",
    "# ])\n",
    "\n",
    "'''\n",
    "# none 是样本数目，8对应的是features的数目\n",
    "Model: \"sequential_1\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "wide_deep_model_13 (WideDeep multiple                  579       \n",
    "=================================================================\n",
    "'''\n",
    "model.build(input_shape=(None, 8))\n",
    "```\n",
    "对于多输入的wide&deep模型的构建：\n",
    "```\n",
    "# 前五个feature当作wide模型的输入\n",
    "input_wide = keras.layers.Input(shape=[5])\n",
    "# 后六个feature当作deep模型的输入\n",
    "input_deep = keras.layers.Input(shape=[6])\n",
    "# deep_model有两个隐含层\n",
    "hidden1 = keras.layers.Dense(30,activation='relu')(input_deep)\n",
    "hidden2 = keras.layers.Dense(30,activation='relu')(hidden1)\n",
    "# 将构建的deep层和wide给拼接起来\n",
    "concat = keras.layers.concatenate([input_wide, hidden2])\n",
    "# 定义输出层\n",
    "output = keras.layers.Dense(1)\n",
    "# 模型定义完成\n",
    "model = keras.model.Model(inputs=[input_wide, input_deep], outputs=[output]))\n",
    "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "# 对于deep&and模型多输入的时候，我们的的训练数据也需要进行拆分\n",
    "x_train_scaled_wide = x_train_scaled[:,:5]\n",
    "x_train_scaled_deep = x_train_scaled[:,2:]\n",
    "x_valid_scaled_wide = x_valid_scaled[:,:5]\n",
    "x_valid_scaled_deep = x_valid_scaled[:,2:]\n",
    "x_test_scaled_wide = x_test_scaled[:,:5]\n",
    "x_test_scaled_deep = x_test_scaled[:,2:]\n",
    "# 数据的训练\n",
    "history = model.fit([x_train_scaled_wide,x_train_scaled_deep], \n",
    "                        y_train,\n",
    "                        epochs=10,\n",
    "                        validation_data=([x_valid_scaled_wide,x_valid_scaled_deep],y_valid))\n",
    "# mdhistory.history\n",
    "```\n",
    "对于多输入多输出的wide&deep模型的构建：\n",
    "```\n",
    "# 前五个feature当作wide模型的输入\n",
    "input_wide = keras.layers.Input(shape=[5])\n",
    "# 后六个feature当作deep模型的输入\n",
    "input_deep = keras.layers.Input(shape=[6])\n",
    "# deep_model有两个隐含层\n",
    "hidden1 = keras.layers.Dense(30,activation='relu')(input_deep)\n",
    "hidden2 = keras.layers.Dense(30,activation='relu')(hidden1)\n",
    "# 将构建的deep层和wide给拼接起来\n",
    "concat = keras.layers.concatenate([input_wide, hidden2])\n",
    "# 定义输出层\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "# 在hidden2层后我们定义一次输出\n",
    "output2 = keras.layers.Dense(1)(hidden2)\n",
    "# 模型定义完成\n",
    "model = keras.models.Model(inputs=[input_wide, input_deep], \n",
    "                            outputs=[output,output2])\n",
    "\n",
    "# reason for sparse : y->index, y->one_hot->[],我们需要将y处理成one_hot向量所以用sparse\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer='sgd', metrics= ['accuracy'])\n",
    "# 定义的模型中的所有层\n",
    "model.layers\n",
    "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "# 对于deep&and模型我们的的训练数据也需要进行拆分\n",
    "x_train_scaled_wide = x_train_scaled[:,:5]\n",
    "x_train_scaled_deep = x_train_scaled[:,2:]\n",
    "x_valid_scaled_wide = x_valid_scaled[:,:5]\n",
    "x_valid_scaled_deep = x_valid_scaled[:,2:]\n",
    "x_test_scaled_wide = x_test_scaled[:,:5]\n",
    "x_test_scaled_deep = x_test_scaled[:,2:]\n",
    "# 数据的训练, 当有多个输出的时候，我们的y_train也需要定义两份\n",
    "history = model.fit([x_train_scaled_wide,x_train_scaled_deep], \n",
    "                        [y_train,y_train],\n",
    "                        epochs=10,\n",
    "                        validation_data=([x_valid_scaled_wide,x_valid_scaled_deep],[y_valid,y_valid]))\n",
    "# mdhistory.history\n",
    "```\n",
    "超参：\n",
    "神经网络有很多训练过程中不变的参数\n",
    "- 网络结构参数：几层，每层宽度，每层激活函数等\n",
    "- 训练参数：batch_size，学习率，学习率衰减算法等\n",
    "手工去试的话比较耗费人力\n",
    "超参搜索策略：\n",
    "- 网络搜索\n",
    "- 随机搜索\n",
    "- 遗传算法搜索\n",
    "- 启发式搜索\n",
    "使用sklearn封装keras模型：\n",
    "```\n",
    "'''\n",
    "RandomizedSearchCV\n",
    "转化为skleran的model\n",
    "定义参数集合\n",
    "搜索参数\n",
    "'''\n",
    "def build_model(hidden_layers = 1,\n",
    "                            layer_size = 30,\n",
    "                            learning_rate = 3e-3):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(layer_size, activation='relu',inpute_shape=x_train.shape[1:]))\n",
    "    for _ in range(hidden_layers - 1):\n",
    "        model.add(keras.layers.Dense(layer_size,activation = 'relu'))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(leraning_rate)\n",
    "    mode.compile(loss='mse',optimizer=optimizer)\n",
    "    return model\n",
    "skleran_model = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n",
    "sklearn_model.fit(x_train_scaled, y_train, epochs = 100, validation_data = (x_valid_scaled,y_valid))\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu'),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "```\n",
    "自定义层的两种方式\n",
    "```\n",
    "# 自定义层的两种方式\n",
    "Customized_softplus = keras.layers.Lambda(lambda x:tf.nn.softplus(x))\n",
    "\n",
    "class CustomizedDenseLayer(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        # 层次的输出：units\n",
    "        self.units = units\n",
    "        self.activation = keras.layers.Activation(activation)\n",
    "        super(CustomizedDenseLayer, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        '''构建所需要的参数'''\n",
    "        # x*w+b input_shape:[none,a], w;[a,b],output_shape:[none,b]\n",
    "        self.kernel = self.add_weight(name='kernel', shape=(input_shape[1],self.units),initializer='uniform',trainable=True)\n",
    "        self.bias = self.add_weight(name='bias', shape=(self.units,),initializer='zeros',trainable=True)\n",
    "        super(CustomizedDenseLayer, self).build(input_shape)\n",
    "    def call(self,x):\n",
    "        '''完整计算向量'''\n",
    "        return self.activation(x @ self.kernel + self.bias)\n",
    "model = keras.models.Sequential([\n",
    "    CustomizedDenseLayer(30, activation='relu'),\n",
    "    CustomizedDenseLayer(1),\n",
    "    # Customized_softplus 和 keras.layers.Dense(1,activation='softplus')是等价的\n",
    "    Customized_softplus,\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型的构建\n",
    "# tf.keras.models.sequential()\n",
    "# 将 28*28 的矩阵展开为一维向量\n",
    "print('训练集中第0个数据：',x_train[0].shape)\n",
    "\n",
    "model.build(input_shape=(None, 8))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer='sgd', metrics= ['accuracy'])\n",
    "# 定义的模型中的所有层\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 超参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于deep&and模型我们的的训练数据也需要进行拆分\n",
    "x_train_scaled_wide = x_train_scaled[:,:5]\n",
    "x_train_scaled_deep = x_train_scaled[:,2:]\n",
    "x_valid_scaled_wide = x_valid_scaled[:,:5]\n",
    "x_valid_scaled_deep = x_valid_scaled[:,2:]\n",
    "x_test_scaled_wide = x_test_scaled[:,:5]\n",
    "x_test_scaled_deep = x_test_scaled[:,2:]\n",
    "# 数据的训练, 当有多个输出的时候，我们的y_train也需要定义两份\n",
    "history = model.fit([x_train_scaled_wide,x_train_scaled_deep], \n",
    "                        [y_train,y_train],\n",
    "                        epochs=10,\n",
    "                        validation_data=([x_valid_scaled_wide,x_valid_scaled_deep],[y_valid,y_valid]))\n",
    "# mdhistory.history"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 数据的训练\n",
    "### [添加回调函数pg](https://tensorflow.google.cn/api_docs/python/tf/keras/callbacks?hl=zh_cn)\n",
    "- Tensorboard: \n",
    "    - Metrics summary plots 指标摘要图\n",
    "    - Training graph visualization 训练图可视化\n",
    "    - Activation histograms 激活直方图\n",
    "    - Sampled profiling 采样分析\n",
    "- EarlyStopping \n",
    "    - 关注某个指标，比如超参\n",
    "        超参数之一是定型周期（epoch）的数量：亦即应当完整遍历数据集多少次（一次为一个epoch）？如果epoch数量太少，网络有可能发生欠拟合（即对于定型数据的学习不够充分）；如果epoch数量太多，则有可能发生过拟合（即网络对定型数据中的“噪声”而非信号拟合）。\n",
    "\n",
    "    早停法旨在解决epoch数量需要手动设置的问题。它也可以被视为一种能够避免网络发生过拟合的正则化方法（与L1/L2权重衰减和丢弃法类似）。\n",
    "\n",
    "    根本原因就是因为继续训练会导致测试集上的准确率下降。\n",
    "    那继续训练导致测试准确率下降的原因猜测可能是1. 过拟合 2. 学习率过大导致不收敛\n",
    "\n",
    "### 对layers进行数据训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回调函数\n",
    "logdir = './callbacks'\n",
    "if not os.path.exists(logdir):\n",
    "    os.mkdir(logdir)\n",
    "output_modle_file = os.path.join(logdir, \"fashion_mnist_model.h5\")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(logdir),\n",
    "    keras.callbacks.ModelCheckpoint(output_modle_file, save_best_only = True),\n",
    "    # 由于epochs设置的比较小，可能不会触发，可以将epochs调大点，看看EarlyStopping的运行情况\n",
    "    keras.callbacks.EarlyStopping(patience=5, min_delta=1e-3),\n",
    "]\n",
    "\n",
    "# 数据的训练\n",
    "history = model.fit(x_train_scaled, y_train, epochs=10,validation_data=(x_valid_scaled,y_valid))\n",
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## matloplib可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matloplib可视化\n",
    "def plot_learning_cruves(history):\n",
    "    pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0,1)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加上Z_scores归一化,dnn后的损失图像：\n",
    "plot_learning_cruves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "### 关于梯度消失问题\n",
    "深度模型中可能出现的问题：参数众多，倒只训练不充分\n",
    "梯度消失问题：首先梯度下降是指，一个数按照其此点最大导数的反方向更新。\n",
    "对于一个多层次的神经网络来说，比目标函数比较远的但是梯度比较微小的现象。\n",
    "什么情况会导致？ 一般发生在深度模型中，根据链式法则：符合函数{f(g(x))}\n",
    "梯度下降的时候我们需要对每一个嵌套的复合函数进行求导再相乘，最后如果求出来的导数小于1，多此相乘就会导致梯度消失。\n",
    "1.01^99=37.8\n",
    "0.99^99=0.03\n",
    "\n",
    "批归一化可以在一定程度上缓解梯度消失~\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test_scaled, y_test)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit ('rec_env': virtualenv)",
   "language": "python",
   "name": "python36764bitrecenvvirtualenvc590a763a4564b6f98e8e2c90348aa96"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}